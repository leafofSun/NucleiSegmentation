import argparse
import os
import torch
import numpy as np
import cv2
import json
from tqdm import tqdm
from collections import defaultdict
from segment_anything import sam_model_registry
from segment_anything.modeling.sam import TextSam 
from metrics import SegMetrics
import torch.nn.functional as F
from PIL import Image # æ–°å¢žï¼šç”¨äºŽ CLIP å¤„ç†
import clip           # æ–°å¢žï¼šCLIP åº“

# åŽå¤„ç†
from skimage.segmentation import watershed
from skimage.feature import peak_local_max
from skimage.morphology import remove_small_objects, opening, disk
from scipy import ndimage
# å½¢æ€å­¦
from skimage.measure import label, regionprops

try:
    from pycocotools import mask as coco_mask
except ImportError:
    pass

# ðŸ”¥ å…¨å±€å®šä¹‰ï¼šç¡®ä¿é¡ºåºä¸Žè®­ç»ƒæ—¶ä¸€è‡´
ID_TO_ORGAN = {
    0: "Kidney", 1: "Breast", 2: "Prostate", 3: "Lung", 
    4: "Colon", 5: "Stomach", 6: "Liver", 7: "Bladder", 
    8: "Brain", 9: "Generic"
}

# ðŸŒŸ æ–°å¢žï¼šåŸºäºŽ CLIP çš„å™¨å®˜è¯Šæ–­å™¨
class OrganPredictor:
    def __init__(self, device):
        self.device = device
        print("ðŸ§  Loading CLIP for Organ Diagnosis...")
        # åŠ è½½ CLIP æ¨¡åž‹ (éœ€ç¡®ä¿æ˜¾å­˜è¶³å¤Ÿï¼ŒViT-B/16 çº¦éœ€å‡ ç™¾MB)
        self.model, self.preprocess = clip.load("ViT-B/16", device=device)
        self.model.eval()
        
        # å‡†å¤‡æ–‡æœ¬ç‰¹å¾
        self.organs = [ID_TO_ORGAN[i] for i in range(len(ID_TO_ORGAN))]
        # æž„é€  prompt æ¨¡æ¿ï¼ŒGeneric ä½œä¸ºå…œåº•
        templates = [f"A histology image of {org} tissue." for org in self.organs]
        
        with torch.no_grad():
            text_inputs = clip.tokenize(templates).to(device)
            self.text_features = self.model.encode_text(text_inputs)
            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)

    def predict(self, image_cv2):
        """
        è¾“å…¥: BGR æ ¼å¼çš„ OpenCV å›¾ç‰‡
        è¾“å‡º: (é¢„æµ‹å™¨å®˜å, å™¨å®˜ID, ç½®ä¿¡åº¦)
        """
        # è½¬ä¸º PIL å¹¶è¿›è¡Œé¢„å¤„ç†
        img_rgb = cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)
        img_pil = Image.fromarray(img_rgb)
        image_input = self.preprocess(img_pil).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = (100.0 * image_features @ self.text_features.T).softmax(dim=-1)
            values, indices = similarity[0].topk(1)
            
        best_idx = indices.item()
        confidence = values.item()
        return self.organs[best_idx], best_idx, confidence

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--work_dir", type=str, default="workdir")
    parser.add_argument("--run_name", type=str, default="text-guided-sam-dynamic-final") 
    
    parser.add_argument("--text_prompt", type=str, default=None, help="Custom prompt override")
    parser.add_argument("--test_attr_path", type=str, default="data/MoNuSeg_SA1B/test_attributes.json")
    
    parser.add_argument("--patch_size", type=int, default=256)
    parser.add_argument("--image_size", type=int, default=1024)
    parser.add_argument("--stride", type=int, default=128)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument("--data_path", type=str, default="data/MoNuSeg_SA1B/test") 
    parser.add_argument("--metrics", nargs='+', default=['dice', 'iou', 'mAJI', 'mPQ'])
    parser.add_argument("--model_type", type=str, default="vit_b")
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--save_pred", action='store_true')
    parser.add_argument("--use_watershed", action='store_true', default=True)
    parser.add_argument("--encoder_adapter", action='store_true', default=True)
    
    return parser.parse_args()

def analyze_predictions(pred_mask):
    labeled = label(pred_mask)
    regions = regionprops(labeled)
    
    if not regions:
        return 0.0, 0.0, 0
    
    areas = []
    roundnesses = []
    
    for r in regions:
        area = r.area
        perimeter = r.perimeter
        if perimeter == 0: roundness = 0
        else: roundness = (4 * np.pi * area) / (perimeter ** 2)
        
        areas.append(area)
        roundnesses.append(roundness)
        
    return np.mean(areas), np.mean(roundnesses), len(regions)

def postprocess_watershed(prob_map, thresh=0.4, min_distance=3):
    binary_mask = prob_map > thresh
    binary_mask = opening(binary_mask, disk(1))
    distance = ndimage.distance_transform_edt(binary_mask)
    coords = peak_local_max(distance, min_distance=min_distance, labels=binary_mask)
    mask = np.zeros(distance.shape, dtype=bool)
    mask[tuple(coords.T)] = True
    markers, _ = ndimage.label(mask)
    labels = watershed(-distance, markers, mask=binary_mask)
    final_mask = remove_small_objects(labels, min_size=15)
    return (final_mask > 0).astype(np.uint8)

# ðŸ”¥ [ä¿®æ”¹] å¢žåŠ äº† organ_id å‚æ•°
def sliding_window_inference(model, image, device, patch_size, image_size, stride, text_prompt, organ_id, filename=None):
    h, w = image.shape[:2]
    pad_h = (patch_size - h % patch_size) % patch_size
    pad_w = (patch_size - w % patch_size) % patch_size
    image_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)
    h_pad, w_pad = image_pad.shape[:2]
    
    prob_map_full = np.zeros((h_pad, w_pad), dtype=np.float32)
    count_map_full = np.zeros((h_pad, w_pad), dtype=np.float32)
    
    y_steps = list(range(0, h_pad - patch_size + 1, stride))
    if (h_pad - patch_size) % stride != 0: y_steps.append(h_pad - patch_size)
    x_steps = list(range(0, w_pad - patch_size + 1, stride))
    if (w_pad - patch_size) % stride != 0: x_steps.append(w_pad - patch_size)
    
    model.eval()
    with torch.no_grad():
        for y in y_steps:
            for x in x_steps:
                patch = image_pad[y:y+patch_size, x:x+patch_size, :]
                patch_large = cv2.resize(patch, (image_size, image_size), interpolation=cv2.INTER_LINEAR)
                img_tensor = torch.from_numpy(patch_large).permute(2, 0, 1).float().to(device)
                
                input_sample = [{
                    'image': img_tensor,
                    'original_size': (image_size, image_size), 
                    'text_prompt': text_prompt,
                    # ðŸ”¥ [å…³é”®] æ³¨å…¥ organ_id ä»¥æ¿€æ´»éšå¼çŸ¥è¯†åº“ (DualPromptLearner)
                    'organ_id': organ_id,
                    'attribute_text': text_prompt # KIM æ¨¡å—åŒæ—¶ä¹Ÿéœ€è¦æ˜¾å¼æ–‡æœ¬
                }]
                
                outputs = model(input_sample, multimask_output=True)
                out = outputs[0]
                
                scores = out['iou_predictions'].squeeze()
                best_idx = torch.argmax(scores).item()
                logits_large = out['masks'][0, best_idx, :, :] 
                
                logits_large = logits_large.unsqueeze(0).unsqueeze(0)
                logits_small = F.interpolate(logits_large, size=(patch_size, patch_size), mode='bilinear', align_corners=False)
                prob_small = torch.sigmoid(logits_small).squeeze().cpu().numpy()
                
                prob_map_full[y:y+patch_size, x:x+patch_size] += prob_small
                count_map_full[y:y+patch_size, x:x+patch_size] += 1.0
                
    count_map_full[count_map_full == 0] = 1.0
    avg_prob = prob_map_full / count_map_full
    return avg_prob[:h, :w]

# ðŸ”¥ [ä¿®å¤ç‰ˆ] test.py ä¸­çš„ load_filtered_gt
def load_filtered_gt(img_path, attr_data, target_tag=None):
    # 1. å°è¯•ä»Žå±žæ€§æ–‡ä»¶ä¸­åŒ¹é…æ–‡ä»¶å
    base_name = os.path.basename(img_path)
    filename_key = None
    
    # æ¨¡ç³ŠåŒ¹é…é€»è¾‘
    if base_name in attr_data:
        filename_key = base_name
    else:
        # å°è¯•æ‰¾åŒ…å«å…³ç³»çš„ key
        for k in attr_data.keys():
            if base_name in k or k in base_name:
                filename_key = k
                break
    
    # 2. ç¡®å®šè¦ä¿ç•™çš„ ID
    valid_ids = None # None è¡¨ç¤ºä¿ç•™æ‰€æœ‰
    if filename_key and filename_key in attr_data:
        instances = attr_data[filename_key]
        # å¦‚æžœæŒ‡å®šäº†ç‰¹å®š Tag (å¦‚ "Tumor")ï¼Œåˆ™ç­›é€‰
        if target_tag and target_tag not in ["Generic", "Cell nuclei", "Auto_Organ", None]:
            valid_ids = set()
            for inst in instances:
                tags = [t.lower() for t in inst.get('tags', [])]
                search_key = target_tag.split()[0].lower() # å–ç¬¬ä¸€ä¸ªè¯åŒ¹é…
                if search_key in tags:
                    valid_ids.add(inst['id'])
    
    # 3. è¯»å–åŽŸå§‹ JSON (å…œåº•é€»è¾‘)
    json_path = os.path.splitext(img_path)[0] + ".json"
    if not os.path.exists(json_path):
        # å°è¯•æ›¿æ¢åŽç¼€æŸ¥æ‰¾
        possible_json = img_path.rsplit('.', 1)[0] + ".json"
        if os.path.exists(possible_json):
            json_path = possible_json
        else:
            return None # çœŸçš„æ²¡æœ‰ GT æ–‡ä»¶

    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        # å…¼å®¹ä¸åŒæ ¼å¼
        if isinstance(data, dict):
            anns = data.get('annotations', [])
        else:
            anns = data
            
        if not anns: return None
        
        # åˆå§‹åŒ– Mask
        temp_img = cv2.imread(img_path)
        h, w = temp_img.shape[:2]
        mask = np.zeros((h, w), dtype=np.uint8)
        
        # 4. ç»˜åˆ¶ Mask
        for idx, ann in enumerate(anns):
            # ðŸ”¥ [æ ¸å¿ƒä¿®å¤] å¦‚æžœ valid_ids æ˜¯ Noneï¼Œè¯´æ˜Žä¸è¿‡æ»¤ï¼Œå…¨éƒ¨ç»˜åˆ¶
            if valid_ids is not None and idx not in valid_ids:
                continue
                
            if 'segmentation' not in ann: continue
            seg = ann['segmentation']
            
            if isinstance(seg, dict) and 'counts' in seg: 
                rle_mask = coco_mask.decode(seg)
                mask[rle_mask > 0] = 1
            elif isinstance(seg, list):
                for poly in seg:
                    pts = np.array(poly, dtype=np.int32).reshape((-1, 2))
                    cv2.fillPoly(mask, [pts], 1)
        
        return mask

    except Exception as e:
        print(f"âš ï¸ Error loading GT for {base_name}: {e}")
        return None

def main(args):
    # åˆå§‹åŒ–æ¨¡åž‹
    vanilla_sam = sam_model_registry[args.model_type](args)
    model = TextSam(
        image_encoder=vanilla_sam.image_encoder,
        prompt_encoder=vanilla_sam.prompt_encoder,
        mask_decoder=vanilla_sam.mask_decoder,
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256,
        num_organs=10 # ç¡®ä¿å’Œä½ è®­ç»ƒæ—¶ä¸€è‡´
    ).to(args.device)
    
    if os.path.exists(args.checkpoint):
        checkpoint = torch.load(args.checkpoint, map_location=args.device)
        model.load_state_dict(checkpoint.get('model', checkpoint), strict=False)
        print(f"âœ… Loaded checkpoint: {args.checkpoint}")
    else:
        print(f"âŒ Checkpoint not found at {args.checkpoint}")
        return

    # ðŸ”¥ åˆå§‹åŒ–å™¨å®˜é¢„æµ‹å™¨
    organ_predictor = OrganPredictor(args.device)

    attr_data = {}
    if os.path.exists(args.test_attr_path):
        with open(args.test_attr_path, 'r') as f:
            content = json.load(f)
            attr_data = content.get("images", {})

    image_files = []
    for root, dirs, files in os.walk(args.data_path):
        for f in files:
            if f.lower().endswith(('.tif', '.png', '.jpg')) and 'mask' not in f.lower():
                image_files.append(os.path.join(root, f))
    
    all_metrics = defaultdict(list)
    prompt_stats = {"avg_area": [], "avg_roundness": [], "count": []}
    
    save_dir = os.path.join(args.work_dir, args.run_name, "inference_viz")
    if args.save_pred: os.makedirs(save_dir, exist_ok=True)

    print('*'*60)
    print(f"ðŸš€ Running Inference: {args.run_name}")
    print(f"ðŸ§  AI-Diagnosis Mode Active")
    print('*'*60)

    pbar = tqdm(image_files)
    for img_path in pbar:
        filename = os.path.basename(img_path)
        image = cv2.imread(img_path)
        if image is None: continue
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ==========================================
        # ðŸŒŸ æ ¸å¿ƒæ­¥éª¤ï¼šAI è‡ªåŠ¨è¯Šæ–­
        # ==========================================
        pred_organ, pred_id, conf = organ_predictor.predict(image)
        
        # é€»è¾‘åˆ†æ”¯
        if args.text_prompt:
            # ç”¨æˆ·å¼ºåˆ¶æŒ‡å®š (Expert Mode)
            prompt_text = args.text_prompt
            current_organ_id = 9 # Generic
            log_msg = f"ðŸ‘¤ Override: '{prompt_text}' (Ignored AI: {pred_organ})"
        else:
            # AI è‡ªé€‚åº” (Auto Mode)
            prompt_text = f"{pred_organ} cell nuclei"
            current_organ_id = pred_id
            log_msg = f"ðŸ§  AI: {pred_organ} ({conf:.1%}) -> '{prompt_text}'"

        pbar.write(f"ðŸ–¼ï¸  {filename} | {log_msg}")
        
        # ==========================================
        # ðŸš€ å¸¦ç€å™¨å®˜ä¿¡æ¯åŽ»åˆ†å‰²
        # ==========================================
        pred_prob = sliding_window_inference(
            model, image_rgb, args.device, 
            patch_size=args.patch_size,
            image_size=args.image_size,
            stride=args.stride,
            text_prompt=prompt_text, 
            organ_id=current_organ_id, # ðŸ”¥ ä¼ å…¥ ID
            filename=filename
        )
        
        pred_mask = (pred_prob > 0.5).astype(np.uint8)
        
        # ç»Ÿè®¡ä¿¡æ¯
        p_area, p_round, p_count = analyze_predictions(pred_mask)
        if p_count > 0:
            prompt_stats["avg_area"].append(p_area)
            prompt_stats["avg_roundness"].append(p_round)
            prompt_stats["count"].append(p_count)

        # æŒ‡æ ‡è®¡ç®— (å¦‚æžœæœ‰å…³è”çš„GT)
        # æ³¨æ„ï¼šè¿™é‡Œ load_filtered_gt ä»ç„¶ä½¿ç”¨ text_prompt æ¥è¿‡æ»¤
        # å¦‚æžœæ˜¯ Auto Modeï¼Œæˆ‘ä»¬æš‚ä¸”è®¤ä¸ºå°±æ˜¯è¯„ä¼°"æ‰€æœ‰ç»†èƒž"
        target_tag_for_eval = args.text_prompt if args.text_prompt else "Auto_Organ"
        gt_mask = load_filtered_gt(img_path, attr_data, target_tag=target_tag_for_eval)
        
        if gt_mask is not None:
            if gt_mask.shape != pred_mask.shape:
                gt_mask = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)
            
            res = SegMetrics(pred_mask, gt_mask, args.metrics)
            for k, v in res.items():
                all_metrics[k].append(v)
        
        # å¯è§†åŒ–
        if args.save_pred:
            # 1. å¤åˆ¶åŽŸå›¾
            vis = image.copy()
            
            # 2. ç»˜åˆ¶ GT (çº¢è‰²) - BGR: (0, 0, 255)
            # æ³¨æ„ï¼šgt_mask å¯èƒ½ä¸º None (å¦‚æžœæ²¡æœ‰å¯¹åº”çš„ JSON)
            if gt_mask is not None:
                # ç¡®ä¿å°ºå¯¸ä¸€è‡´
                if gt_mask.shape != pred_mask.shape:
                    gt_mask = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)
                
                cnts_gt, _ = cv2.findContours(gt_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                # ä½¿ç”¨çº¿å®½ 2
                cv2.drawContours(vis, cnts_gt, -1, (0, 0, 255), 2)

            # 3. ç»˜åˆ¶é¢„æµ‹ (ç»¿è‰²) - BGR: (0, 255, 0)
            cnts_pred, _ = cv2.findContours(pred_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(vis, cnts_pred, -1, (0, 255, 0), 2)
            
            # 4. æ·»åŠ å›¾ä¾‹å’Œä¿¡æ¯
            # é¡¶éƒ¨ä¿¡æ¯ï¼šPrompt å’Œ AI è¯Šæ–­
            cv2.putText(vis, f"AI: {pred_organ} ({conf:.2f})", (10, 30), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2) # é’è‰²æ–‡å­—
            
            # åº•éƒ¨ä¿¡æ¯ï¼šå›¾ä¾‹ (çº¢=GT, ç»¿=Pred)
            h_img, w_img = vis.shape[:2]
            legend_text = f"Red: GT | Green: Pred | Area: {int(p_area)}"
            cv2.putText(vis, legend_text, (10, h_img - 20), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) # ç™½è‰²æ–‡å­—

            # 5. ä¿å­˜
            save_path = os.path.join(save_dir, filename.replace('.tif','.jpg').replace('.png', '.jpg'))
            cv2.imwrite(save_path, vis)

    print("\n" + "="*40)
    print(f"ðŸ“Š Final Results:")
    for k, v in all_metrics.items():
        if len(v) > 0:
            print(f"  {k:>10}: {np.mean(v):.4f}")
    print("="*40)

if __name__ == '__main__':
    args = parse_args()
    main(args)