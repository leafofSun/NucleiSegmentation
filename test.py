import argparse
import os
import torch
import numpy as np
import cv2
from tqdm import tqdm
from collections import defaultdict
from segment_anything import sam_model_registry
from segment_anything.modeling.sam import TextSam 
from metrics import SegMetrics

# åå¤„ç†åº“
from skimage.segmentation import watershed
from skimage.feature import peak_local_max
from skimage.morphology import remove_small_objects, opening, disk
from scipy import ndimage

# GT è§£æ
try:
    from pycocotools import mask as coco_mask
except ImportError:
    pass

# === ğŸ”¥ [æ ¸å¿ƒ] MoNuSeg æµ‹è¯•é›†å™¨å®˜æ˜ å°„è¡¨ (Hardcoded) ===
# åªè¦æ–‡ä»¶ååŒ…å« Keyï¼Œå°±è‡ªåŠ¨ä½¿ç”¨å¯¹åº”çš„ Prompt
ORGAN_MAP = {
    "TCGA-2Z-A9J9": "Prostate", "TCGA-44-2665": "Kidney", 
    "TCGA-69-7764": "Kidney", "TCGA-A6-2675": "Colorectal",
    "TCGA-A6-2680": "Colorectal", "TCGA-A6-5662": "Lung",
    "TCGA-AC-A2FO": "Lung", "TCGA-AO-A0J2": "Breast",
    "TCGA-CU-A0YN": "Bladder", "TCGA-EJ-A46H": "Prostate",
    "TCGA-FG-A4MU": "Prostate", "TCGA-GL-A4EM": "Kidney",
    "TCGA-HC-7209": "Lung", "TCGA-HT-8564": "Brain"
}

def get_smart_prompt(filename):
    """æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨è¿”å›æœ€ç²¾å‡†çš„ Organ Prompt"""
    organ = "tissue"
    for key, val in ORGAN_MAP.items():
        if key in filename:
            organ = val
            break
            
    # æ„é€  Rich Text
    # è¿™é‡Œçš„å½¢å®¹è¯æ˜¯æˆ‘ä»¬æ ¹æ®ç—…ç†ç»éªŒåŠ çš„ï¼Œå¼ºè¿«æ¨¡å‹å…³æ³¨å½¢æ€
    prompt = f"Deep purple {organ} cell nuclei, densely packed, H&E stained"
    return prompt, organ

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--work_dir", type=str, default="workdir")
    parser.add_argument("--run_name", type=str, default="text-guided-sam-rich") 
    parser.add_argument("--patch_size", type=int, default=256)
    parser.add_argument("--stride", type=int, default=128)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument("--data_path", type=str, default="data/MoNuSeg_SA1B/test") 
    parser.add_argument("--prompt_path", type=str, default=None, help="Deprecated. We use hardcoded map.")
    parser.add_argument("--metrics", nargs='+', default=['dice', 'iou', 'mAJI', 'mPQ'])
    parser.add_argument("--model_type", type=str, default="vit_b")
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--encoder_adapter", action='store_true', default=True)
    parser.add_argument("--save_pred", action='store_true')
    parser.add_argument("--use_watershed", action='store_true', default=True)
    return parser.parse_args()

def load_gt_mask(img_path):
    """
    å…¨èƒ½å‹ GT åŠ è½½å‡½æ•°ï¼šæ”¯æŒ JSON, PNG, _mask, Labels ç›®å½•ç­‰å¤šç§å˜ä½“
    """
    import json # <--- ğŸ”¥ [å…³é”®ä¿®å¤] å¼ºåˆ¶åœ¨å‡½æ•°å†…å¼•å…¥ json æ¨¡å—
    import os
    import cv2
    import numpy as np
    try:
        from pycocotools import mask as coco_mask
    except ImportError:
        pass

    base_name = os.path.splitext(os.path.basename(img_path))[0]
    dir_name = os.path.dirname(img_path)
    
    # 1. å°è¯•åŒå SA-1B JSON
    json_path = os.path.splitext(img_path)[0] + ".json"
    
    # å°è¯•è¯»å–å›¾ç‰‡è·å–å°ºå¯¸
    temp_img = cv2.imread(img_path)
    if temp_img is None: 
        # print(f"âš ï¸ Image not found: {img_path}")
        return None
    h, w = temp_img.shape[:2]
    mask = np.zeros((h, w), dtype=np.uint8)
    
    # === ç­–ç•¥ A: è¯»å– JSON ===
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r') as f:
                data = json.load(f) # ç°åœ¨è¿™é‡Œç»å¯¹ä¸ä¼šæŠ¥ name 'json' is not defined äº†
            anns = data.get('annotations', [])
            if not anns and isinstance(data, list): anns = data
            found_ann = False
            for ann in anns:
                if 'segmentation' in ann:
                    found_ann = True
                    seg = ann['segmentation']
                    if isinstance(seg, dict) and 'counts' in seg: 
                        rle_mask = coco_mask.decode(seg)
                        mask[rle_mask > 0] = 1
                    elif isinstance(seg, list):
                        for poly in seg:
                            pts = np.array(poly, dtype=np.int32).reshape((-1, 2))
                            cv2.fillPoly(mask, [pts], 1)
            if found_ann: 
                # print(f"âœ… Loaded GT from JSON: {json_path}")
                return mask
        except Exception as e:
            print(f"âš ï¸ Error parsing JSON {json_path}: {e}")

    # === ç­–ç•¥ B: è¯»å– PNG/TIF Mask ===
    # MoNuSeg å¸¸è§çš„ Mask å­˜æ”¾ä½ç½®
    candidates = [
        # 1. åŒç›®å½•ä¸‹åŒå
        os.path.join(dir_name, base_name + ".png"),
        os.path.join(dir_name, base_name + ".tif"),
        # 2. åŒç›®å½•ä¸‹åŠ åç¼€
        os.path.join(dir_name, base_name + "_mask.png"),
        os.path.join(dir_name, base_name + "_label.png"),
        # 3. çˆ¶ç›®å½•ä¸‹çš„ Labels/BinaryMask æ–‡ä»¶å¤¹
        img_path.replace("Images", "Labels").replace(".tif", ".png"),
        img_path.replace("test", "test/Labels").replace(".tif", ".png"),
        # 4. æš´åŠ›æ›¿æ¢æ‰©å±•å
        img_path.replace(".tif", ".png"),
        img_path.replace(".tif", "_mask.png")
    ]
    
    for p in candidates:
        if os.path.exists(p):
            m = cv2.imread(p, 0) # è¯»å–ç°åº¦
            if m is not None:
                # print(f"âœ… Loaded GT from PNG: {p}")
                # ç¡®ä¿å°ºå¯¸ä¸€è‡´
                if m.shape != (h, w):
                    m = cv2.resize(m, (w, h), interpolation=cv2.INTER_NEAREST)
                return (m > 0).astype(np.uint8)
    
    # print(f"âŒ No GT found for {base_name}")
    return None

from skimage.segmentation import watershed
from skimage.feature import peak_local_max
from skimage.morphology import remove_small_objects, opening, disk
from scipy import ndimage

def postprocess_watershed(prob_map, thresh=0.35, min_distance=3):
    """
    é€‚é… TextSam çš„è·ç¦»å˜æ¢åˆ†æ°´å²­
    """
    # 1. æ¿€è¿›çš„äºŒå€¼åŒ–ï¼šåªè¦æœ‰ 35% æŠŠæ¡å°±è®¤ä¸ºæ˜¯å‰æ™¯ï¼Œå…ˆå¬å›å†åˆ‡åˆ†
    binary_mask = prob_map > thresh
    binary_mask = opening(binary_mask, disk(1))
    # 2. ç¨å¾®è…èš€ä¸€ç‚¹ç‚¹ï¼Œæ–­å¼€æå…¶ç»†å¾®çš„ç²˜è¿
    # binary_mask = opening(binary_mask, disk(1)) 
    
    # 3. è®¡ç®—è·ç¦»åœºï¼šè¶Šé è¿‘ç»†èƒä¸­å¿ƒï¼Œå€¼è¶Šå¤§
    distance = ndimage.distance_transform_edt(binary_mask)
    
    # 4. å¯»æ‰¾å±±å³° (ç§å­ç‚¹)
    # min_distance=3 æ˜¯å…³é”®ï¼å…è®¸ä¸¤ä¸ªç»†èƒæ ¸ä¸­å¿ƒè·ç¦»åªæœ‰3åƒç´ 
    # è¿™èƒ½è§£å†³ MoNuSeg ä¸­é‚£ç§æå…¶æ‹¥æŒ¤çš„ç»†èƒç²˜è¿
    coords = peak_local_max(distance, min_distance=min_distance, labels=binary_mask)
    
    mask = np.zeros(distance.shape, dtype=bool)
    mask[tuple(coords.T)] = True
    markers, _ = ndimage.label(mask)
    
    # 5. æ‰§è¡Œåˆ†æ°´å²­ï¼šè®©æ°´ä» markers å¼€å§‹æµï¼Œå¡«æ»¡ binary_mask
    labels = watershed(-distance, markers, mask=binary_mask)
    
    # 6. å»é™¤å™ªç‚¹
    final_mask = remove_small_objects(labels, min_size=15)
    
    return (final_mask > 0).astype(np.uint8)

def sliding_window_inference(model, image, device, patch_size=256, stride=128, text_prompt="Cell nuclei"):
    h, w = image.shape[:2]
    # Padding
    pad_h = (patch_size - h % patch_size) % patch_size
    pad_w = (patch_size - w % patch_size) % patch_size
    image_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)
    h_pad, w_pad = image_pad.shape[:2]
    
    prob_map_full = np.zeros((h_pad, w_pad), dtype=np.float32)
    count_map_full = np.zeros((h_pad, w_pad), dtype=np.float32)
    
    y_steps = list(range(0, h_pad - patch_size + 1, stride))
    if (h_pad - patch_size) % stride != 0: y_steps.append(h_pad - patch_size)
    x_steps = list(range(0, w_pad - patch_size + 1, stride))
    if (w_pad - patch_size) % stride != 0: x_steps.append(w_pad - patch_size)
    
    model.eval()
    with torch.no_grad():
        for y in y_steps:
            for x in x_steps:
                patch = image_pad[y:y+patch_size, x:x+patch_size, :]
                img_tensor = torch.from_numpy(patch).permute(2, 0, 1).float().to(device)
                
                input_sample = [{
                    'image': img_tensor,
                    'original_size': (patch_size, patch_size),
                    'text_prompt': text_prompt
                }]
                
                outputs = model(input_sample, multimask_output=True)
                out = outputs[0]
                
                scores = out['iou_predictions'].squeeze()
                best_idx = torch.argmax(scores).item()
                logits = out['masks'][0, best_idx, :, :]
                prob = torch.sigmoid(logits).cpu().numpy()
                
                prob_map_full[y:y+patch_size, x:x+patch_size] += prob
                count_map_full[y:y+patch_size, x:x+patch_size] += 1.0
                
    count_map_full[count_map_full == 0] = 1.0
    avg_prob = prob_map_full / count_map_full
    return avg_prob[:h, :w]

def main(args):
    print('*'*60)
    print(f"ğŸš€ Running Inference: {args.run_name}")
    print(f"   Patch: {args.patch_size} | Watershed: {args.use_watershed}")
    print(f"   Prompt Strategy: Hardcoded Organ Mapping (Robust)")
    print('*'*60)

    args.image_size = args.patch_size 
    args.sam_checkpoint = None 

    # Model
    vanilla_sam = sam_model_registry[args.model_type](args)
    model = TextSam(
        image_encoder=vanilla_sam.image_encoder,
        prompt_encoder=vanilla_sam.prompt_encoder,
        mask_decoder=vanilla_sam.mask_decoder,
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256
    ).to(args.device)
    
    # Checkpoint
    if os.path.exists(args.checkpoint):
        checkpoint = torch.load(args.checkpoint, map_location=args.device)
        state_dict = checkpoint.get('model', checkpoint)
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        print("âœ… Checkpoint Loaded.")
    else:
        print(f"âŒ Checkpoint not found at {args.checkpoint}")
        return

    # Data Scan
    image_files = []
    for root, dirs, files in os.walk(args.data_path):
        for f in files:
            if f.lower().endswith(('.tif', '.png', '.jpg')) and 'mask' not in f.lower():
                image_files.append(os.path.join(root, f))
    
    print(f"ğŸ“‚ Found {len(image_files)} test images.")
    all_metrics = defaultdict(list)
    
    save_dir = os.path.join(args.work_dir, args.run_name, "viz_final")
    if args.save_pred: os.makedirs(save_dir, exist_ok=True)

    # Inference Loop
    for img_path in tqdm(image_files):
        filename = os.path.basename(img_path)
        image = cv2.imread(img_path)
        if image is None: continue
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ğŸ”¥ [å…³é”®] å¼ºåˆ¶ä½¿ç”¨ Organ Prompt
        # prompt_text, organ_name = get_smart_prompt(filename)
        prompt_text = "Cell nuclei"  # <--- å¼ºåˆ¶å›é€€åˆ°é€šç”¨æç¤º
        organ_name = "Generic"
        # æ‰“å°å‡ºæ¥ç¡®è®¤ä¸€ä¸‹ï¼
        # tqdm.write(f"Processing {filename} -> Organ: {organ_name} | Prompt: {prompt_text[:30]}...")
        
        # Inference
        pred_prob = sliding_window_inference(
            model, image_rgb, args.device, 
            patch_size=args.patch_size, 
            stride=args.stride,
            text_prompt=prompt_text # ä¼ å…¥ç²¾å‡†æ–‡æœ¬
        )
        
        # Post-process
        if args.use_watershed:
            # è°ƒæ•´å‚æ•°ï¼šthresh=0.4 (æ•è·æ›´å¤š), min_distance=5 (åˆ‡å¾—æ›´ç»†)
            pred_mask = postprocess_watershed(pred_prob, thresh=0.4, min_distance=5)
        else:
            pred_mask = (pred_prob > 0.5).astype(np.uint8)
        
        # Metrics
        gt_mask = load_gt_mask(img_path)
        if gt_mask is not None:
            if gt_mask.shape != pred_mask.shape:
                gt_mask = cv2.resize(gt_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation=cv2.INTER_NEAREST)
            res = SegMetrics(pred_mask, gt_mask, args.metrics)
            for k, v in res.items():
                all_metrics[k].append(v)
        
        # Viz
        if args.save_pred:
            vis = image.copy()
            cnts, _ = cv2.findContours(pred_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            cv2.drawContours(vis, cnts, -1, (0, 255, 0), 2)
            cv2.putText(vis, f"{organ_name}", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.imwrite(os.path.join(save_dir, filename.replace('.tif','.jpg')), vis)

    print("\n" + "="*40)
    print(f"ğŸ“Š Final Results (Watershed+, RichPrompt):")
    for k, v in all_metrics.items():
        if len(v) > 0:
            print(f"{k:>10}: {np.mean(v):.4f}")
    print("="*40)

if __name__ == '__main__':
    main(parse_args())