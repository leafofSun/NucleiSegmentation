import torch
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import argparse
import os
import cv2
import numpy as np
import json
import glob
from tqdm import tqdm

# å¼•å…¥æ¨¡å— 
# (è¯·ç¡®ä¿ prompt_generator.py æ˜¯åŒ…å« build_target_v2 å’Œ auto_box_loss_v2 çš„æœ€æ–°ç‰ˆ)
from segment_anything import sam_model_registry
from prompt_generator import AutoBoxGenerator, build_target_v2, auto_box_loss_v2

# =================================================================================
# 1. SA-1B æ ‡å‡†æ ¼å¼æ•°æ®é›†åŠ è½½å™¨
# =================================================================================
class SA1BDataset(Dataset):
    def __init__(self, data_root, image_size=1024):
        self.image_size = image_size
        self.pixel_mean = [123.675, 116.28, 103.53]
        self.pixel_std = [58.395, 57.12, 57.375]
        
        # é€’å½’æ‰«ææ‰€æœ‰ JSON æ–‡ä»¶
        # SA-1B æ ¼å¼çš„æ ¸å¿ƒæ˜¯ JSONï¼Œå›¾ç‰‡ä¸ JSON åŒå
        self.json_files = sorted(glob.glob(os.path.join(data_root, '**', '*.json'), recursive=True))
        
        # è¿‡æ»¤æ‰éæ ‡æ³¨æ–‡ä»¶ï¼ˆä»¥é˜²ä¸‡ä¸€æ–‡ä»¶å¤¹é‡Œæœ‰æ— å…³jsonï¼‰
        self.valid_files = []
        for jf in self.json_files:
            if "image2label" in jf: continue # æ’é™¤ MoNuSeg æ—§ç‰ˆç´¢å¼•æ–‡ä»¶
            self.valid_files.append(jf)
            
        print(f"âœ… [Dataset] Found {len(self.valid_files)} JSON annotation files in {data_root}")

    def __len__(self):
        return len(self.valid_files)

    def __getitem__(self, index):
        json_path = self.valid_files[index]
        
        # 1. å¯»æ‰¾å¯¹åº”çš„å›¾ç‰‡
        # å‡è®¾å›¾ç‰‡å’Œ JSON åŒåï¼Œå°è¯•å¸¸è§åç¼€
        base_path = os.path.splitext(json_path)[0]
        img_path = None
        for ext in ['.png', '.jpg', '.jpeg', '.tif', '.tiff']:
            if os.path.exists(base_path + ext):
                img_path = base_path + ext
                break
        
        if img_path is None:
            raise FileNotFoundError(f"âŒ Image not found for JSON: {json_path}")

        # 2. è¯»å–å›¾ç‰‡
        image = cv2.imread(img_path)
        if image is None:
            raise ValueError(f"âŒ Failed to read image: {img_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        ori_h, ori_w = image.shape[:2]
        
        # Resize å›¾ç‰‡åˆ° 1024x1024
        image_resized = cv2.resize(image, (self.image_size, self.image_size))
        
        # 3. è§£æ JSON è·å– BBox
        with open(json_path, 'r') as f:
            data = json.load(f)
            
        boxes_list = []
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale_x = self.image_size / ori_w
        scale_y = self.image_size / ori_h
        
        # SA-1B æ ¼å¼é€šå¸¸åŒ…å« 'annotations' åˆ—è¡¨
        annotations = data.get('annotations', [])
        
        for ann in annotations:
            if 'bbox' not in ann: continue
            
            # SA-1B æ ‡å‡† bbox æ ¼å¼: [x, y, w, h]
            x, y, w, h = ann['bbox']
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬éœ€è¦çš„æ ¼å¼: [x1, y1, x2, y2]
            x1 = x
            y1 = y
            x2 = x + w
            y2 = y + h
            
            # æ‰§è¡Œåæ ‡ç¼©æ”¾
            x1 = x1 * scale_x
            y1 = y1 * scale_y
            x2 = x2 * scale_x
            y2 = y2 * scale_y
            
            # ç®€å•çš„è¾¹ç•Œä¿æŠ¤å’Œå™ªç‚¹è¿‡æ»¤
            if (x2 - x1) < 2 or (y2 - y1) < 2: continue
            
            boxes_list.append([x1, y1, x2, y2])
            
        # 4. å½’ä¸€åŒ– & è½¬ Tensor
        image_tensor = (image_resized - self.pixel_mean) / self.pixel_std
        image_tensor = torch.tensor(image_tensor).permute(2, 0, 1).float()
        
        if len(boxes_list) > 0:
            boxes_tensor = torch.tensor(boxes_list).float()
        else:
            # é˜²æ­¢ç©ºå›¾æŠ¥é”™ï¼Œç»™ä¸€ä¸ªå‡çš„ 0 é¢ç§¯æ¡†ï¼ˆLoss è®¡ç®—æ—¶ä¼šè‡ªåŠ¨å¿½ç•¥ï¼‰
            boxes_tensor = torch.tensor([[0,0,1,1]]).float()

        return {
            "image": image_tensor,
            "all_boxes": boxes_tensor
        }

def collate_fn_dense(batch):
    images = torch.stack([item['image'] for item in batch], dim=0)
    all_boxes = [item['all_boxes'] for item in batch]
    return {'image': images, 'all_boxes': all_boxes}

# =================================================================================
# 2. è®­ç»ƒä¸»æµç¨‹
# =================================================================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_path', type=str, required=True, help="SA-1Bæ ¼å¼æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument('--sam_checkpoint', type=str, required=True, help="SAMæ¨¡å‹æƒé‡è·¯å¾„")
    parser.add_argument('--save_path', type=str, default='workdir/models/auto_box_sa1b')
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch_size', type=int, default=4)
    parser.add_argument('--image_size', type=int, default=1024)
    parser.add_argument('--encoder_adapter', action='store_true', default=True)
    
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    os.makedirs(args.save_path, exist_ok=True)

    # 1. åŠ è½½ SAM (å†»ç»“å‚æ•°ï¼Œåªåšç‰¹å¾æå–)
    print("Loading SAM (Frozen)...")
    sam = sam_model_registry['vit_b'](args=args)
    sam.to(device)
    for param in sam.parameters():
        param.requires_grad = False
    sam.eval()

    # 2. åˆå§‹åŒ– AutoBoxGenerator
    print("Initializing AutoBoxGenerator...")
    box_generator = AutoBoxGenerator(embed_dim=256).to(device)
    box_generator.train()
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(box_generator.parameters(), lr=args.lr)

    # 3. åŠ è½½æ•°æ®é›†
    print(f"Initializing SA-1B Dataset from: {args.data_path}")
    dataset = SA1BDataset(data_root=args.data_path, image_size=args.image_size)
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn_dense)

    print(f"Start training Auto-Box Head for {args.epochs} epochs...")
    
    best_loss = float('inf')

    for epoch in range(args.epochs):
        epoch_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{args.epochs}")
        
        for batch in pbar:
            images = batch['image'].to(device)
            gt_boxes_list = [b.to(device) for b in batch['all_boxes']]
            
            with torch.no_grad():
                image_embedding = sam.image_encoder(images)
            
            pred_heatmap, pred_wh = box_generator(image_embedding)
            
            # === æ ¸å¿ƒç­–ç•¥: V2 Target (é«˜æ–¯çƒ­åŠ›å›¾) ===
            target_heatmap, target_wh, target_mask = build_target_v2(
                gt_boxes_list, 
                feature_shape=(64, 64), 
                original_image_size=args.image_size,
                device=device
            )
            
            # === æ ¸å¿ƒç­–ç•¥: V2 Loss (Focal Loss + L1 Loss) ===
            loss_hm, loss_wh = auto_box_loss_v2(pred_heatmap, pred_wh, target_heatmap, target_wh, target_mask)
            
            # === æ ¸å¿ƒç­–ç•¥: åŠ å¤§ WH æƒé‡ ===
            loss = loss_hm + 1.0 * loss_wh
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            pbar.set_postfix({'loss': loss.item(), 'hm': loss_hm.item(), 'wh': loss_wh.item()})
            
        # å­¦ä¹ ç‡è¡°å‡ (å¯é€‰ï¼Œç®€å•èµ·è§è¿™é‡Œçœç•¥ï¼ŒAdamW é€šå¸¸ä¸éœ€è¦å¤ªå¤æ‚çš„è°ƒåº¦)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        avg_loss = epoch_loss / len(dataloader)
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(box_generator.state_dict(), os.path.join(args.save_path, 'best_box_head.pth'))
            print(f"ğŸ”¥ Best Model Saved (Loss: {best_loss:.4f})")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % 10 == 0:
            save_name = os.path.join(args.save_path, f'box_head_epoch{epoch+1}.pth')
            torch.save(box_generator.state_dict(), save_name)

if __name__ == "__main__":
    main()