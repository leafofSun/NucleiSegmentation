import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from DataLoader import TrainingDataset, stack_dict_batched
from utils import FocalDiceloss_IoULoss, get_logger
from metrics import SegMetrics
import time
from tqdm import tqdm
import numpy as np
import datetime
from torch.nn import functional as F
from torch.cuda import amp
import random

# === [Imports] ===
from segment_anything import sam_model_registry
# ç¡®ä¿è¿™é‡Œå¯¼å…¥çš„æ˜¯æˆ‘ä»¬åœ¨ segment_anything/modeling/sam.py ä¸­ä¿®æ”¹åçš„ TextSam ç±»
from segment_anything.modeling.sam import TextSam 
# ç¡®ä¿ prompt_generator.py å°±åœ¨æ ¹ç›®å½•ä¸‹ï¼Œä¸”åŒ…å« point_guidance_loss
from prompt_generator import point_guidance_loss 

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--work_dir", type=str, default="workdir", help="work dir")
    parser.add_argument("--run_name", type=str, default="text-guided-sam", help="run model name")
    parser.add_argument("--epochs", type=int, default=100, help="number of epochs")
    parser.add_argument("--batch_size", type=int, default=2, help="train batch size")
    parser.add_argument("--image_size", type=int, default=256, help="image_size")
    parser.add_argument("--mask_num", type=int, default=1, help="get mask number")
    parser.add_argument("--data_path", type=str, default="data/MoNuSeg_SA1B", help="train data path") 
    parser.add_argument("--attribute_info_path", type=str, default="data/MoNuSeg_SA1B/attribute_info_train.json", help="json path")
    parser.add_argument("--metrics", nargs='+', default=['dice', 'iou'], help="metrics")
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument("--lr", type=float, default=1e-4, help="learning rate")
    parser.add_argument("--resume", type=str, default=None, help="load resume") 
    parser.add_argument("--model_type", type=str, default="vit_b", help="sam model_type")
    # æ³¨æ„ï¼šè¿™é‡Œçš„ checkpoint æœ€å¥½æ˜¯å®˜æ–¹çš„ sam_vit_b_01ec64.pth æˆ– sam-med2d é¢„è®­ç»ƒæƒé‡
    parser.add_argument("--sam_checkpoint", type=str, default="workdir/models/sam-med2d_b.pth", help="sam checkpoint")
    parser.add_argument("--save_interval", type=int, default=10, help="save model interval (epochs)")
    parser.add_argument("--use_amp", action='store_true', help="use amp")
    
    # SAM-Med2D ç‰¹å®šå‚æ•°
    parser.add_argument("--encoder_adapter", action='store_true', default=True, help="use adapter in image encoder")
    
    args = parser.parse_args()
    return args

# =========================================================================================
# ğŸ› ï¸ [Helper] Generate Gaussian Heatmap from Mask
# =========================================================================================
def generate_gaussian_target(masks, shape=(64, 64), sigma=1.0, device='cuda'):
    """
    ä¸ºå‰æ™¯ï¼ˆç»†èƒæ ¸ï¼‰ç”Ÿæˆé«˜æ–¯çƒ­åŠ›å›¾
    Input: masks [B, 1, H, W] (åŸå§‹å°ºå¯¸æˆ– 256x256)
    Output: heatmap [B, 1, 64, 64]
    """
    B = len(masks)
    H_feat, W_feat = shape
    targets = torch.zeros(B, 1, H_feat, W_feat, device=device)
    
    y_grid, x_grid = torch.meshgrid(
        torch.arange(H_feat, device=device), 
        torch.arange(W_feat, device=device), 
        indexing='ij'
    )
    
    for i in range(B):
        mask = masks[i, 0] # [H, W]
        if mask.sum() == 0: continue 
        
        # 1. è®¡ç®—è´¨å¿ƒ
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ¯ä¸ª Instance å·²ç»è¢«åˆå¹¶æˆä¸€ä¸ª Binary Mask
        # å¦‚æœæ˜¯ Instance Segmentationï¼Œæœ€å¥½å…ˆç”¨ measure.label åˆ†ç¦»
        # è¿™é‡Œä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬ç›´æ¥å¯¹ Mask > 0 çš„åŒºåŸŸè®¡ç®—
        # (æ›´ä¸¥è°¨çš„åšæ³•æ˜¯ï¼šå¦‚æœ DataLoader æä¾›äº† instance maskï¼Œåº”è¯¥å¯¹æ¯ä¸ª instance ç”Ÿæˆé«˜æ–¯ï¼Œç„¶åå– max)
        
        # ç®€å•å¤„ç†ï¼šå°† Mask ç¼©æ”¾åˆ° 64x64ï¼Œç„¶åå¯¹æ¯ä¸ªåƒç´ ç‚¹åšé«˜æ–¯æ¨¡ç³Šï¼ˆæˆ–è€…æ˜¯ Distance Transformï¼‰
        # ä½†æœ€æ ‡å‡†çš„ CenterNet åšæ³•æ˜¯å¯¹æ¯ä¸ª Instance ä¸­å¿ƒç”»é«˜æ–¯ã€‚
        # æ—¢ç„¶ MoNuSeg æ˜¯å¯†é›†é¢„æµ‹ï¼Œæˆ‘ä»¬é‡‡ç”¨ "Downsampled Mask" ä½œä¸ºåŸºç¡€ç›‘ç£å¯èƒ½ä¼šæ›´ç¨³å¥ã€‚
        # -----------------------------------------------------------
        # æ–¹æ¡ˆ A: ä»…åœ¨è´¨å¿ƒç”»é«˜æ–¯ (é€‚åˆç¨€ç–ç›®æ ‡ï¼Œå¦‚æ£€æµ‹)
        # æ–¹æ¡ˆ B: ä½¿ç”¨ Distance Transform æˆ– ç¼©æ”¾åçš„ Mask (é€‚åˆå¯†é›†åˆ†å‰²)
        # è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨æ–¹æ¡ˆ A çš„å˜ä½“ï¼šå¯¹æ‰€æœ‰å‰æ™¯ç‚¹è®¡ç®—é«˜æ–¯å¯èƒ½ä¼šå¤ªæ…¢ã€‚
        # å»ºè®®ï¼šç›´æ¥å°† Mask ç¼©æ”¾åˆ° 64x64 ä½œä¸º "Dense Heatmap Target"
        # -----------------------------------------------------------
        
        # é‡æ–°å®ç°ï¼šç›´æ¥ç¼©æ”¾ Maskï¼Œç„¶ååšä¸€ç‚¹å¹³æ»‘ (å¯é€‰)
        # è¿™ç§æ–¹å¼å¯¹äºå½¢çŠ¶ä¸è§„åˆ™çš„ç»†èƒæ ¸æ›´é²æ£’
        mask_float = mask.float().unsqueeze(0).unsqueeze(0) # [1, 1, H, W]
        resized_mask = F.interpolate(mask_float, size=(H_feat, W_feat), mode='bilinear', align_corners=False)
        targets[i] = resized_mask.squeeze(0)
        
    return targets

def train_one_epoch(args, model, optimizer, train_loader, epoch, criterion, scaler=None):
    model.train()
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    losses = []
    mask_losses = []
    heatmap_losses = []
    
    for batch, batched_input in enumerate(pbar):
        # 1. Prepare Data
        images = batched_input['image'].to(args.device)     # [B, 3, 256, 256]
        labels = batched_input['label'].to(args.device)     # [B, 1, 256, 256]
        
        if labels.ndim == 3:
            labels = labels.unsqueeze(1) # Ensure [B, 1, H, W]

        # 2. Format Input for SAM
        sam_input = []
        for i in range(len(images)):
            sam_input.append({
                'image': images[i],
                'original_size': (args.image_size, args.image_size)
            })
            
        # 3. (åˆ é™¤) åŸæ¥çš„ GT Heatmap ç”Ÿæˆä»£ç å·²ç§»é™¤ï¼Œæ”¹ä¸ºåœ¨ Loss è®¡ç®—ä¸­åŠ¨æ€ç”Ÿæˆ

        optimizer.zero_grad()

        # 4. Loss Computation Function
        def compute_loss(outputs):
            loss_batch_sum = 0
            loss_m_val_sum = 0
            loss_h_val_sum = 0
            
            for i, out in enumerate(outputs):
                # --- A. Segmentation Mask Loss ---
                # å¤„ç† Mask ç»´åº¦ [1, C, H, W] æˆ– [C, H, W]
                if out['masks'].ndim == 4:
                    pred_mask = out['masks'][0, 0, :, :] # [H, W]
                else:
                    pred_mask = out['masks'][0, :, :] # [H, W]
                
                gt_mask = labels[i].float() # [1, H, W]

                # ç¡®ä¿ Mask å°ºå¯¸å¯¹é½ (GT -> Pred)
                if pred_mask.shape != gt_mask.shape[-2:]:
                    gt_mask = F.interpolate(
                        gt_mask.unsqueeze(0), 
                        size=pred_mask.shape, 
                        mode='nearest'
                    ).squeeze(0)

                # IoU Prediction
                pred_iou = out['iou_predictions'][0]
                
                # Mask Loss
                loss_m = criterion(
                    pred_mask.unsqueeze(0).unsqueeze(0), 
                    gt_mask.unsqueeze(0), 
                    pred_iou.unsqueeze(0)
                )
                
                # --- B. Heatmap Loss (åŠ¨æ€å°ºå¯¸é€‚é…) ---
                pred_heatmap = out['heatmap_logits'] # shape å¯èƒ½æ˜¯ [2, 16, 16] æˆ– [2, 64, 64]
                
                # è·å–æ¨¡å‹å®é™…è¾“å‡ºçš„ç‰¹å¾å›¾å°ºå¯¸
                current_feat_size = pred_heatmap.shape[-2:] # (H_feat, W_feat)
                
                # å®æ—¶ç”ŸæˆåŒ¹é…å°ºå¯¸çš„ GT
                # labels[i] æ˜¯ [1, 256, 256] -> æ’å€¼åˆ° [1, 16, 16]
                with torch.no_grad():
                    # å‰æ™¯ GT
                    gt_nuclei = F.interpolate(
                        labels[i].float().unsqueeze(0), # [1, 1, 256, 256]
                        size=current_feat_size, 
                        mode='nearest' # æˆ– 'bilinear'
                    ).squeeze(0) # [1, 16, 16]
                    
                    # èƒŒæ™¯ GT
                    gt_background = 1.0 - gt_nuclei

                # è®¡ç®— Loss
                # Channel 0 vs Nuclei
                loss_h_pos = point_guidance_loss(pred_heatmap[0:1].unsqueeze(0), gt_nuclei.unsqueeze(0))
                # Channel 1 vs Background
                loss_h_neg = point_guidance_loss(pred_heatmap[1:2].unsqueeze(0), gt_background.unsqueeze(0))
                
                loss_h = loss_h_pos + loss_h_neg
                
                # Total Loss (Mask Loss ä¸ºä¸»ï¼ŒHeatmap ä¸ºè¾…)
                loss_i = loss_m + 0.5 * loss_h
                
                loss_batch_sum += loss_i
                loss_m_val_sum += loss_m.item()
                loss_h_val_sum += loss_h.item()
            
            return loss_batch_sum / len(images), loss_m_val_sum, loss_h_val_sum

        # 5. Forward & Backward
        if args.use_amp and scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(sam_input, multimask_output=True)
                loss_batch, loss_m_val, loss_h_val = compute_loss(outputs)

            scaler.scale(loss_batch).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(sam_input, multimask_output=True)
            loss_batch, loss_m_val, loss_h_val = compute_loss(outputs)
            
            loss_batch.backward()
            optimizer.step()

        # Log
        losses.append(loss_batch.item())
        mask_losses.append(loss_m_val / len(images))
        heatmap_losses.append(loss_h_val / len(images))
        
        pbar.set_postfix(
            loss=f"{loss_batch.item():.4f}", 
            msk=f"{(loss_m_val / len(images)):.4f}", 
            ht=f"{(loss_h_val / len(images)):.4f}"
        )

    return np.mean(losses), np.mean(mask_losses), np.mean(heatmap_losses)
    model.train()
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    losses = []
    mask_losses = []
    heatmap_losses = []
    
    for batch, batched_input in enumerate(pbar):
        # 1. Prepare Data
        images = batched_input['image'].to(args.device)     # [B, 3, 256, 256]
        labels = batched_input['label'].to(args.device)     # [B, 1, 256, 256] or [B, 256, 256]
        
        if labels.ndim == 3:
            labels = labels.unsqueeze(1) # Ensure [B, 1, H, W]

        # 2. Format Input for SAM
        # TextSam ä¼šè‡ªåŠ¨åœ¨å†…éƒ¨ç”Ÿæˆ [Nuclei, Background] çš„æ–‡æœ¬ç‰¹å¾
        # æ‰€ä»¥è¿™é‡Œçš„ text_prompts å®é™…ä¸Šä¸ä¼šè¢« TextSam ç”¨åˆ°ï¼Œä½†ä¸ºäº†ä¿æŒæ¥å£å…¼å®¹ï¼Œæˆ‘ä»¬ä¼ ä¸ªç©ºçš„æˆ–è€…é»˜è®¤çš„
        sam_input = []
        for i in range(len(images)):
            sam_input.append({
                'image': images[i],
                'original_size': (args.image_size, args.image_size) # å‘Šè¯‰ SAM æœ€ç»ˆè¿˜åŸå›ä»€ä¹ˆå°ºå¯¸
            })
            
        # 3. Generate GT Heatmaps (ç”¨äºç›‘ç£ TextGuidedPointGenerator)
        with torch.no_grad():
            # A. å‰æ™¯ (Nuclei) GT: 64x64 çš„é«˜æ–¯å›¾æˆ–ç¼©æ”¾å›¾
            # å»ºè®®ï¼šå¯¹äºå¯†é›†åˆ†å‰²ï¼Œç›´æ¥ç”¨ Downsample çš„ Mask æ•ˆæœæ›´å¥½
            gt_nuclei_map = F.interpolate(labels.float(), size=(64, 64), mode='nearest') # [B, 1, 64, 64]
            
            # B. èƒŒæ™¯ (Background) GT: 1 - Nuclei
            gt_background_map = 1.0 - gt_nuclei_map # [B, 1, 64, 64]

        optimizer.zero_grad()

        # 4. Loss Computation Function
        def compute_loss(outputs):
            loss_batch_sum = 0
            loss_m_val_sum = 0
            loss_h_val_sum = 0
            
            for i, out in enumerate(outputs):
                # --- A. Segmentation Mask Loss ---
                # out['masks'] shape: [1, C, H, W] 
                # æˆ‘ä»¬å–ç¬¬ 0 ä¸ªé€šé“ (å¯¹åº”ç¬¬ 0 ä¸ª Mask)
                # è¿™ç§å†™æ³•å…¼å®¹ [1, C, H, W] å’Œ [C, H, W]
                if out['masks'].ndim == 4:
                    pred_mask = out['masks'][0, 0, :, :] # [H, W]
                else:
                    pred_mask = out['masks'][0, :, :] # [H, W]
                
                gt_mask = labels[i].float() # [1, H, W]

                # ç¡®ä¿ GT å’Œ Pred å°ºå¯¸å¯¹é½
                # pred_mask æ˜¯ [H, W]ï¼Œgt_mask æ˜¯ [1, H, W]
                if pred_mask.shape != gt_mask.shape[-2:]:
                    gt_mask = F.interpolate(
                        gt_mask.unsqueeze(0), # [1, 1, H, W]
                        size=pred_mask.shape, 
                        mode='nearest'
                    ).squeeze(0) # [1, H, W]

                # --- B. IoU Prediction ---
                # âŒ é”™è¯¯åŸå› ï¼šout['iou_predictions'] æ˜¯ 1D Tensor [C]ï¼Œæ£€æŸ¥ shape[1] ä¼šæŠ¥é”™
                # âœ… ä¿®å¤ï¼šç›´æ¥å–ç¬¬ 0 ä¸ªå€¼å³å¯
                pred_iou = out['iou_predictions'][0]
                
                # --- C. Calc Loss ---
                # criterion éœ€è¦ [B, 1, H, W] æ ¼å¼
                loss_m = criterion(
                    pred_mask.unsqueeze(0).unsqueeze(0), # [1, 1, H, W]
                    gt_mask.unsqueeze(0),                # [1, 1, H, W] (gt_maskå·²ç»æ˜¯[1,H,W])
                    pred_iou.unsqueeze(0)                # [1]
                )
                
                # --- D. Heatmap Loss (åŒå‘ç›‘ç£) ---
                pred_heatmap = out['heatmap_logits'] # [2, 64, 64]
                
                # å‰æ™¯ Loss (Channel 0 vs GT Nuclei)
                loss_h_pos = point_guidance_loss(pred_heatmap[0:1].unsqueeze(0), gt_nuclei_map[i].unsqueeze(0))
                
                # èƒŒæ™¯ Loss (Channel 1 vs GT Background)
                loss_h_neg = point_guidance_loss(pred_heatmap[1:2].unsqueeze(0), gt_background_map[i].unsqueeze(0))
                
                loss_h = loss_h_pos + loss_h_neg
                
                # Total Loss
                loss_i = loss_m + 0.5 * loss_h
                
                loss_batch_sum += loss_i
                loss_m_val_sum += loss_m.item()
                loss_h_val_sum += loss_h.item()
            
            return loss_batch_sum / len(images), loss_m_val_sum, loss_h_val_sum
        # 5. Forward & Backward
        if args.use_amp and scaler is not None:
            with torch.cuda.amp.autocast():
                outputs = model(sam_input, multimask_output=True)
                loss_batch, loss_m_val, loss_h_val = compute_loss(outputs)

            scaler.scale(loss_batch).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            outputs = model(sam_input, multimask_output=True)
            loss_batch, loss_m_val, loss_h_val = compute_loss(outputs)
            
            loss_batch.backward()
            optimizer.step()

        # Log
        losses.append(loss_batch.item())
        mask_losses.append(loss_m_val / len(images))
        heatmap_losses.append(loss_h_val / len(images))
        
        pbar.set_postfix(
            loss=f"{loss_batch.item():.4f}", 
            msk=f"{(loss_m_val / len(images)):.4f}", 
            ht=f"{(loss_h_val / len(images)):.4f}"
        )

    return np.mean(losses), np.mean(mask_losses), np.mean(heatmap_losses)

def main(args):
    # Init Loggers
    os.makedirs(os.path.join(args.work_dir, "models", args.run_name), exist_ok=True)
    os.makedirs(os.path.join(args.work_dir, "logs"), exist_ok=True)
    logger = get_logger(os.path.join(args.work_dir, "logs", f"{args.run_name}_{datetime.datetime.now().strftime('%Y%m%d-%H%M')}.log"))
    
    logger.info(f"Args: {args}")

    # 1. Dataset
    train_dataset = TrainingDataset(
        args.data_path, 
        image_size=args.image_size, 
        mode='train', 
        point_num=1, 
        mask_num=args.mask_num, 
        requires_name=True, 
        attribute_info_path=args.attribute_info_path 
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=4, 
        collate_fn=stack_dict_batched,
        pin_memory=True
    )
    logger.info(f"Train data size: {len(train_dataset)}")

    # 2. Build TextSam Model
    logger.info("Building TextSam model...")
    
    # === A. åˆå§‹åŒ–åŸºç¡€ SAM (SAM-Med2D) ===
    # è¿™é‡Œä½¿ç”¨ sam_model_registry åŠ è½½åŸºç¡€ç»“æ„
    # args åŒ…å«äº† encoder_adapterï¼Œå¦‚æœæ˜¯ sam-med2d çš„ä»£ç åº“ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç† Adapter
    vanilla_sam = sam_model_registry[args.model_type](args)

    # === B. åŠ è½½é¢„è®­ç»ƒæƒé‡ ===
    if args.sam_checkpoint and os.path.exists(args.sam_checkpoint):
        logger.info(f"Loading checkpoint from {args.sam_checkpoint}...")
        try:
            checkpoint = torch.load(args.sam_checkpoint, map_location=args.device,weights_only=False)
            # å…¼å®¹æ€§å¤„ç†ï¼šæœ‰äº›æƒé‡åœ¨ 'model' key ä¸‹ï¼Œæœ‰äº›ç›´æ¥æ˜¯ dict
            state_dict = checkpoint.get("model", checkpoint)
            
            # ä½¿ç”¨ strict=Falseï¼Œå› ä¸º args.encoder_adapter å¯èƒ½ä¼šæ”¹å˜æ¨¡å‹ç»“æ„
            keys = vanilla_sam.load_state_dict(state_dict, strict=False)
            logger.info(f"Checkpoint loaded. Missing keys: {len(keys.missing_keys)}, Unexpected keys: {len(keys.unexpected_keys)}")
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            logger.info("Continuing with random initialization for missing parts...")
    else:
        logger.warning(f"Checkpoint path {args.sam_checkpoint} does not exist. Training from scratch (not recommended).")

    # === C. æ„å»º TextSam å¹¶è¿ç§»æƒé‡ ===
    # æˆ‘ä»¬å°† vanilla_sam çš„ encoder/decoder ä¼ ç»™ TextSam
    model = TextSam(
        image_encoder=vanilla_sam.image_encoder,
        prompt_encoder=vanilla_sam.prompt_encoder,
        mask_decoder=vanilla_sam.mask_decoder,
        pixel_mean=vanilla_sam.pixel_mean,
        pixel_std=vanilla_sam.pixel_std,
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256
    ).to(args.device)
    
    # é‡Šæ”¾ vanilla_sam èŠ‚çœå†…å­˜
    del vanilla_sam 

    # 3. Optimizer
    # è¿™é‡Œçš„å­¦ä¹ ç‡è®¾ç½®å¾ˆé‡è¦ï¼š
    # Mask Decoder éœ€è¦å¾®è°ƒ
    # Prompt Generator æ˜¯å…¨æ–°å±‚ï¼Œå»ºè®® LR å¤§ä¸€ç‚¹ (x10)
    # Image Encoder å¦‚æœä½¿ç”¨äº† Adapterï¼Œä¹Ÿéœ€è¦æ”¾å…¥ä¼˜åŒ–å™¨
    
    params_to_optimize = [
        {'params': model.mask_decoder.parameters(), 'lr': args.lr},
        {'params': model.prompt_generator.parameters(), 'lr': args.lr * 10}
    ]
    
    # å¦‚æœå¯ç”¨äº† Adapterï¼ŒImage Encoder ä¹Ÿæ˜¯å¯è®­ç»ƒçš„
    if args.encoder_adapter:
        # å‡è®¾ Image Encoder é‡Œæœ‰ adapter ç›¸å…³çš„å‚æ•°
        # ç®€å•çš„åšæ³•æ˜¯æŠŠ image_encoder åŠ å…¥ï¼Œä½†åªè®­ç»ƒ requires_grad=True çš„éƒ¨åˆ†
        params_to_optimize.append({'params': filter(lambda p: p.requires_grad, model.image_encoder.parameters()), 'lr': args.lr})

    optimizer = optim.AdamW(params_to_optimize, weight_decay=1e-4)

    criterion = FocalDiceloss_IoULoss()
    scaler = torch.cuda.amp.GradScaler() if args.use_amp else None

    # 4. Training Loop
    best_loss = 1e10
    start_epoch = 0
    
    # Resume Logic
    if args.resume and os.path.exists(args.resume):
        logger.info(f"Resuming from {args.resume}")
        checkpoint = torch.load(args.resume, map_location=args.device,weights_only=False)
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        start_epoch = checkpoint['epoch']
        if 'best_loss' in checkpoint:
            best_loss = checkpoint['best_loss']

    logger.info("Start Training...")
    for epoch in range(start_epoch, args.epochs):
        avg_loss, avg_msk, avg_ht = train_one_epoch(args, model, optimizer, train_loader, epoch, criterion, scaler)
        
        logger.info(f"Epoch {epoch+1}/{args.epochs} | Loss: {avg_loss:.4f} (Mask: {avg_msk:.4f}, Heatmap: {avg_ht:.4f})")
        
        # Save Best
        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = os.path.join(args.work_dir, "models", args.run_name, "best_model.pth")
            torch.save({
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'epoch': epoch + 1,
                'best_loss': best_loss
            }, save_path)
            logger.info(f"Saved Best Model (Loss: {best_loss:.4f})")
            
        # Regular Save
        if (epoch + 1) % args.save_interval == 0:
            save_path = os.path.join(args.work_dir, "models", args.run_name, f"epoch_{epoch+1}.pth")
            torch.save({
                'model': model.state_dict(), 
                'optimizer': optimizer.state_dict(),
                'epoch': epoch+1
            }, save_path)

if __name__ == '__main__':
    args = parse_args()
    main(args)