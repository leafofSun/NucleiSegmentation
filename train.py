import argparse
import os
import time
import datetime
import random
import numpy as np
from tqdm import tqdm
import logging

import torch
import torch.nn as nn
import torch.optim as optim
from torch.nn import functional as F
from torch.utils.data import DataLoader

# Native AMP support
try:
    from torch.amp import autocast, GradScaler 
except ImportError:
    from torch.cuda.amp import autocast, GradScaler

# === é¡¹ç›®æ¨¡å—å¯¼å…¥ ===
from segment_anything import sam_model_registry
from segment_anything.modeling.sam import TextSam 
from DataLoader import TrainingDataset, stack_dict_batched

# ğŸ”¥ æ ¸å¿ƒå·¥å…·å¯¼å…¥ (Loss å’Œ Logger)
from utils import FocalDiceloss_IoULoss, point_guidance_loss, get_logger

# ğŸ”¥ é«˜æ€§èƒ½æŒ‡æ ‡å¯¼å…¥ (AJI, PQ, DQ, SQ)
from metrics import SegMetrics

# ==================================================================================================
# 1. å‚æ•°é…ç½® (Configuration)
# ==================================================================================================
def parse_args():
    parser = argparse.ArgumentParser(description="MP-SAM: Explicit-Implicit Dual-Stream Training")
    
    # --- åŸºç¡€ç¯å¢ƒ ---
    parser.add_argument("--work_dir", type=str, default="workdir", help="Directory to save logs and models")
    parser.add_argument("--run_name", type=str, default="mp_sam_monuseg_final", help="Experiment name")
    parser.add_argument("--seed", type=int, default=42, help="Random seed for reproducibility")
    parser.add_argument('--device', type=str, default='cuda', help="Device to use (cuda/cpu)")
    
    # --- æ•°æ®è·¯å¾„ (å®Œå…¨è§£è€¦) ---
    parser.add_argument("--data_path", type=str, default="data/MoNuSeg_SA1B", help="Root directory of dataset")
    parser.add_argument("--knowledge_path", type=str, default="data/MoNuSeg_SA1B/medical_knowledge.json", 
                        help="Path to the generated Explicit Knowledge Base JSON")
    
    # --- å›¾åƒå‚æ•° ---
    parser.add_argument("--image_size", type=int, default=1024, help="Input image resolution")
    parser.add_argument("--crop_size", type=int, default=1024, help="Random crop size during training")
    parser.add_argument("--mask_num", type=int, default=1, help="Number of masks per proposal")

    # --- æ¨¡å‹é…ç½® ---
    parser.add_argument("--model_type", type=str, default="vit_b", choices=["vit_b", "vit_l", "vit_h"], help="SAM backbone type")
    parser.add_argument("--sam_checkpoint", type=str, default="workdir/models/sam-med2d_b.pth", help="Path to original/medsam checkpoint")
    parser.add_argument("--clip_model", type=str, default="ViT-B/16", help="CLIP model version for Text Encoder")
    parser.add_argument("--num_organs", type=int, default=10, help="Number of organ categories for DualPromptLearner")
    parser.add_argument("--encoder_adapter", action='store_true', default=True, help="Use Adapters in Image Encoder")

    # --- è®­ç»ƒè¶…å‚ ---
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--lr", type=float, default=1e-4, help="Base learning rate")
    parser.add_argument("--min_lr", type=float, default=1e-6, help="Minimum learning rate for scheduler")
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--use_amp", action='store_true', default=True, help="Use Automatic Mixed Precision")
    
    # --- Loss æƒé‡ ---
    parser.add_argument("--mask_weight", type=float, default=2.0, help="Weight for Segmentation Loss (Focal+Dice)")
    parser.add_argument("--heatmap_weight", type=float, default=1.0, help="Weight for Auto-Prompt Heatmap Loss")

    # --- éªŒè¯æŒ‡æ ‡ (PromptNu æ ‡å‡†) ---
    parser.add_argument("--metrics", nargs='+', default=['dice', 'iou', 'mAJI', 'mPQ', 'mDQ', 'mSQ'], 
                        help="Metrics to evaluate: dice, iou, mAJI, mPQ, mDQ, mSQ")

    return parser.parse_args()

# ==================================================================================================
# 2. è¾…åŠ©å‡½æ•° (Utils)
# ==================================================================================================
def setup_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def to_device(batch_input, device):
    device_input = {}
    for key, value in batch_input.items():
        if value is not None:
            if isinstance(value, torch.Tensor):
                device_input[key] = value.to(device)
            elif isinstance(value, list):
                device_input[key] = value
            else:
                device_input[key] = value
    return device_input

def resize_pos_embed(state_dict, model_state_dict):
    """è°ƒæ•´ SAM ä½ç½®ç¼–ç å°ºå¯¸"""
    new_state_dict = {}
    for k, v in state_dict.items():
        if k in model_state_dict:
            if v.shape != model_state_dict[k].shape:
                if 'pos_embed' in k:
                    v = v.permute(0, 3, 1, 2)
                    v = F.interpolate(v, size=model_state_dict[k].shape[1:3], mode='bicubic', align_corners=False)
                    v = v.permute(0, 2, 3, 1)
                elif 'rel_pos' in k:
                    v = v.unsqueeze(0).permute(0, 2, 1)
                    target_len = model_state_dict[k].shape[0]
                    v = F.interpolate(v, size=target_len, mode='linear', align_corners=False)
                    v = v.permute(0, 2, 1).squeeze(0)
            new_state_dict[k] = v
        else:
            new_state_dict[k] = v
    return new_state_dict

# ==================================================================================================
# 3. è®­ç»ƒé€»è¾‘ (Train Loop)
# ==================================================================================================
def train_one_epoch(args, model, optimizer, train_loader, epoch, criterion, scaler):
    model.train()
    pbar = tqdm(train_loader, desc=f"Ep {epoch+1} Train")
    
    losses = []
    mask_losses = []
    heatmap_losses = []
    
    for batch, batched_input in enumerate(pbar):
        batched_input = to_device(batched_input, args.device)
        images = batched_input['image']
        labels = batched_input['label']
        
        optimizer.zero_grad()

        # === ğŸ”¥ [æ ¸å¿ƒ] æ„å»º MP-SAM æ•°æ®æµ (Knowledge Injection) ğŸ”¥ ===
        model_input = []
        organ_ids = batched_input.get('organ_id', None) # éšå¼æµ
        attr_texts = batched_input.get('attribute_text', ["Cell nuclei"] * len(images)) # æ˜¾å¼æµ
        base_texts = batched_input.get('text_prompt', ["Cell nuclei"] * len(images))

        for i in range(len(images)):
            model_input.append({
                'image': images[i],
                'original_size': (args.image_size, args.image_size),
                # æ³¨å…¥ MP-SAM å­—æ®µ
                'organ_id': organ_ids[i] if organ_ids is not None else 9, # 9=Generic fallback
                'attribute_text': attr_texts[i],
                'text_prompt': base_texts[i]
            })

        # === Forward Pass (AMP) ===
        with autocast('cuda', enabled=args.use_amp):
            # TextSam.forward å†…éƒ¨ä¼šè‡ªåŠ¨åˆ†å‘ organ_id å’Œ attribute_text åˆ°å¯¹åº”æ¨¡å—
            outputs = model(model_input, multimask_output=True)
            
            loss_batch = 0
            loss_m_accum = 0
            loss_h_accum = 0
            
            for i, out in enumerate(outputs):
                # --- A. Mask Loss Calculation ---
                iou_preds = out['iou_predictions']
                if iou_preds.ndim == 2: iou_preds = iou_preds.squeeze(0)
                
                # é€‰å– IoU é¢„æµ‹æœ€é«˜çš„ Mask è®¡ç®— Loss
                best_idx = torch.argmax(iou_preds).item()
                
                # å¤„ç† Mask ç»´åº¦ [1, 3, H, W] or [3, H, W]
                pred_mask = out['masks'][best_idx, :, :] if out['masks'].ndim==3 else out['masks'][0, best_idx]
                pred_iou = iou_preds[best_idx]
                
                gt_mask = labels[i].squeeze(0).float() # [H, W]
                
                # å°ºå¯¸å¯¹é½
                if pred_mask.shape != gt_mask.shape:
                     gt_mask = F.interpolate(gt_mask.unsqueeze(0).unsqueeze(0), size=pred_mask.shape, mode='nearest').squeeze()

                # ğŸ”¥ ä½¿ç”¨ utils.py ä¸­çš„ Loss è®¡ç®— (è‡ªåŠ¨å¤„ç† ignore_index=255)
                loss_m, _ = criterion(pred_mask.unsqueeze(0).unsqueeze(0), gt_mask.unsqueeze(0).unsqueeze(0), pred_iou.unsqueeze(0))
                
                # --- B. Heatmap Loss Calculation (Auto-Point Supervision) ---
                pred_heatmap = out['heatmap_logits'] # [1, H_feat, W_feat]
                
                with torch.no_grad():
                    # å°† GT ç¼©æ”¾åˆ° Feature Map å°ºå¯¸ç”Ÿæˆç›‘ç£ä¿¡å·
                    target_mask = labels[i].float().unsqueeze(0)
                    gt_nuclei = F.interpolate(target_mask, size=pred_heatmap.shape[-2:], mode='nearest').squeeze(0)
                    gt_nuclei[gt_nuclei==255] = 0 # å¿½ç•¥åŒºåŸŸè®¾ä¸ºèƒŒæ™¯
                
                # ğŸ”¥ ä½¿ç”¨ utils.py ä¸­çš„ point_guidance_loss
                loss_h = point_guidance_loss(pred_heatmap, gt_nuclei.unsqueeze(0))
                
                # --- C. Weighted Sum ---
                loss_i = args.mask_weight * loss_m + args.heatmap_weight * loss_h
                
                loss_batch += loss_i
                loss_m_accum += loss_m.item()
                loss_h_accum += loss_h.item()
            
            final_loss = loss_batch / len(images)

        # === Backward ===
        if scaler:
            scaler.scale(final_loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            final_loss.backward()
            optimizer.step()

        losses.append(final_loss.item())
        mask_losses.append(loss_m_accum / len(images))
        heatmap_losses.append(loss_h_accum / len(images))
        
        # æ˜¾ç¤ºå½“å‰ prompt
        prompt_preview = attr_texts[0][:15] + ".." if len(attr_texts[0]) > 15 else attr_texts[0]
        pbar.set_postfix(Loss=f"{final_loss.item():.3f}", Prompt=prompt_preview)

    return np.mean(losses), np.mean(mask_losses), np.mean(heatmap_losses)

# ==================================================================================================
# 4. éªŒè¯é€»è¾‘ (Val Loop)
# ==================================================================================================
@torch.no_grad()
def validate_one_epoch(args, model, val_loader, epoch):
    model.eval()
    
    # åŠ¨æ€åˆå§‹åŒ–æ‰€æœ‰æŒ‡æ ‡åˆ—è¡¨
    val_results = {k: [] for k in args.metrics} 
    
    pbar = tqdm(val_loader, desc=f"Ep {epoch+1} Val")
    
    for batch, batched_input in enumerate(pbar):
        batched_input = to_device(batched_input, args.device)
        images = batched_input['image']
        labels = batched_input['label'].cpu().numpy()
        
        # æ„å»ºéªŒè¯è¾“å…¥
        model_input = []
        organ_ids = batched_input.get('organ_id', None)
        attr_texts = batched_input.get('attribute_text', ["Cell nuclei"] * len(images))
        
        for i in range(len(images)):
            model_input.append({
                'image': images[i],
                'original_size': (args.image_size, args.image_size),
                'organ_id': organ_ids[i] if organ_ids is not None else 9,
                'attribute_text': attr_texts[i],
                'text_prompt': "Cell nuclei"
            })
            
        outputs = model(model_input, multimask_output=True)
        
        for i, out in enumerate(outputs):
            iou_preds = out['iou_predictions']
            best_idx = torch.argmax(iou_preds).item()
            
            # è·å–é¢„æµ‹ Mask (0/1)
            # æ³¨æ„ï¼šMetrics ä¸­ä¼šè‡ªåŠ¨å¤„ç†äºŒå€¼->å®ä¾‹ (label)ï¼Œæ‰€ä»¥è¿™é‡Œåªéœ€ç»™æ¦‚ç‡å›¾æˆ–äºŒå€¼å›¾
            pred_logits = out['masks'][0, best_idx]
            pred_mask = (torch.sigmoid(pred_logits).cpu().numpy() > 0.5).astype(np.uint8)
            
            gt = labels[i]
            if gt.ndim == 3: gt = gt[0]
            
            # å¤„ç† Ignore åŒºåŸŸ (Metrics å‡è®¾ 0 ä¸ºèƒŒæ™¯)
            gt_valid = gt.copy()
            gt_valid[gt == 255] = 0
            
            # ğŸ”¥ [å…³é”®] è®¡ç®— SegMetrics (å« AJI, PQ, DQ, SQ)
            # metrics.py ä¼šè‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦è½¬å®ä¾‹
            res = SegMetrics(pred_mask, gt_valid, args.metrics)
            
            for k in args.metrics:
                if k in res:
                    val_results[k].append(res[k])
        
        # å®æ—¶æ˜¾ç¤º AJIï¼Œå› ä¸ºå®ƒæ˜¯æœ€é‡è¦çš„å®ä¾‹æŒ‡æ ‡
        if 'mAJI' in args.metrics and len(val_results['mAJI']) > 0:
            pbar.set_postfix(AJI=f"{val_results['mAJI'][-1]:.3f}")
                
    # è®¡ç®—å¹³å‡å€¼
    avg_results = {k: np.mean(v) if len(v) > 0 else 0.0 for k, v in val_results.items()}
    return avg_results

# ==================================================================================================
# 5. ä¸»ç¨‹åº (Main)
# ==================================================================================================
def main(args):
    setup_seed(args.seed)
    
    # --- æ—¥å¿—è®¾ç½® ---
    os.makedirs(os.path.join(args.work_dir, "models", args.run_name), exist_ok=True)
    os.makedirs(os.path.join(args.work_dir, "logs"), exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d-%H%M')
    logger = get_logger(os.path.join(args.work_dir, "logs", f"{args.run_name}_{timestamp}.log"))
    logger.info(f"ğŸš€ [Start] MP-SAM Training | Device: {args.device}")
    logger.info(f"ğŸ“ Data: {args.data_path}")
    logger.info(f"ğŸ§  Knowledge Base: {args.knowledge_path}")

    # --- æ•°æ®åŠ è½½ ---
    train_dataset = TrainingDataset(
        os.path.join(args.data_path, "train"),
        knowledge_path=args.knowledge_path,
        image_size=args.image_size, crop_size=args.crop_size, mode='train'
    )
    val_dataset = TrainingDataset(
        os.path.join(args.data_path, "test"),
        knowledge_path=args.knowledge_path,
        image_size=args.image_size, crop_size=args.crop_size, mode='test'
    )
    
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, 
                              num_workers=4, collate_fn=stack_dict_batched, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, 
                            num_workers=2, collate_fn=stack_dict_batched, pin_memory=True)
    
    logger.info(f"ğŸ“Š Train Size: {len(train_dataset)} | Val Size: {len(val_dataset)}")

    # --- æ¨¡å‹æ„å»º ---
    logger.info(f"ğŸ—ï¸ Building TextSam (Organs={args.num_organs})...")
    args.checkpoint = args.sam_checkpoint
    # 1. åŠ è½½ Vanilla SAM
    vanilla_sam = sam_model_registry[args.model_type](args)
    if os.path.exists(args.sam_checkpoint):
        logger.info(f"ğŸ“¥ Loading checkpoint: {args.sam_checkpoint}")
        try:
            ckpt = torch.load(args.sam_checkpoint, map_location='cpu',weights_only=False)
            state_dict = ckpt.get("model", ckpt)
            state_dict = resize_pos_embed(state_dict, vanilla_sam.state_dict())
            vanilla_sam.load_state_dict(state_dict, strict=False)
        except Exception as e:
            logger.warning(f"âš ï¸ Checkpoint loading failed: {e}. Using random init.")
    
    # 2. æ„å»º MP-SAM
    model = TextSam(
        image_encoder=vanilla_sam.image_encoder,
        prompt_encoder=vanilla_sam.prompt_encoder,
        mask_decoder=vanilla_sam.mask_decoder,
        clip_model_name=args.clip_model,
        num_organs=args.num_organs
    ).to(args.device)
    
    del vanilla_sam

    # --- Adapter åˆå§‹åŒ– ---
    if args.encoder_adapter:
        logger.info("ğŸ§¹ Resetting Adapter weights to Zero...")
        for n, p in model.image_encoder.named_parameters():
            if "Adapter" in n and "weight" in n:
                torch.nn.init.zeros_(p)

    # --- ä¼˜åŒ–å™¨é…ç½® (è‡ªåŠ¨æ¨¡å—å‘ç°) ---
    params = [
        {'params': model.mask_decoder.parameters(), 'lr': args.lr},
        {'params': model.prompt_generator.parameters(), 'lr': args.lr * 5}
    ]
    
    # è‡ªåŠ¨å‘ç° DualPromptLearner
    if hasattr(model, 'prompt_learner'):
        logger.info(f"âœ¨ Optimizing DualPromptLearner (Implicit Stream)")
        params.append({'params': model.prompt_learner.parameters(), 'lr': args.lr})
        
    # è‡ªåŠ¨å‘ç° KIM/PNuRL
    if hasattr(model, 'pnurl'): 
        logger.info(f"âœ¨ Optimizing KIM/PNuRL (Explicit Stream)")
        params.append({'params': model.pnurl.parameters(), 'lr': args.lr})
    elif hasattr(model, 'kim'):
        logger.info(f"âœ¨ Optimizing KIM (Explicit Stream)")
        params.append({'params': model.kim.parameters(), 'lr': args.lr})

    # Adapter
    adapter_params = [p for n, p in model.image_encoder.named_parameters() if "Adapter" in n and p.requires_grad]
    if adapter_params:
        logger.info(f"âœ¨ Optimizing Adapters ({len(adapter_params)} tensors)")
        params.append({'params': adapter_params, 'lr': args.lr})

    optimizer = optim.AdamW(params, weight_decay=args.weight_decay)
    
    # ğŸ”¥ åˆå§‹åŒ– Loss (Utils)
    criterion = FocalDiceloss_IoULoss(weight=20.0, iou_scale=1.0, ignore_index=255)
    
    scaler = GradScaler() if args.use_amp else None
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=args.min_lr)

    # --- è®­ç»ƒå¾ªç¯ ---
    best_dice = 0.0
    best_aji = 0.0
    start_time = time.time()
    
    for epoch in range(args.epochs):
        # 1. Train
        loss, m_loss, h_loss = train_one_epoch(args, model, optimizer, train_loader, epoch, criterion, scaler)
        
        # 2. Val
        val_res = validate_one_epoch(args, model, val_loader, epoch)
        
        dice = val_res.get('dice', 0.0)
        aji = val_res.get('mAJI', 0.0)
        pq = val_res.get('mPQ', 0.0)
        
        # 3. Log
        logger.info(
            f"Ep {epoch+1}/{args.epochs} | "
            f"Loss: {loss:.4f} (M:{m_loss:.3f}, H:{h_loss:.3f}) | "
            f"Dice: {dice:.4f} | AJI: {aji:.4f} | PQ: {pq:.4f}"
        )
        
        # 4. Save Best (é€šå¸¸ AJI æ˜¯å®ä¾‹åˆ†å‰²çš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œå¯ä»¥æŒ‰ AJI æˆ– Dice ä¿å­˜)
        if aji > best_aji:
            best_aji = aji
            torch.save(model.state_dict(), os.path.join(args.work_dir, "models", args.run_name, "best_model.pth"))
            logger.info(f"â­ Best Model Saved (AJI: {best_aji:.4f})")
        
        # 5. Scheduler Step
        scheduler.step()
        
        # å®šæœŸä¿å­˜
        if (epoch + 1) % 10 == 0:
            torch.save(model.state_dict(), os.path.join(args.work_dir, "models", args.run_name, f"epoch_{epoch+1}.pth"))

    total_time = time.time() - start_time
    logger.info(f"ğŸ Training Finished. Total time: {datetime.timedelta(seconds=int(total_time))}")

if __name__ == '__main__':
    args = parse_args()
    main(args)