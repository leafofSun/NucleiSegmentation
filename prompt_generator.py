import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from scipy.spatial import KDTree

class TextGuidedPointGenerator(nn.Module):
    def __init__(self, embed_dim=256, text_dim=512):
        super().__init__()
        # 1. æ–‡æœ¬æŠ•å½±å±‚
        self.text_proj = nn.Linear(text_dim, embed_dim)
        
        # 2. å›¾åƒå·ç§¯å±‚ (æå–å±€éƒ¨ç‰¹å¾)
        self.img_conv = nn.Sequential(
            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1),
            nn.BatchNorm2d(embed_dim),
            nn.ReLU(inplace=True)
        )
        
        # 3. Logit Scale (ç”¨äºæ”¾å¤§ç›¸ä¼¼åº¦ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±)
        # åˆå§‹åŒ–ä¸º log(1/0.07) â‰ˆ 2.65
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def forward(self, image_embeddings, text_embeddings):
        """
        è¾“å…¥:
            image_embeddings: [B, 256, 64, 64] (SAM Image Encoder è¾“å‡º)
            text_embeddings:  [B, N_Classes, 512] (CLIP Text Encoder è¾“å‡º)
        è¾“å‡º:
            heatmap_logits:   [B, N_Classes, 64, 64]
        """
        B, C, H, W = image_embeddings.shape
        _, N_Classes, _ = text_embeddings.shape 
        
        # ç‰¹å¾æå–ä¸å½’ä¸€åŒ–
        img_feat = self.img_conv(image_embeddings) 
        txt_feat = self.text_proj(text_embeddings)
        
        img_feat = F.normalize(img_feat, dim=1)      # [B, 256, 64, 64]
        txt_feat = F.normalize(txt_feat, dim=-1)     # [B, N, 256]

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (Attention Map)
        img_flat = img_feat.view(B, C, -1)           # [B, 256, 4096]
        match_score = torch.bmm(txt_feat, img_flat)  # [B, N, 4096]
        
        # ç¼©æ”¾ Logits
        logit_scale = self.logit_scale.exp().clamp(max=100)
        match_score = match_score * logit_scale
        
        heatmap_logits = match_score.view(B, N_Classes, H, W)
        return heatmap_logits

    @torch.no_grad()
    def generate_adaptive_prompts(self, heatmap_logits, threshold=0.3, k_neighbors=3, dense_dist_thresh=15.0):
        """
        ğŸ”¥ [æ ¸å¿ƒåŠŸèƒ½] å¯†åº¦è‡ªé€‚åº” + é‚»åŸŸè´Ÿæç¤ºé‡‡æ · (Density-Adaptive Sampling)
        
        ç­–ç•¥ï¼š
        1. ç¨€ç–åŒº: [1 æ­£æç¤º] (ç›¸ä¿¡ SAM çš„æ³›åŒ–èƒ½åŠ›)
        2. æ‹¥æŒ¤åŒº: [1 æ­£æç¤º + K è´Ÿæç¤º] (åˆ©ç”¨é‚»å±…ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œåˆ‡æ–­ç²˜è¿)
        
        Args:
            heatmap_logits: [B, C, H, W]
            threshold: çƒ­åŠ›å›¾é˜ˆå€¼
            k_neighbors: æœ€å¤šå–å¤šå°‘ä¸ªé‚»å±…ä½œä¸ºè´Ÿæç¤º
            dense_dist_thresh: åˆ¤å®šä¸ºâ€œæ‹¥æŒ¤â€çš„è·ç¦»é˜ˆå€¼ (åƒç´ )
            
        Returns:
            prompts_list: List[Dict], é•¿åº¦ä¸º Bã€‚
                          æ¯ä¸ªå…ƒç´ åŒ…å«:
                          - 'point_coords': [N_cells, K+1, 2]
                          - 'point_labels': [N_cells, K+1]
        """
        B, C, H, W = heatmap_logits.shape
        device = heatmap_logits.device
        
        # 1. å½’ä¸€åŒ–å¹¶è¿›è¡Œ NMS (éæå¤§å€¼æŠ‘åˆ¶)ï¼Œæå–å³°å€¼ç‚¹
        scores = torch.sigmoid(heatmap_logits)
        # MaxPool åš NMS (çª—å£å¤§å° 5x5)
        local_max = F.max_pool2d(scores, kernel_size=5, stride=1, padding=2)
        is_local_max = (scores == local_max) & (scores > threshold)
        
        batch_prompts = []

        for b in range(B):
            # è·å–å½“å‰å›¾çš„æ‰€æœ‰å‰æ™¯ç‚¹ (å‡è®¾ Channel 0 æ˜¯ Nuclei å‰æ™¯)
            fg_map = is_local_max[b, 0] 
            y_inds, x_inds = torch.where(fg_map)
            
            # === æƒ…å†µ A: å›¾ä¸­æ— ç»†èƒ ===
            if len(y_inds) == 0:
                # è¿”å›ç©º tensor é˜²æ­¢æŠ¥é”™
                batch_prompts.append({
                    "point_coords": torch.empty((0, k_neighbors + 1, 2), device=device),
                    "point_labels": torch.empty((0, k_neighbors + 1), device=device),
                    "has_points": False
                })
                continue
                
            # æ„å»ºåæ ‡æ•°ç»„ [N, 2] (x, y) - æ³¨æ„ SAM éœ€è¦ (x, y) æ ¼å¼
            points_np = torch.stack([x_inds.float(), y_inds.float()], dim=1).cpu().numpy()
            num_points = len(points_np)
            
            # === æ„å»º KDTree æŸ¥æ‰¾é‚»å±… ===
            dists, indices = None, None
            if num_points > 1:
                tree = KDTree(points_np)
                # æŸ¥è¯¢æœ€è¿‘çš„ k+1 ä¸ªç‚¹ (ç¬¬1ä¸ªæ˜¯è‡ªå·±ï¼Œåkä¸ªæ˜¯é‚»å±…)
                k_query = min(num_points, k_neighbors + 1)
                dists, indices = tree.query(points_np, k=k_query)

            # === æ„é€  Prompt (N ä¸ªç»†èƒï¼Œæ¯ä¸ªç»†èƒæœ‰ä¸€ç»„ Points) ===
            image_point_coords = []
            image_point_labels = []

            for i in range(num_points):
                # 1. æ­£æç¤º (Self)
                current_pt = points_np[i]
                pts = [current_pt]
                lbls = [1] # 1 = Positive
                
                # 2. å¯†åº¦åˆ¤æ–­
                is_crowded = False
                if dists is not None:
                    # dists[i, 1] æ˜¯ç¦»è‡ªå·±æœ€è¿‘çš„é‚»å±…è·ç¦» (ä¸‹æ ‡0æ˜¯è‡ªå·±)
                    # å¦‚æœæœ€è¿‘çš„é‚»å±…è·ç¦»å°äºé˜ˆå€¼ï¼Œè¯´æ˜æ˜¯æ‹¥æŒ¤åŒºåŸŸ
                    if len(dists[i]) > 1:
                        nearest_dist = dists[i, 1] 
                        if nearest_dist < dense_dist_thresh:
                            is_crowded = True
                
                # 3. è´Ÿæç¤ºæ³¨å…¥ (Neighboring Negatives)
                if is_crowded:
                    # éå†é‚»å±… (è·³è¿‡ä¸‹æ ‡0ï¼Œå› ä¸ºæ˜¯è‡ªå·±)
                    for j in range(1, len(indices[i])):
                        neighbor_idx = indices[i][j]
                        neighbor_pt = points_np[neighbor_idx]
                        
                        pts.append(neighbor_pt)
                        lbls.append(0) # 0 = Negative (å‘Šè¯‰ SAM è¿™é‡Œä¸æ˜¯æˆ‘)
                
                # 4. Padding (è¡¥é½åˆ°å›ºå®šé•¿åº¦ k+1)
                # å¿…é¡» Pad åˆ°å›ºå®šé•¿åº¦æ‰èƒ½ stack æˆ Tensor
                while len(pts) < k_neighbors + 1:
                    pts.append([0.0, 0.0]) # Pad åæ ‡ (0,0)
                    lbls.append(-1)        # -1 = Ignore Label (SAM ä¼šå¿½ç•¥æ­¤ç‚¹)
                
                image_point_coords.append(pts)
                image_point_labels.append(lbls)

            # è½¬ä¸º Tensor
            batch_prompts.append({
                # coords: [N_cells, K+1, 2]
                "point_coords": torch.tensor(np.array(image_point_coords), device=device).float(),
                # labels: [N_cells, K+1]
                "point_labels": torch.tensor(np.array(image_point_labels), device=device).long(),
                "has_points": True
            })
            
        return batch_prompts

    def get_points_from_heatmap(self, heatmap_logits, topk=1):
        """
        [æ—§æ–¹æ³•] ç®€å•çš„ Top-K é‡‡æ · (ä¿ç•™ä½œä¸º fallback æˆ– baseline)
        ä»…ç”¨äºç®€å•éªŒè¯ï¼Œä¸å…·å¤‡å¯†åº¦è‡ªé€‚åº”èƒ½åŠ›ã€‚
        """
        B, C, H, W = heatmap_logits.shape
        device = heatmap_logits.device
        all_points = []
        all_labels = []

        for b in range(B):
            flat_fg = heatmap_logits[b, 0].view(-1)
            val, idx = torch.topk(flat_fg, k=topk)
            y = (idx // W).float()
            x = (idx % W).float()
            
            batch_points = []
            batch_labels = []
            for i in range(topk):
                batch_points.append([x[i], y[i]])
                batch_labels.append(1) 
            
            all_points.append(torch.tensor(batch_points, device=device))
            all_labels.append(torch.tensor(batch_labels, device=device))

        return torch.stack(all_points), torch.stack(all_labels)

# === Loss å‡½æ•° ===
def point_guidance_loss(pred_heatmap_logits, target_heatmap):
    """
    pred_heatmap_logits: [B, C, H, W] (æœªè¿‡ sigmoid)
    target_heatmap:      [B, C, H, W] (DataLoaderç”Ÿæˆçš„æ¤­åœ†çƒ­åŠ›å›¾)
    """
    pred_prob = torch.sigmoid(pred_heatmap_logits)
    return focal_loss(pred_prob, target_heatmap)

def focal_loss(pred, target, alpha=2, beta=4):
    """
    CenterNet é£æ ¼ Focal Loss
    """
    pos_inds = target.eq(1).float()
    neg_inds = target.lt(1).float()
    neg_weights = torch.pow(1 - target, beta)
    
    loss = 0
    pred = torch.clamp(pred, 1e-6, 1 - 1e-6)
    
    pos_loss = torch.log(pred) * torch.pow(1 - pred, alpha) * pos_inds
    neg_loss = torch.log(1 - pred) * torch.pow(pred, alpha) * neg_weights * neg_inds
    
    num_pos = pos_inds.float().sum()
    pos_loss = pos_loss.sum()
    neg_loss = neg_loss.sum()
    
    if num_pos == 0:
        loss = -neg_loss
    else:
        loss = -(pos_loss + neg_loss) / num_pos
    return loss