import torch
import torch.nn as nn
import torch.nn.functional as F

class TextGuidedPointGenerator(nn.Module):
    def __init__(self, embed_dim=256, text_dim=512): # æ³¨æ„è¿™é‡Œ text_dim=512 (CLIP standard)
        super().__init__()
        
        # ============================================================
        # ğŸ”‘ å…³é”®æ¨¡å— 1: å¯¹é½å±‚ (Alignment Layer)
        # ============================================================
        # è¿™ä¸€å±‚è´Ÿè´£â€œç¿»è¯‘â€ã€‚
        # å®ƒæ˜¯å¯è®­ç»ƒçš„ï¼åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒä¼šå­¦ä¹ å¦‚ä½•æŠŠ CLIP çš„æ–‡æœ¬ç‰¹å¾
        # æ˜ å°„åˆ° SAM çš„å›¾åƒç‰¹å¾ç©ºé—´ï¼Œä½¿å¾—å®ƒä»¬å¯ä»¥è¿›è¡Œæ•°å­¦äº¤äº’ã€‚
        self.text_proj = nn.Linear(text_dim, embed_dim) # 512 -> 256
        
        # ============================================================
        # ğŸ”‘ å…³é”®æ¨¡å— 2: èåˆå±‚ (Fusion Layer)
        # ============================================================
        # è¿™é‡Œçš„è¾“å…¥æ˜¯ concat åçš„ç»“æœï¼Œæˆ‘ä»¬å°†åˆ©ç”¨å·ç§¯å±‚æ¥è¿›ä¸€æ­¥
        # å¤„ç†â€œå¯¹é½åâ€çš„ç‰¹å¾ã€‚
        self.fusion_convs = nn.Sequential(
            nn.Conv2d(embed_dim * 2, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # é¢„æµ‹çƒ­åŠ›å›¾ (å•é€šé“)
        self.heatmap_head = nn.Conv2d(128, 1, kernel_size=1)
        # åˆå§‹åŒ– bias ä¸ºè´Ÿæ•°ï¼Œé˜²æ­¢åˆæœŸé¢„æµ‹è¿‡å¤šçš„å‰æ™¯ç‚¹ (Focal Loss å¸¸ç”¨æŠ€å·§)
        self.heatmap_head.bias.data.fill_(-2.19)

    def forward(self, image_embeddings, text_embeddings):
        """
        image_embeddings: [B, 256, 64, 64] (æ¥è‡ª SAMï¼Œå†»ç»“çš„)
        text_embeddings:  [B, 512] (æ¥è‡ª CLIPï¼Œå†»ç»“çš„)
        """
        B, C, H, W = image_embeddings.shape
        
        # ------------------------------------------------------------
        # 1. ç»´åº¦å¯¹é½ (Feature Alignment)
        # ------------------------------------------------------------
        # å°† CLIP ç‰¹å¾ (512) æŠ•å½±åˆ° SAM ç©ºé—´ (256)
        # è¿™å°±æ˜¯â€œç¿»è¯‘â€è¿‡ç¨‹
        text_feat = self.text_proj(text_embeddings) # [B, 512] -> [B, 256]
        
        # æ‰©å±•åˆ°å›¾åƒå°ºå¯¸ï¼Œå‡†å¤‡èåˆ
        text_feat = text_feat.view(B, C, 1, 1) # [B, 256, 1, 1]
        
        # ------------------------------------------------------------
        # 2. äº¤äº’èåˆ (Interaction / Modulation)
        # ------------------------------------------------------------
        # ä¹˜æ³•èåˆ (æ˜¾å¼å¯¹é½)
        # ç±»ä¼¼äº Attentionï¼šç”¨æ–‡æœ¬å»â€œæ¿€æ´»â€å›¾åƒä¸­åŒ¹é…çš„åŒºåŸŸ
        # å¦‚æœ text_proj è®­ç»ƒå¾—å¥½ï¼Œè¿™é‡Œçš„ä¹˜ç§¯å°±èƒ½é«˜äº®å‡ºç›®æ ‡ç»†èƒ
        activated_features = image_embeddings * text_feat 
        
        # æ‹¼æ¥ (ä¿ç•™åŸå§‹ä¿¡æ¯)
        # æŠŠâ€œæ¿€æ´»åçš„ç‰¹å¾â€å’Œâ€œåŸå§‹ç‰¹å¾â€æ‹¼åœ¨ä¸€èµ·ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±
        fusion_input = torch.cat([activated_features, image_embeddings], dim=1) # [B, 512, 64, 64]
        
        # ------------------------------------------------------------
        # 3. ç”Ÿæˆçƒ­åŠ›å›¾
        # ------------------------------------------------------------
        features = self.fusion_convs(fusion_input)
        heatmap_logits = self.heatmap_head(features)
        
        return heatmap_logits

    def get_coordinates_differentiable(self, heatmap_logits, temperature=1.0):
        """
        Spatial Soft-Argmax (ä¿æŒä¸å˜ï¼Œç”¨äºç”Ÿæˆå¯å¯¼åæ ‡)
        """
        B, _, H, W = heatmap_logits.shape
        device = heatmap_logits.device
        
        grid_y, grid_x = torch.meshgrid(
            torch.arange(H, device=device, dtype=torch.float32),
            torch.arange(W, device=device, dtype=torch.float32),
            indexing='ij'
        )
        
        flat_logits = heatmap_logits.view(B, -1)
        prob_map = F.softmax(flat_logits / temperature, dim=1).view(B, 1, H, W)
        
        pred_x = torch.sum(prob_map * grid_x, dim=[2, 3])
        pred_y = torch.sum(prob_map * grid_y, dim=[2, 3])
        
        return torch.cat([pred_x, pred_y], dim=1).unsqueeze(1)

# =====================================================================
# 3. è¾…åŠ© Loss å‡½æ•°
# =====================================================================
def point_guidance_loss(pred_heatmap_logits, target_heatmap):
    """
    è¾…åŠ©æŸå¤±ï¼šè®©ç”Ÿæˆçš„çƒ­åŠ›å›¾å»æ‹Ÿåˆé«˜æ–¯åˆ†å¸ƒçš„ GT
    """
    pred_prob = torch.sigmoid(pred_heatmap_logits)
    return focal_loss(pred_prob, target_heatmap)

def focal_loss(pred, target, alpha=2, beta=4):
    pos_inds = target.eq(1).float()
    neg_inds = target.lt(1).float()
    neg_weights = torch.pow(1 - target, beta)

    loss = 0
    pred = torch.clamp(pred, 1e-6, 1 - 1e-6)
    
    pos_loss = torch.log(pred) * torch.pow(1 - pred, alpha) * pos_inds
    neg_loss = torch.log(1 - pred) * torch.pow(pred, alpha) * neg_weights * neg_inds

    num_pos = pos_inds.float().sum()
    pos_loss = pos_loss.sum()
    neg_loss = neg_loss.sum()

    if num_pos == 0:
        loss = -neg_loss
    else:
        loss = -(pos_loss + neg_loss) / num_pos
    return loss