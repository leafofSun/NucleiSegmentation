# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import torch
from torch import nn
from torch.nn import functional as F
from typing import Any, Dict, List, Tuple, Optional
from .image_encoder import ImageEncoderViT
from .mask_decoder import MaskDecoder
from .prompt_encoder import PromptEncoder
from .pnurl import PNuRL  # å‡è®¾ pnurl.py å·²åˆ›å»º
import sys
import os

# === ä¾èµ–æ£€æŸ¥ ===
try:
    import clip
except ImportError:
    # å°è¯•æ·»åŠ è·¯å¾„ (æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´)
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../../CLIP")) 
    try:
        import clip
    except ImportError:
        print("âš ï¸ Warning: CLIP not found. DualPromptLearner will fail.")

try:
    from prompt_generator import TextGuidedPointGenerator
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../.."))
    try:
        from prompt_generator import TextGuidedPointGenerator
    except ImportError:
        print("âš ï¸ Warning: prompt_generator.py not found.")

# === åŸºç¡€ SAM ç±» ===
class Sam(nn.Module):
    mask_threshold: float = 0.0
    image_format: str = "RGB"

    def __init__(
        self,
        image_encoder: ImageEncoderViT,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
        pixel_mean: List[float] = [123.675, 116.28, 103.53],
        pixel_std: List[float] = [58.395, 57.12, 57.375],
    ) -> None:
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self.register_buffer("pixel_mean", torch.Tensor(pixel_mean).view(-1, 1, 1), False)
        self.register_buffer("pixel_std", torch.Tensor(pixel_std).view(-1, 1, 1), False)

    @property
    def device(self) -> Any:
        return self.pixel_mean.device

    @torch.no_grad()
    def forward(self, batched_input: List[Dict[str, Any]], multimask_output: bool):
        # åŸºç¡€ SAM forward é€»è¾‘ä¿æŒä¸å˜ï¼Œä¸»è¦é€»è¾‘åœ¨ TextSam ä¸­é‡å†™
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images)
        outputs = []
        for image_record, curr_embedding in zip(batched_input, image_embeddings):
            if "point_coords" in image_record:
                points = (image_record["point_coords"], image_record["point_labels"])
            else:
                points = None
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=points,
                boxes=image_record.get("boxes", None),
                masks=image_record.get("mask_inputs", None),
            )
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_embedding.unsqueeze(0),
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )
            masks = self.postprocess_masks(
                low_res_masks,
                input_size=image_record["image"].shape[-2:],
                original_size=image_record["original_size"],
            )
            masks = masks > self.mask_threshold
            outputs.append({
                "masks": masks,
                "iou_predictions": iou_predictions,
                "low_res_logits": low_res_masks,
            })
        return outputs

    def postprocess_masks(self, masks: torch.Tensor, input_size: Tuple[int, ...], original_size: Tuple[int, ...]) -> torch.Tensor:
        if masks.dim() == 3: masks = masks.unsqueeze(0)
        masks = F.interpolate(
            masks,
            (self.image_encoder.img_size, self.image_encoder.img_size),
            mode="bilinear",
            align_corners=False,
        )
        masks = masks[..., : input_size[0], : input_size[1]]
        masks = F.interpolate(masks, original_size, mode="bilinear", align_corners=False)
        return masks

    def preprocess(self, x: torch.Tensor) -> torch.Tensor:
        x = (x - self.pixel_mean) / self.pixel_std
        h, w = x.shape[-2:]
        padh = self.image_encoder.img_size - h
        padw = self.image_encoder.img_size - w
        x = F.pad(x, (0, padw, 0, padh))
        return x

# === ğŸ”¥ [æ¨¡å— 1] Dual-Prompt Learner (åŒå±‚æç¤ºåº“) ===
class DualPromptLearner(nn.Module):
    def __init__(self, clip_model, num_organs=14, n_ctx_gen=8, n_ctx_spec=8):
        super().__init__()
        ctx_dim = clip_model.ln_final.weight.shape[0] # 512
        dtype = clip_model.dtype
        self.dtype = dtype

        # é€šç”¨ç‰¹å¾åº“
        print(f"ğŸ§  Init DualLearner: General({n_ctx_gen}) + Specific({n_ctx_spec}x{num_organs})")
        self.ctx_general = nn.Parameter(torch.empty(n_ctx_gen, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_general, std=0.02)
        
        # ç‰¹å®šç‰¹å¾åº“
        self.ctx_specific = nn.Parameter(torch.empty(num_organs, n_ctx_spec, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_specific, std=0.02)
        
        # ä¿å­˜ CLIP ç»„ä»¶
        self.clip_token_embedding = clip_model.token_embedding
        self.clip_transformer = clip_model.transformer
        self.clip_ln_final = clip_model.ln_final
        self.clip_text_projection = clip_model.text_projection
        
        self.n_ctx_gen = n_ctx_gen
        self.n_ctx_spec = n_ctx_spec
        self.total_ctx = n_ctx_gen + n_ctx_spec

    def forward(self, organ_indices, tokenized_prompts):
        batch_size = len(organ_indices)
        embedding = self.clip_token_embedding(tokenized_prompts).type(self.dtype)
        
        ctx_gen = self.ctx_general.unsqueeze(0).expand(batch_size, -1, -1)
        ctx_spec = self.ctx_specific[organ_indices]
        ctx = torch.cat([ctx_gen, ctx_spec], dim=1) # [B, total_ctx, dim]

        prefix = embedding[:, :1, :] 
        suffix = embedding[:, 1 : 77 - self.total_ctx, :] 
        x = torch.cat([prefix, ctx, suffix], dim=1)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.clip_transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.clip_ln_final(x).type(self.dtype)

        original_eos_idx = tokenized_prompts.argmax(dim=-1)
        eos_idx = torch.clamp(original_eos_idx + self.total_ctx, max=76)
        text_features = x[torch.arange(x.shape[0]), eos_idx] @ self.clip_text_projection

        return text_features


# === ğŸ”¥ [æ¨¡å— 2] MP-SAM (TextSam) æ ¸å¿ƒç±» ===
class TextSam(Sam):
    def __init__(
        self, 
        image_encoder, 
        prompt_encoder, 
        mask_decoder,
        pixel_mean=[123.675, 116.28, 103.53],
        pixel_std=[58.395, 57.12, 57.375],
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256,
        num_organs=14 
    ):
        super().__init__(image_encoder, prompt_encoder, mask_decoder, pixel_mean, pixel_std)
        
        print(f"ğŸš€ Initializing MP-SAM (Multi-granularity Prompt SAM)...")
        
        # 1. åŠ è½½ CLIP (Freeze)
        self.clip_model, _ = clip.load(clip_model_name, device="cpu")
        for param in self.clip_model.parameters():
            param.requires_grad = False 
            
        # 2. Dual-Prompt Learner (Trainable)
        self.prompt_learner = DualPromptLearner(
            self.clip_model, 
            num_organs=num_organs, 
            n_ctx_gen=8, 
            n_ctx_spec=8 
        )
        for param in self.prompt_learner.parameters():
            param.requires_grad = True 
            
        # 3. PNuRL (Trainable)
        self.pnurl = PNuRL(
            feature_dim=embed_dim, # æ³¨æ„å‚æ•°åå¯èƒ½è¦å¯¹åº” pnurl.py
            # clip_model_path=None, 
            # num_classes_per_attr=[2, 3, 2, 3, 3] 
        )
        
        # 4. Auto-Prompt Generator (Trainable)
        self.prompt_generator = TextGuidedPointGenerator(
            embed_dim=embed_dim,
            text_dim=text_dim
        )
        
        # 5. å†»ç»“ç­–ç•¥
        for param in self.image_encoder.parameters(): param.requires_grad = False
        for param in self.prompt_encoder.parameters(): param.requires_grad = False
        for param in self.mask_decoder.parameters(): param.requires_grad = True
        
        # è§£å†» Adapter
        adapter_count = 0
        for name, param in self.image_encoder.named_parameters():
            if "Adapter" in name:
                param.requires_grad = True
                adapter_count += 1
                
        print(f"âœ… Model Ready: Adapters({adapter_count}), DualLearner, PNuRL, Generator Unfrozen.")

    def forward(self, batched_input, multimask_output=False):
        # === Step 1: åŸºç¡€å›¾åƒç¼–ç  ===
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images) # [B, 256, 64, 64]
        device = image_embeddings.device

        if self.clip_model.visual.conv1.weight.device != device:
            self.clip_model = self.clip_model.to(device)

        # === Step 2: æ•°æ®æå– ===
        organ_indices = []
        attribute_texts = []
        base_texts = [] 

        for x in batched_input:
            organ_indices.append(x.get("organ_id", 0)) 
            attribute_texts.append(x.get("attribute_text", "")) 
            base_texts.append(x.get("text_prompt", "Cell nuclei"))

        organ_indices = torch.tensor(organ_indices).to(device)

        # === Step 3: Dual-Prompt Learner (Implicit Context) ===
        # Positive
        pos_tokens = clip.tokenize(base_texts, truncate=True).to(device)
        pos_feats = self.prompt_learner(organ_indices, pos_tokens) # [B, 512]
        
        # Negative (Background)
        neg_tokens = clip.tokenize(["Background"] * len(base_texts), truncate=True).to(device)
        neg_feats = self.prompt_learner(organ_indices, neg_tokens) # [B, 512]

        pos_feats = pos_feats / pos_feats.norm(dim=-1, keepdim=True)
        neg_feats = neg_feats / neg_feats.norm(dim=-1, keepdim=True)
        text_features = torch.stack([pos_feats, neg_feats], dim=1).float() # [B, 2, 512]

        # === Step 4: PNuRL (Explicit Attribute Injection) ===
        if next(self.pnurl.parameters()).device != device:
            self.pnurl = self.pnurl.to(device)
        
        # å‡†å¤‡å±æ€§æ ‡ç­¾ (Attribute Labels)
        attribute_labels_list = []
        for x in batched_input:
            attr_labels = x.get("attr_labels", None)
            if attr_labels is not None:
                attribute_labels_list.append(attr_labels)
            else:
                attribute_labels_list.append(torch.tensor([0, 0, 0, 1, 1], dtype=torch.long))
        
        if len(attribute_labels_list) > 0:
            attr_labels_batch = torch.stack(attribute_labels_list).to(device)  # [B, 5]
        else:
            attr_labels_batch = None
            
        # PNuRL Forward
        # è¿”å›: [logits_list], loss (refined_embeddings æš‚æœªå®ç°ï¼Œå¦‚æœ PNuRL åªæ˜¯åˆ†ç±»å¤´)
        # å¦‚æœä½ å¸Œæœ› PNuRL ä¿®æ­£ Image Embeddingï¼Œéœ€è¦åœ¨ PNuRL forward ä¸­å®ç° Attention
        # å‡è®¾ PNuRL åªè´Ÿè´£è®¡ç®— Lossï¼Œä¸æ”¹å˜ Feature (å®è§‚ç›‘ç£)
        # æˆ–è€… PNuRL è¿”å› refined_features (å¦‚æœå®ç°äº†)
        
        # è¿™é‡Œå‡è®¾ PNuRL åªæ˜¯ç®€å•çš„åˆ†ç±»å¤´é›†åˆï¼Œä¸ä¿®æ”¹ image_features
        # å¦‚æœéœ€è¦ä¿®æ”¹ image_featuresï¼Œè¯·ç¡®ä¿ PNuRL forward è¿”å›ä¿®æ”¹åçš„ç‰¹å¾
        # è¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨åŸå§‹ image_embeddings ç»§ç»­ï¼ŒPNuRL ä»…ä½œä¸ºè¾…åŠ© Loss
        attr_logits, pnurl_loss = self.pnurl(image_embeddings, attr_labels_batch)
        
        # å¦‚æœ PNuRL è¿”å› refined_featuresï¼Œåˆ™æ›´æ–°
        # refined_image_embeddings = ...
        refined_image_embeddings = image_embeddings # ç›®å‰ä¿æŒä¸å˜

        # === Step 5: Auto-Prompt Generation (SAC - Adaptive) ===
        heatmap_logits = self.prompt_generator(refined_image_embeddings, text_features)
        
        # ğŸ”¥ [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨è‡ªé€‚åº”é‡‡æ · (Adaptive Sampling)
        # è·å–å«æœ‰ æ­£ç‚¹+è´Ÿé‚»å±… çš„ Prompt åˆ—è¡¨
        prompts_list = self.prompt_generator.generate_adaptive_prompts(
            heatmap_logits, 
            threshold=0.3,       # çƒ­åŠ›å›¾é˜ˆå€¼
            k_neighbors=3,       # é‚»å±…æ•°é‡
            dense_dist_thresh=15.0 # æ‹¥æŒ¤è·ç¦»é˜ˆå€¼
        )
        
        # åæ ‡æ˜ å°„ (Feature Grid -> Original Image)
        feat_size = image_embeddings.shape[-1] 
        input_size = self.image_encoder.img_size 
        scale_factor = input_size / feat_size

        # === Step 6: SAM Mask Decoder (Loop Batch) ===
        outputs = []
        
        for i in range(len(batched_input)):
            # è·å–å½“å‰æ ·æœ¬çš„ Prompt æ•°æ®
            prompt_data = prompts_list[i]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‚¹ (å…¨èƒŒæ™¯)ï¼Œåˆ›å»ºä¸€ä¸ª Dummy Prompt é˜²æ­¢æŠ¥é”™
            # æˆ–è€…ç›´æ¥é¢„æµ‹ç©º Mask (æ›´åˆç†)
            if not prompt_data["has_points"]:
                # æ— ç‚¹ -> è¾“å‡ºå…¨é»‘ Mask
                # æ„é€ ä¸€ä¸ªç©ºçš„ output ç»“æ„
                outputs.append({
                    "masks": torch.zeros((1, 1, 1024, 1024), device=device, dtype=torch.bool),
                    "iou_predictions": torch.zeros((1, 1), device=device),
                    "low_res_logits": torch.zeros((1, 1, 256, 256), device=device),
                    "heatmap_logits": heatmap_logits[i].unsqueeze(0),
                    "pnurl_loss": pnurl_loss
                })
                continue

            # æå–åæ ‡å’Œæ ‡ç­¾
            # coords: [N_cells, K+1, 2]
            # labels: [N_cells, K+1]
            point_coords = prompt_data["point_coords"]
            point_labels = prompt_data["point_labels"]
            
            # ç¼©æ”¾åæ ‡åˆ° 1024
            point_coords = (point_coords * scale_factor) + (scale_factor * 0.5)
            
            # å–‚ç»™ Prompt Encoder
            # sparse_embeddings: [N_cells, tokens, channel]
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=(point_coords, point_labels),
                boxes=None,
                masks=None,
            )
            
            # æ‰©å±• Image Embedding ä»¥åŒ¹é… N_cells
            # curr_embedding: [256, 64, 64] -> [1, 256, 64, 64] -> [N_cells, 256, 64, 64]
            num_cells = point_coords.shape[0]
            curr_img_embed = refined_image_embeddings[i].unsqueeze(0).expand(num_cells, -1, -1, -1)
            
            # è§£ç 
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_img_embed,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )
            
            # === Step 7: åå¤„ç† & èšåˆ ===
            # low_res_masks: [N_cells, 1, 256, 256]
            # æˆ‘ä»¬éœ€è¦æŠŠå®ƒåˆå¹¶æˆä¸€å¼ å›¾ (Instance Segmentation -> Semantic Mask for Loss)
            # æˆ–è€…ä¿ç•™ Instance å½¢å¼è®¡ç®— Loss (å¦‚æœ Loss æ”¯æŒ)
            # è¿™é‡Œä¸ºäº†é€‚é…åŸæ¥çš„ pipelineï¼Œæˆ‘ä»¬å°† N ä¸ª Mask å– Max åˆå¹¶
            # (æ³¨æ„ï¼šè¿™æ˜¯ç®€åŒ–å¤„ç†ï¼Œä¸¥æ ¼æ¥è¯´åº”è¯¥åŒ¹é… GT çš„ Instance ID)
            
            # åˆå¹¶ç­–ç•¥: Max Pool (åªè¦æœ‰ä¸€ä¸ªç»†èƒé¢„æµ‹æ˜¯å‰æ™¯ï¼Œå°±æ˜¯å‰æ™¯)
            merged_logits, _ = torch.max(low_res_masks, dim=0, keepdim=True) # [1, 1, 256, 256]
            
            # IoU ä¹Ÿå¯ä»¥å–å¹³å‡æˆ–æœ€å¤§
            merged_iou, _ = torch.max(iou_predictions, dim=0, keepdim=True)

            mask_post = self.postprocess_masks(
                merged_logits,
                input_size=batched_input[i]["image"].shape[-2:], 
                original_size=batched_input[i]["original_size"],
            )
            
            outputs.append({
                "masks": mask_post > self.mask_threshold, # Boolean Mask
                "iou_predictions": merged_iou,
                "low_res_logits": merged_logits,
                "heatmap_logits": heatmap_logits[i].unsqueeze(0),
                "attr_logits": None, # æš‚ä¸è¿”å›è¯¦ç»† Logits ä»¥çœæ˜¾å­˜
                "pnurl_loss": pnurl_loss
            })
            
        return outputs