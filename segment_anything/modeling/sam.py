# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import torch
from torch import nn
from torch.nn import functional as F
from typing import Any, Dict, List, Tuple, Optional
from .image_encoder import ImageEncoderViT
from .mask_decoder import MaskDecoder
from .prompt_encoder import PromptEncoder
from .pnurl import PNuRL
import sys
import os

# === ä¾èµ–æ£€æŸ¥ ===
try:
    import clip
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../../CLIP")) 
    try:
        import clip
    except ImportError:
        print("âš ï¸ Warning: CLIP not found. DualPromptLearner will fail.")

try:
    from prompt_generator import TextGuidedPointGenerator
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../.."))
    try:
        from prompt_generator import TextGuidedPointGenerator
    except ImportError:
        print("âš ï¸ Warning: prompt_generator.py not found.")

# === åŸºç¡€ SAM ç±» ===
class Sam(nn.Module):
    mask_threshold: float = 0.0
    image_format: str = "RGB"

    def __init__(
        self,
        image_encoder: ImageEncoderViT,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
        pixel_mean: List[float] = [123.675, 116.28, 103.53],
        pixel_std: List[float] = [58.395, 57.12, 57.375],
    ) -> None:
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self.register_buffer("pixel_mean", torch.Tensor(pixel_mean).view(-1, 1, 1), False)
        self.register_buffer("pixel_std", torch.Tensor(pixel_std).view(-1, 1, 1), False)

    @property
    def device(self) -> Any:
        return self.pixel_mean.device

    @torch.no_grad()
    def forward(self, batched_input: List[Dict[str, Any]], multimask_output: bool):
        # åŸºç¡€ SAM forward é€»è¾‘ä¿æŒä¸å˜
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images)
        outputs = []
        for image_record, curr_embedding in zip(batched_input, image_embeddings):
            if "point_coords" in image_record:
                points = (image_record["point_coords"], image_record["point_labels"])
            else:
                points = None
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=points,
                boxes=image_record.get("boxes", None),
                masks=image_record.get("mask_inputs", None),
            )
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_embedding.unsqueeze(0),
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )
            masks = self.postprocess_masks(
                low_res_masks,
                input_size=image_record["image"].shape[-2:],
                original_size=image_record["original_size"],
            )
            masks = masks > self.mask_threshold
            outputs.append({
                "masks": masks,
                "iou_predictions": iou_predictions,
                "low_res_logits": low_res_masks,
            })
        return outputs

    def postprocess_masks(self, masks: torch.Tensor, input_size: Tuple[int, ...], original_size: Tuple[int, ...]) -> torch.Tensor:
        if masks.dim() == 3: masks = masks.unsqueeze(0)
        masks = F.interpolate(
            masks,
            (self.image_encoder.img_size, self.image_encoder.img_size),
            mode="bilinear",
            align_corners=False,
        )
        masks = masks[..., : input_size[0], : input_size[1]]
        masks = F.interpolate(masks, original_size, mode="bilinear", align_corners=False)
        return masks

    def preprocess(self, x: torch.Tensor) -> torch.Tensor:
        x = (x - self.pixel_mean) / self.pixel_std
        h, w = x.shape[-2:]
        padh = self.image_encoder.img_size - h
        padw = self.image_encoder.img_size - w
        x = F.pad(x, (0, padw, 0, padh))
        return x

# === Physical Adapter (é€‚é…å™¨) ===
class PhysicalAdapter(nn.Module):
    """
    ç‰©ç†ç‰¹å¾é€‚é…å™¨ï¼šå°†èåˆåçš„ Shape+Size+Density ç‰¹å¾è½¬æ¢ä¸º FiLM å‚æ•°
    """
    def __init__(self, feat_dim_low: int, feat_dim_high: int, ctx_dim: int):
        """
        Args:
            feat_dim_low: æµ…å±‚ç‰¹å¾ç»´åº¦ (3 * in_dim // 4) - æ‹¼æ¥åçš„ç»´åº¦
            feat_dim_high: æ·±å±‚ç‰¹å¾ç»´åº¦ (3 * in_dim // 2) - æ‹¼æ¥åçš„ç»´åº¦
            ctx_dim: Prompt çš„ç»´åº¦ (CLIP ctx_dim)
        """
        super().__init__()
        
        # æ³¨æ„ï¼šè¿™é‡Œçš„ feat_dim_low å’Œ high å·²ç»æ˜¯æ‹¼æ¥åçš„ç»´åº¦ (x3)
        
        # æµ…å±‚ç‰¹å¾é€‚é…å™¨ï¼ˆç”¨äºè°ƒåˆ¶æµ…å±‚ Promptï¼‰
        self.adapter_low = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(feat_dim_low, ctx_dim),
            nn.ReLU(),
            nn.Linear(ctx_dim, ctx_dim * 2)  # è¾“å‡º [Î³, Î²]ï¼Œæ¯ä¸ªç»´åº¦ä¸º ctx_dim
        )
        
        # æ·±å±‚ç‰¹å¾é€‚é…å™¨ï¼ˆç”¨äºè°ƒåˆ¶æ·±å±‚ Promptï¼‰
        self.adapter_high = nn.Sequential(
            nn.Linear(feat_dim_high, ctx_dim),
            nn.ReLU(),
            nn.Linear(ctx_dim, ctx_dim * 2)  # è¾“å‡º [Î³, Î²]ï¼Œæ¯ä¸ªç»´åº¦ä¸º ctx_dim
        )
        
        # Zero-initialization: å°†æœ€åä¸€å±‚æƒé‡è®¾ä¸º0
        nn.init.zeros_(self.adapter_low[-1].weight)
        nn.init.zeros_(self.adapter_low[-1].bias)
        nn.init.zeros_(self.adapter_high[-1].weight)
        nn.init.zeros_(self.adapter_high[-1].bias)
    
    def forward(self, feat_low: torch.Tensor, feat_high: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Args:
            feat_low: [B, feat_dim_low, H, W] æµ…å±‚ç‰¹å¾ï¼ˆæ‹¼æ¥åçš„ï¼‰
            feat_high: [B, feat_dim_high] æ·±å±‚ç‰¹å¾ï¼ˆæ‹¼æ¥åçš„ï¼‰
        
        Returns:
            gamma_low: [B, ctx_dim] æµ…å±‚ç¼©æ”¾å‚æ•°
            beta_low: [B, ctx_dim] æµ…å±‚åç½®å‚æ•°
            gamma_high: [B, ctx_dim] æ·±å±‚ç¼©æ”¾å‚æ•°
            beta_high: [B, ctx_dim] æ·±å±‚åç½®å‚æ•°
        """
        # æµ…å±‚é€‚é…å™¨
        low_params = self.adapter_low(feat_low)  # [B, ctx_dim * 2]
        gamma_low, beta_low = torch.chunk(low_params, 2, dim=1)  # å„ [B, ctx_dim]
        
        # æ·±å±‚é€‚é…å™¨
        high_params = self.adapter_high(feat_high)  # [B, ctx_dim * 2]
        gamma_high, beta_high = torch.chunk(high_params, 2, dim=1)  # å„ [B, ctx_dim]
        
        return gamma_low, beta_low, gamma_high, beta_high


# === ğŸ”¥ [æ¨¡å— 1] Dual-Prompt Learner (åŒå±‚æç¤ºåº“) ===
class DualPromptLearner(nn.Module):
    def __init__(self, clip_model, num_organs=21, n_ctx_gen=8, n_ctx_spec=8, embed_dim=256):
        super().__init__()
        ctx_dim = clip_model.ln_final.weight.shape[0] # 512
        dtype = clip_model.dtype
        self.dtype = dtype

        # é€šç”¨ç‰¹å¾åº“
        print(f"ğŸ§  Init DualLearner: General({n_ctx_gen}) + Specific({n_ctx_spec}x{num_organs})")
        self.ctx_general = nn.Parameter(torch.empty(n_ctx_gen, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_general, std=0.02)
        
        # ç‰¹å®šç‰¹å¾åº“
        self.ctx_specific = nn.Parameter(torch.empty(num_organs, n_ctx_spec, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_specific, std=0.02)
        
        # ä¿å­˜ CLIP ç»„ä»¶
        self.clip_token_embedding = clip_model.token_embedding
        self.clip_transformer = clip_model.transformer
        self.clip_ln_final = clip_model.ln_final
        self.clip_text_projection = clip_model.text_projection
        
        self.n_ctx_gen = n_ctx_gen
        self.n_ctx_spec = n_ctx_spec
        self.total_ctx = n_ctx_gen + n_ctx_spec
        self.ctx_dim = ctx_dim
        
        # ğŸ”¥ [å…³é”®ä¿®æ”¹] è®¡ç®—è¾“å…¥ç»´åº¦
        # PNuRL æ‹¼æ¥äº† 3 ä¸ªå¤´ (Shape, Size, Density)
        # æ¯ä¸ªå¤´çš„ shallow_branch è¾“å‡º C/4 -> æ‹¼æ¥å 3 * C/4
        # æ¯ä¸ªå¤´çš„ deep_branch è¾“å‡º C/2    -> æ‹¼æ¥å 3 * C/2
        num_fused_heads = 3 
        
        feat_dim_low = (embed_dim // 4) * num_fused_heads
        feat_dim_high = (embed_dim // 2) * num_fused_heads
        
        self.physical_adapter = PhysicalAdapter(feat_dim_low, feat_dim_high, ctx_dim)
        print(f"âœ… PhysicalAdapter initialized: in_low={feat_dim_low}, in_high={feat_dim_high} (Fused {num_fused_heads} heads)")

    def forward(self, organ_indices, tokenized_prompts, density_features: Optional[List[torch.Tensor]] = None):
        """
        Args:
            organ_indices: [B] å™¨å®˜ç´¢å¼•
            tokenized_prompts: [B, 77] tokenized prompts
            density_features: Optional[List[torch.Tensor]] = [fused_low, fused_high]
                - fused_low: [B, 3*C/4, H, W] æ‹¼æ¥åçš„æµ…å±‚ç‰¹å¾ (Shape+Size+Density)
                - fused_high: [B, 3*C/2] æ‹¼æ¥åçš„æ·±å±‚ç‰¹å¾ (Shape+Size+Density)
        """
        batch_size = len(organ_indices)
        embedding = self.clip_token_embedding(tokenized_prompts).type(self.dtype)
        
        ctx_gen = self.ctx_general.unsqueeze(0).expand(batch_size, -1, -1)
        ctx_spec = self.ctx_specific[organ_indices]
        ctx = torch.cat([ctx_gen, ctx_spec], dim=1) # [B, total_ctx, dim]

        # === FiLM è°ƒåˆ¶ï¼šä½¿ç”¨ç‰©ç†ç‰¹å¾è°ƒåˆ¶ Prompt ===
        if density_features is not None:
            # è¿™é‡Œ density_features å®é™…ä¸Šæ˜¯ [fused_low, fused_high]
            feat_low, feat_high = density_features
            gamma_low, beta_low, gamma_high, beta_high = self.physical_adapter(feat_low, feat_high)
            
            # å¤šå°ºåº¦ç­–ç•¥ï¼š
            # - æµ…å±‚ç‰¹å¾è°ƒåˆ¶æµ…å±‚ Prompt (ctx_gen çš„å‰åŠéƒ¨åˆ†)
            # - æ·±å±‚ç‰¹å¾è°ƒåˆ¶æ·±å±‚ Prompt (ctx_gen çš„ååŠéƒ¨åˆ† + ctx_spec)
            n_gen_low = self.n_ctx_gen // 2
            n_gen_high = self.n_ctx_gen - n_gen_low
            
            # è°ƒåˆ¶æµ…å±‚ Prompt (ctx_gen çš„å‰åŠéƒ¨åˆ†)
            ctx_gen_low = ctx_gen[:, :n_gen_low, :]  # [B, n_gen_low, ctx_dim]
            gamma_low_expanded = gamma_low.unsqueeze(1).expand(-1, n_gen_low, -1)  # [B, n_gen_low, ctx_dim]
            beta_low_expanded = beta_low.unsqueeze(1).expand(-1, n_gen_low, -1)  # [B, n_gen_low, ctx_dim]
            ctx_gen_low_modulated = (1 + gamma_low_expanded) * ctx_gen_low + beta_low_expanded
            
            # è°ƒåˆ¶æ·±å±‚ Prompt (ctx_gen çš„ååŠéƒ¨åˆ† + ctx_spec)
            ctx_gen_high = ctx_gen[:, n_gen_low:, :]  # [B, n_gen_high, ctx_dim]
            ctx_spec_mod = ctx_spec  # [B, n_ctx_spec, ctx_dim]
            
            gamma_high_expanded_gen = gamma_high.unsqueeze(1).expand(-1, n_gen_high, -1)  # [B, n_gen_high, ctx_dim]
            beta_high_expanded_gen = beta_high.unsqueeze(1).expand(-1, n_gen_high, -1)  # [B, n_gen_high, ctx_dim]
            ctx_gen_high_modulated = (1 + gamma_high_expanded_gen) * ctx_gen_high + beta_high_expanded_gen
            
            gamma_high_expanded_spec = gamma_high.unsqueeze(1).expand(-1, self.n_ctx_spec, -1)  # [B, n_ctx_spec, ctx_dim]
            beta_high_expanded_spec = beta_high.unsqueeze(1).expand(-1, self.n_ctx_spec, -1)  # [B, n_ctx_spec, ctx_dim]
            ctx_spec_modulated = (1 + gamma_high_expanded_spec) * ctx_spec_mod + beta_high_expanded_spec
            
            # é‡æ–°ç»„åˆ
            ctx_gen = torch.cat([ctx_gen_low_modulated, ctx_gen_high_modulated], dim=1)
            ctx_spec = ctx_spec_modulated
            ctx = torch.cat([ctx_gen, ctx_spec], dim=1)

        prefix = embedding[:, :1, :] 
        suffix = embedding[:, 1 : 77 - self.total_ctx, :] 
        x = torch.cat([prefix, ctx, suffix], dim=1)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.clip_transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.clip_ln_final(x).type(self.dtype)

        original_eos_idx = tokenized_prompts.argmax(dim=-1)
        eos_idx = torch.clamp(original_eos_idx + self.total_ctx, max=76)
        text_features = x[torch.arange(x.shape[0]), eos_idx] @ self.clip_text_projection

        return text_features


# === ğŸ”¥ [æ¨¡å— 2] MP-SAM (TextSam) æ ¸å¿ƒç±» ===
class TextSam(Sam):
    def __init__(
        self, 
        image_encoder, 
        prompt_encoder, 
        mask_decoder,
        pixel_mean=[123.675, 116.28, 103.53],
        pixel_std=[58.395, 57.12, 57.375],
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256,
        num_organs=21 
    ):
        super().__init__(image_encoder, prompt_encoder, mask_decoder, pixel_mean, pixel_std)
        
        print(f"ğŸš€ Initializing MP-SAM (Multi-granularity Prompt SAM)...")
        
        # 1. åŠ è½½ CLIP (Freeze)
        self.clip_model, _ = clip.load(clip_model_name, device="cpu")
        for param in self.clip_model.parameters():
            param.requires_grad = False 
            
        # 2. Dual-Prompt Learner (Trainable)
        self.prompt_learner = DualPromptLearner(
            self.clip_model, 
            num_organs=num_organs, 
            n_ctx_gen=8, 
            n_ctx_spec=8,
            embed_dim=embed_dim  # ä¼ é€’ embed_dim ç”¨äºåˆå§‹åŒ– DensityAdapter
        )
        for param in self.prompt_learner.parameters():
            param.requires_grad = True 
            
        # 3. PNuRL (Trainable)
        self.pnurl = PNuRL(
            embed_dim=embed_dim,
        )
        
        # 4. Auto-Prompt Generator (Trainable)
        self.prompt_generator = TextGuidedPointGenerator(
            embed_dim=embed_dim,
            text_dim=text_dim
        )
        
        # 5. å†»ç»“ç­–ç•¥
        for param in self.image_encoder.parameters(): param.requires_grad = False
        for param in self.prompt_encoder.parameters(): param.requires_grad = False
        for param in self.mask_decoder.parameters(): param.requires_grad = True
        
        # è§£å†» Adapter
        adapter_count = 0
        for name, param in self.image_encoder.named_parameters():
            if "Adapter" in name:
                param.requires_grad = True
                adapter_count += 1
                
        print(f"âœ… Model Ready: Adapters({adapter_count}), DualLearner, PNuRL, Generator Unfrozen.")

    def forward(self, batched_input, multimask_output=False):
        # === Step 1: åŸºç¡€å›¾åƒç¼–ç  ===
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images) # [B, 256, 64, 64]
        device = image_embeddings.device

        if self.clip_model.visual.conv1.weight.device != device:
            self.clip_model = self.clip_model.to(device)

        # === Step 2: æ•°æ®æå– ===
        organ_indices = []
        attribute_texts = []
        base_texts = [] 

        for x in batched_input:
            organ_indices.append(x.get("organ_id", 0)) 
            attribute_texts.append(x.get("attribute_text", "")) 
            base_texts.append(x.get("text_prompt", "Cell nuclei"))

        organ_indices = torch.tensor(organ_indices).to(device)

        # === Step 3: PNuRL (å…ˆè·å–å¯†åº¦ç‰¹å¾) ===
        if next(self.pnurl.parameters()).device != device:
            self.pnurl = self.pnurl.to(device)
        
        # å‡†å¤‡å±æ€§æ ‡ç­¾ (Attribute Labels)
        attribute_labels_list = []
        for x in batched_input:
            attr_labels = x.get("attr_labels", None)
            if attr_labels is not None:
                attribute_labels_list.append(attr_labels)
            else:
                attribute_labels_list.append(torch.tensor([0, 0, 0, 1, 1], dtype=torch.long))
        
        if len(attribute_labels_list) > 0:
            attr_labels_batch = torch.stack(attribute_labels_list).to(device)  # [B, 5]
            # æ‹†åˆ†ä¸º 5 ä¸ª tensor list
            attribute_labels = [attr_labels_batch[:, i] for i in range(5)]
        else:
            attribute_labels = None
        
        # PNuRL Forward - è·å–å¯†åº¦ç‰¹å¾ï¼ˆå¤šä»»åŠ¡ï¼šåˆ†ç±» + å›å½’ï¼‰
        refined_image_embeddings, pnurl_context, pnurl_loss, attr_logits, density_features, density_map = self.pnurl(
            image_features=image_embeddings,
            attribute_labels=attribute_labels,
            attribute_prompts=attribute_texts,
            return_loss=True
        )
        # density_map: [B, 1, H', W'] - åƒç´ çº§å¯†åº¦å›¾ï¼ˆç”¨äº DeNSe å¼å¼ºå¯¹é½ï¼‰
        # density_features: [fused_low, fused_high] - å¤šå°ºåº¦ç‰¹å¾
        # ğŸ”¥ [æ ¸å¿ƒæ”¹è¿›] æå–é«˜é¢‘ç‰¹å¾ç”¨äº ASR-Guided Decoder
        high_freq_guide = density_features[0] if isinstance(density_features, (list, tuple)) and len(density_features) > 0 else None
        # high_freq_guide: [B, 192, 64, 64] - PNuRL æµ…å±‚ç‰¹å¾ï¼ˆç”¨äºè¾¹ç•Œç»†åŒ–ï¼‰
        
        # === Step 4: Dual-Prompt Learner (Implicit Context with Density Modulation) ===
        # Positive
        pos_tokens = clip.tokenize(base_texts, truncate=True).to(device)
        pos_feats = self.prompt_learner(organ_indices, pos_tokens, density_features=density_features) # [B, 512]
        
        # Negative (Background)
        neg_tokens = clip.tokenize(["Background"] * len(base_texts), truncate=True).to(device)
        neg_feats = self.prompt_learner(organ_indices, neg_tokens, density_features=density_features) # [B, 512]

        pos_feats = pos_feats / pos_feats.norm(dim=-1, keepdim=True)
        neg_feats = neg_feats / neg_feats.norm(dim=-1, keepdim=True)
        text_features = torch.stack([pos_feats, neg_feats], dim=1).float() # [B, 2, 512]

        # === Step 5: Auto-Prompt Generation (SAC - Adaptive) ===
        heatmap_logits = self.prompt_generator(refined_image_embeddings, text_features)
        
        # ğŸ”¥ [New] å‡†å¤‡ä¼ ç»™ ASR çš„å¯†åº¦å›¾
        # Sigmoid å½’ä¸€åŒ–åˆ° 0~1ï¼Œä½œä¸ºç©ºé—´é—¨æ§
        density_map_proxy = torch.sigmoid(heatmap_logits[:, 0:1, :, :])  # [B, 1, 64, 64]
        
        # ğŸ”¥ [æ ¸å¿ƒæ”¹è¿›] åŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼šåŸºäº PNuRL çš„ Size é¢„æµ‹
        # 1. è·å– Size é¢„æµ‹ç±»åˆ«
        size_logits = attr_logits.get('size', None)  # [B, num_classes] æˆ– None
        if size_logits is not None and size_logits.numel() > 0:
            # è·å–é¢„æµ‹çš„ Size ç±»åˆ« (0=Small, 1=Medium, 2=Large)
            pred_size_class = torch.argmax(size_logits, dim=1)  # [B] -> 0, 1, 2
            
            # 2. å®šä¹‰æ˜ å°„è§„åˆ™ï¼šå°ç»†èƒå…è®¸é å¾—æ›´è¿‘ï¼Œå¤§ç»†èƒéœ€è¦æ›´ä¸¥æ ¼
            # Small(0) -> 10.0, Medium(1) -> 15.0, Large(2) -> 20.0
            size_threshold_map = torch.tensor([10.0, 15.0, 20.0], device=device)
            adaptive_thresh = size_threshold_map[pred_size_class]  # [B]
        else:
            # å¦‚æœ PNuRL æœªè¾“å‡º Size æˆ–å¤„äºè®­ç»ƒåˆæœŸï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼
            batch_size = image_embeddings.shape[0]
            adaptive_thresh = torch.tensor(15.0, device=device).expand(batch_size)
        
        # æ™ºèƒ½å†³å®šæ˜¯å¦é™åˆ¶ç‚¹æ•°: è®­ç»ƒæ—¶é™åˆ¶(50)ï¼ŒéªŒè¯æ—¶ä¸é™åˆ¶
        limit_points = 50 if self.training else None

        prompts_list = self.prompt_generator.generate_adaptive_prompts(
            heatmap_logits, 
            threshold=0.3,       
            k_neighbors=3,       
            dense_dist_thresh=adaptive_thresh,  # ğŸ”¥ ä¼ å…¥åŠ¨æ€é˜ˆå€¼ Tensor
            max_points=limit_points
        )
        
        # åæ ‡æ˜ å°„ (Feature Grid -> Original Image)
        feat_size = image_embeddings.shape[-1] 
        input_size = self.image_encoder.img_size 
        scale_factor = input_size / feat_size

        # === Step 6: SAM Mask Decoder (Loop Batch) ===
        outputs = []
        
        for i in range(len(batched_input)):
            # è·å–å½“å‰æ ·æœ¬çš„ Prompt æ•°æ®
            prompt_data = prompts_list[i]
            
            # ğŸ”¥ [ä¿®å¤] å¤„ç†æ— ç‚¹æƒ…å†µï¼Œé˜²æ­¢ DDP æ­»é”
            if not prompt_data["has_points"]:
                # âœ… æ­£ç¡®å†™æ³•ï¼šåˆ©ç”¨ image_embeddings æ„é€  dummyï¼Œä¿æŒè®¡ç®—å›¾è¿æ¥
                # 0 * sum() æ—¢ä¿ç•™äº†è¿æ¥ï¼Œåˆä¸ä¼šäº§ç”Ÿå®é™…æ¢¯åº¦å½±å“
                dummy_connection = refined_image_embeddings[i].sum() * 0.0
                
                # è·å–æ­£ç¡®çš„ç›®æ ‡å°ºå¯¸ (Original Size, e.g., 512x512)
                target_h, target_w = batched_input[i]["original_size"]
                
                # ğŸ”¥ å¤„ç† density_map
                density_map_i = None
                if density_map is not None:
                    density_map_raw = density_map[i]  # [1, H', W']
                    # å¦‚æœå°ºå¯¸ä¸å¯¹ï¼Œæ’å€¼åˆ° original_size
                    if density_map_raw.shape[-2:] != (target_h, target_w):
                        density_map_i = F.interpolate(
                            density_map_raw.unsqueeze(0), 
                            size=(target_h, target_w), 
                            mode='bilinear', 
                            align_corners=False
                        ).squeeze(0)  # [1, target_h, target_w]
                    else:
                        density_map_i = density_map_raw
                    
                    # åŠ ä¸Š dummy_connection (è™½ç„¶ density_map æœ¬èº«å°±åœ¨å›¾é‡Œï¼ŒåŠ è¿™ä¸ªåŒä¿é™©)
                    density_map_i = density_map_i + dummy_connection
                
                outputs.append({
                    # ğŸ”¥ [ä¿®æ­£] ä½¿ç”¨ (target_h, target_w) è€Œä¸æ˜¯ input_size (1024)
                    "masks": (torch.zeros((1, 1, target_h, target_w), device=device, dtype=torch.float32) - 100.0) + dummy_connection,
                    "iou_predictions": torch.zeros((1, 1), device=device) + dummy_connection,
                    # low_res_logits ä¿æŒ 256x256 æ˜¯å¯¹çš„ï¼Œè¿™æ˜¯ Decoder çš„åŸå§‹è¾“å‡ºå°ºå¯¸
                    "low_res_logits": (torch.zeros((1, 1, 256, 256), device=device) - 100.0) + dummy_connection,
                    "heatmap_logits": heatmap_logits[i].unsqueeze(0),
                    "attr_logits": attr_logits,
                    "density_map": density_map_i,
                    "pnurl_loss": pnurl_loss
                })
                continue

            # æå–åæ ‡å’Œæ ‡ç­¾
            point_coords = prompt_data["point_coords"]
            point_labels = prompt_data["point_labels"]
            
            # ç¼©æ”¾åæ ‡åˆ° 1024
            point_coords = (point_coords * scale_factor) + (scale_factor * 0.5)
            
            # å–‚ç»™ Prompt Encoder (points=(N_cells, K+1, 2))
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=(point_coords, point_labels),
                boxes=None,
                masks=None,
            )
            
            # æ‰©å±• Image Embedding ä»¥åŒ¹é… N_cells
            # refined_image_embeddings[i]: [256, 64, 64] -> [1, 256, 64, 64] -> [N_cells, 256, 64, 64]
            num_cells = point_coords.shape[0]
            curr_img_embed = refined_image_embeddings[i].unsqueeze(0).expand(num_cells, -1, -1, -1)
            
            # ğŸ”¥ [æ ¸å¿ƒæ”¹è¿›] æ‰©å±•é«˜é¢‘ç‰¹å¾ä»¥åŒ¹é… N_cellsï¼ˆç”¨äº ASR-Guided Decoderï¼‰
            curr_high_freq = None
            if high_freq_guide is not None:
                # high_freq_guide[i]: [192, 64, 64] -> [1, 192, 64, 64] -> [N_cells, 192, 64, 64]
                curr_high_freq = high_freq_guide[i].unsqueeze(0).expand(num_cells, -1, -1, -1)
            
            # ğŸ”¥ [New] å‡†å¤‡å½“å‰æ ·æœ¬çš„å¯†åº¦å›¾
            # density_map_proxy: [B, 1, 64, 64] -> å–ç¬¬ i ä¸ªæ ·æœ¬
            curr_density_map = density_map_proxy[i].unsqueeze(0)  # [1, 1, 64, 64]
            # éœ€è¦æ‰©å±•åˆ° N_cells ç»´åº¦
            curr_density_map = curr_density_map.expand(num_cells, -1, -1, -1)  # [N_cells, 1, 64, 64]
            
            # è§£ç ï¼ˆæ³¨å…¥é«˜é¢‘ç‰¹å¾ç”¨äºè¾¹ç•Œç»†åŒ–ï¼ŒåŒæ—¶ä¼ å…¥å¯†åº¦å›¾ï¼‰
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_img_embed,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
                high_freq_features=curr_high_freq,  # ğŸ”¥ æ³¨å…¥é«˜é¢‘ç‰¹å¾
                density_map=curr_density_map,  # ğŸ”¥ [New] ä¼ å…¥å¯†åº¦å›¾
            )
            
            # === Step 7: åå¤„ç† & èšåˆ ===
            # low_res_masks: [N_cells, 1, 256, 256] -> åˆå¹¶æˆå•å›¾ [1, 1, 256, 256]
            merged_logits, _ = torch.max(low_res_masks, dim=0, keepdim=True) 
            
            # IoU èšåˆ: å–å¹³å‡
            merged_iou = torch.mean(iou_predictions, dim=0, keepdim=True)

            mask_post = self.postprocess_masks(
                merged_logits,
                input_size=batched_input[i]["image"].shape[-2:], 
                original_size=batched_input[i]["original_size"],
            )
            
            # ğŸ”¥ [æ–°å¢] å°† density_map è°ƒæ•´åˆ°ä¸ mask ç›¸åŒçš„å¤§å°
            # mask_post çš„å¤§å°æ˜¯ original_sizeï¼Œdensity_map éœ€è¦åŒ¹é…
            target_h, target_w = mask_post.shape[-2:]
            density_map_i = density_map[i]  # [1, H', W']
            if density_map_i.shape[-2:] != (target_h, target_w):
                density_map_i = F.interpolate(
                    density_map_i.unsqueeze(0), 
                    size=(target_h, target_w), 
                    mode='bilinear', 
                    align_corners=False
                ).squeeze(0)  # [1, target_h, target_w]
            
            outputs.append({
                "masks": mask_post, # âœ… ç¡®è®¤ï¼šè¿”å› Float Logits
                "iou_predictions": merged_iou,
                "low_res_logits": merged_logits,
                "heatmap_logits": heatmap_logits[i].unsqueeze(0),
                "attr_logits": attr_logits,  # ä¼ é€’å±æ€§ logitsï¼ˆåŒ…å« density åˆ†ç±»ï¼‰
                "density_map": density_map_i,  # ğŸ”¥ [æ–°å¢] åƒç´ çº§å¯†åº¦å›¾ï¼ˆç”¨äº DeNSe å¼å¼ºå¯¹é½ï¼‰
                "pnurl_loss": pnurl_loss
            })
            
        return outputs