# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import torch
from torch import nn
from torch.nn import functional as F
from typing import Any, Dict, List, Tuple, Optional
from .image_encoder import ImageEncoderViT
from .mask_decoder import MaskDecoder
from .prompt_encoder import PromptEncoder
from .pnurl import PNuRL
import sys
import os

# === ä¾èµ–æ£€æŸ¥ ===
try:
    import clip
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../../CLIP")) 
    try:
        import clip
    except ImportError:
        print("âš ï¸ Warning: CLIP not found.")

try:
    from prompt_generator import TextGuidedPointGenerator
except ImportError:
    sys.path.append(os.path.join(os.path.dirname(__file__), "../../.."))
    try:
        from prompt_generator import TextGuidedPointGenerator
    except ImportError:
        print("âš ï¸ Warning: prompt_generator.py not found.")

# === åŸºç¡€ SAM ç±» ===
class Sam(nn.Module):
    mask_threshold: float = 0.0
    image_format: str = "RGB"

    def __init__(
        self,
        image_encoder: ImageEncoderViT,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
        pixel_mean: List[float] = [123.675, 116.28, 103.53],
        pixel_std: List[float] = [58.395, 57.12, 57.375],
    ) -> None:
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self.register_buffer("pixel_mean", torch.Tensor(pixel_mean).view(-1, 1, 1), False)
        self.register_buffer("pixel_std", torch.Tensor(pixel_std).view(-1, 1, 1), False)

    @property
    def device(self) -> Any:
        return self.pixel_mean.device

    @torch.no_grad()
    def forward(self, batched_input: List[Dict[str, Any]], multimask_output: bool):
        # åŸºç¡€ SAM forward é€»è¾‘ä¿æŒä¸å˜ï¼Œä¸»è¦é€»è¾‘åœ¨ TextSam ä¸­é‡å†™
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images)
        outputs = []
        for image_record, curr_embedding in zip(batched_input, image_embeddings):
            if "point_coords" in image_record:
                points = (image_record["point_coords"], image_record["point_labels"])
            else:
                points = None
            sparse_embeddings, dense_embeddings = self.prompt_encoder(
                points=points,
                boxes=image_record.get("boxes", None),
                masks=image_record.get("mask_inputs", None),
            )
            low_res_masks, iou_predictions = self.mask_decoder(
                image_embeddings=curr_embedding.unsqueeze(0),
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )
            masks = self.postprocess_masks(
                low_res_masks,
                input_size=image_record["image"].shape[-2:],
                original_size=image_record["original_size"],
            )
            masks = masks > self.mask_threshold
            outputs.append({
                "masks": masks,
                "iou_predictions": iou_predictions,
                "low_res_logits": low_res_masks,
            })
        return outputs

    def postprocess_masks(self, masks: torch.Tensor, input_size: Tuple[int, ...], original_size: Tuple[int, ...]) -> torch.Tensor:
        if masks.dim() == 3: masks = masks.unsqueeze(0)
        masks = F.interpolate(
            masks,
            (self.image_encoder.img_size, self.image_encoder.img_size),
            mode="bilinear",
            align_corners=False,
        )
        masks = masks[..., : input_size[0], : input_size[1]]
        masks = F.interpolate(masks, original_size, mode="bilinear", align_corners=False)
        return masks

    def preprocess(self, x: torch.Tensor) -> torch.Tensor:
        x = (x - self.pixel_mean) / self.pixel_std
        h, w = x.shape[-2:]
        padh = self.image_encoder.img_size - h
        padw = self.image_encoder.img_size - w
        x = F.pad(x, (0, padw, 0, padh))
        return x

# === ğŸ”¥ [å…³é”®ä¿®æ”¹] 1. å®šä¹‰ Dual-Prompt Learner (åŒå±‚æç¤ºåº“) ===
# çµæ„Ÿæ¥æº: CA-SAM2 (Context-Aware)
class DualPromptLearner(nn.Module):
    def __init__(self, clip_model, num_organs=14, n_ctx_gen=8, n_ctx_spec=8):
        super().__init__()
        # è·å– CLIP å±æ€§
        ctx_dim = clip_model.ln_final.weight.shape[0] # 512
        dtype = clip_model.dtype
        self.dtype = dtype

        # --- A. é€šç”¨ç‰¹å¾åº“ (General Bank) ---
        # æ‰€æœ‰ç»†èƒæ ¸å…±äº«çš„çŸ¥è¯† (Implicit Knowledge)
        print(f"ğŸ§  Init DualLearner: General Ctx ({n_ctx_gen}) + Specific Ctx ({n_ctx_spec} x {num_organs} organs)")
        self.ctx_general = nn.Parameter(torch.empty(n_ctx_gen, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_general, std=0.02)
        
        # --- B. ç‰¹å®šç‰¹å¾åº“ (Specific Bank) ---
        # é’ˆå¯¹ä¸åŒå™¨å®˜/ç»„ç»‡çš„ç‰¹å®šçŸ¥è¯†åº“
        self.ctx_specific = nn.Parameter(torch.empty(num_organs, n_ctx_spec, ctx_dim, dtype=dtype))
        nn.init.normal_(self.ctx_specific, std=0.02)
        
        # ä¿å­˜ CLIP ç»„ä»¶
        self.clip_token_embedding = clip_model.token_embedding
        self.clip_transformer = clip_model.transformer
        self.clip_ln_final = clip_model.ln_final
        self.clip_text_projection = clip_model.text_projection
        
        self.n_ctx_gen = n_ctx_gen
        self.n_ctx_spec = n_ctx_spec
        self.total_ctx = n_ctx_gen + n_ctx_spec

    def forward(self, organ_indices, tokenized_prompts):
        """
        Args:
            organ_indices: [Batch] å½“å‰ batch å¯¹åº”çš„å™¨å®˜ ID
            tokenized_prompts: [Batch, 77] è¾“å…¥çš„åŸºç¡€æ–‡æœ¬ (e.g. "Cell nuclei")
        """
        batch_size = len(organ_indices)
        
        # 1. å‡†å¤‡æ–‡æœ¬ Embedding (e.g., "Cell nuclei")
        embedding = self.clip_token_embedding(tokenized_prompts).type(self.dtype)
        
        # 2. å‡†å¤‡é€šç”¨ Context (æ‰©å±•åˆ° Batch)
        # [Batch, n_gen, dim]
        ctx_gen = self.ctx_general.unsqueeze(0).expand(batch_size, -1, -1)
        
        # 3. å‡†å¤‡ç‰¹å®š Context (æŸ¥è¡¨)
        # [Batch, n_spec, dim]
        ctx_spec = self.ctx_specific[organ_indices]
        
        # 4. èåˆ Context: [é€šç”¨] + [ç‰¹å®š]
        ctx = torch.cat([ctx_gen, ctx_spec], dim=1) # [Batch, total_ctx, dim]

        # 5. æ‹¼æ¥æœ€ç»ˆåºåˆ—: [SOS] + [Dual_CTX] + [Text] + [EOS]
        prefix = embedding[:, :1, :] 
        suffix = embedding[:, 1 : 77 - self.total_ctx, :] 

        x = torch.cat([prefix, ctx, suffix], dim=1)

        # 6. CLIP ç¼–ç æµç¨‹
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.clip_transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.clip_ln_final(x).type(self.dtype)

        # 7. æå–ç‰¹å¾ (EOSä½ç½®)
        original_eos_idx = tokenized_prompts.argmax(dim=-1)
        eos_idx = torch.clamp(original_eos_idx + self.total_ctx, max=76)
        text_features = x[torch.arange(x.shape[0]), eos_idx] @ self.clip_text_projection

        return text_features


# === ğŸ”¥ [å…³é”®ä¿®æ”¹] 2. MP-SAM (TextSam) æ ¸å¿ƒç±» ===
class TextSam(Sam):
    def __init__(
        self, 
        image_encoder, 
        prompt_encoder, 
        mask_decoder,
        pixel_mean=[123.675, 116.28, 103.53],
        pixel_std=[58.395, 57.12, 57.375],
        clip_model_name="ViT-B/16",
        text_dim=512,
        embed_dim=256,
        num_organs=14 # MoNuSeg é»˜è®¤ 14 ç±»ï¼Œå¯è§†æƒ…å†µè°ƒæ•´
    ):
        super().__init__(image_encoder, prompt_encoder, mask_decoder, pixel_mean, pixel_std)
        
        print(f"ğŸš€ Initializing MP-SAM (Multi-granularity Prompt SAM)...")
        
        # 1. åŠ è½½ CLIP
        self.clip_model, _ = clip.load(clip_model_name, device="cpu")
        for param in self.clip_model.parameters():
            param.requires_grad = False # å†»ç»“åŸå§‹ CLIP
            
        # 2. åˆå§‹åŒ– Dual-Prompt Learner (CA-SAM2)
        # å­¦ä¹ é€šç”¨çš„å’Œå™¨å®˜ç‰¹å®šçš„ Context
        self.prompt_learner = DualPromptLearner(
            self.clip_model, 
            num_organs=num_organs, 
            n_ctx_gen=8,  # é€šç”¨é•¿åº¦
            n_ctx_spec=8  # ç‰¹å®šé•¿åº¦
        )
        for param in self.prompt_learner.parameters():
            param.requires_grad = True # è§£å†» Learner
            
        # 3. åˆå§‹åŒ– PNuRL (PromptNu)
        # ç”¨äº Explicit Attribute Injection (æ˜¾å¼å±æ€§æ³¨å…¥)
        self.pnurl = PNuRL(
            feat_dim=embed_dim, # SAM ViT çš„è¾“å‡ºé€šå¸¸æ˜¯ 256
            embed_dim=embed_dim,
            clip_model_path=None # å·²ç»æœ‰ CLIP äº†ï¼ŒPNuRL å†…éƒ¨å¦‚æœä¸ä¼  path å¯ä»¥å¤ç”¨é€»è¾‘æˆ–è·³è¿‡åŠ è½½
        )
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å…±äº«ä¸€ä¸‹ CLIP ç»™ PNuRL (å¦‚æœ PNuRL ä»£ç æ”¯æŒ) æˆ–è€…è®© PNuRL ç‹¬ç«‹åŠ è½½
        # ä¸ºç®€åŒ–ï¼Œå‡è®¾ PNuRL ä½œä¸ºä¸€ä¸ª Attention æ¨¡å—ä½¿ç”¨
        
        # 4. åˆå§‹åŒ– Auto-Prompt Generator (SAC)
        self.prompt_generator = TextGuidedPointGenerator(
            embed_dim=embed_dim,
            text_dim=text_dim
        )
        
        # 5. å†»ç»“ç­–ç•¥
        for param in self.image_encoder.parameters(): param.requires_grad = False
        for param in self.prompt_encoder.parameters(): param.requires_grad = False
        for param in self.mask_decoder.parameters(): param.requires_grad = True
        
        # è§£å†» Adapter
        adapter_count = 0
        for name, param in self.image_encoder.named_parameters():
            if "Adapter" in name:
                param.requires_grad = True
                adapter_count += 1
                
        print(f"âœ… Model Ready: Adapters({adapter_count}), DualLearner, PNuRL Attention Unfrozen.")

    def forward(self, batched_input, multimask_output=False):
        # === Step 1: åŸºç¡€å›¾åƒç¼–ç  ===
        input_images = torch.stack([self.preprocess(x["image"]) for x in batched_input], dim=0)
        image_embeddings = self.image_encoder(input_images) # [B, 256, 64, 64]
        device = image_embeddings.device

        # ç¡®ä¿ CLIP åœ¨æ­£ç¡®è®¾å¤‡
        if self.clip_model.visual.conv1.weight.device != device:
            self.clip_model = self.clip_model.to(device)

        # === Step 2: æ•°æ®æå– (Organ ID & Attribute Text) ===
        # éœ€è¦ DataLoader é…åˆä¼ å…¥ 'organ_id' å’Œ 'attribute_text'
        # å¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤å€¼å…œåº•
        organ_indices = []
        attribute_texts = []
        base_texts = [] # "Cell nuclei"

        for x in batched_input:
            # Organ ID: ç”¨äºç‰¹å®šåº“ (DualLearner)
            organ_indices.append(x.get("organ_id", 0)) 
            # Attribute Text: ç”¨äºæ˜¾å¼è§„åˆ™ (PNuRL) - e.g. "Large, dark nuclei"
            attribute_texts.append(x.get("attribute_text", "")) 
            # Base Text: ç”¨äº DualLearner çš„åŸºç¡€ - e.g. "Cell nuclei"
            base_texts.append(x.get("text_prompt", "Cell nuclei"))

        organ_indices = torch.tensor(organ_indices).to(device)

        # === Step 3: Dual-Prompt Learner (Implicit Context) ===
        # ç”Ÿæˆéšå¼çš„ã€åŒ…å«é€šç”¨å’Œç‰¹å®šçŸ¥è¯†çš„æ–‡æœ¬ç‰¹å¾
        # åŒæ—¶æ„é€ è´Ÿæ ·æœ¬ (Background) ç”¨äº Heatmap
        
        # Positive Prompts
        pos_tokens = clip.tokenize(base_texts, truncate=True).to(device)
        pos_feats = self.prompt_learner(organ_indices, pos_tokens) # [B, 512]
        
        # Negative Prompts (Background)
        # æˆ‘ä»¬å¯ä»¥è®¤ä¸º Background ä¹Ÿæ˜¯ä¸€ç§â€œå™¨å®˜â€ï¼Œæˆ–è€…ä½¿ç”¨é€šç”¨çš„ Background
        neg_tokens = clip.tokenize(["Background"] * len(base_texts), truncate=True).to(device)
        # å¯¹äº Backgroundï¼Œæˆ‘ä»¬å¯èƒ½åªç”¨é€šç”¨åº“ï¼Œæˆ–è€…è®¾å®šä¸€ä¸ªç‰¹æ®Šçš„ organ_id
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šå¤ç”¨ organ_indicesï¼Œå‡è®¾æ¯ä¸ªå™¨å®˜çš„èƒŒæ™¯ä¹Ÿä¸åŒ
        neg_feats = self.prompt_learner(organ_indices, neg_tokens) # [B, 512]

        # å½’ä¸€åŒ–å¹¶æ‹¼æ¥
        pos_feats = pos_feats / pos_feats.norm(dim=-1, keepdim=True)
        neg_feats = neg_feats / neg_feats.norm(dim=-1, keepdim=True)
        text_features = torch.stack([pos_feats, neg_feats], dim=1).float() # [B, 2, 512]

        # === Step 4: PNuRL (Explicit Attribute Injection) ===
        # åˆ©ç”¨æ˜¾å¼çš„å±æ€§æè¿°ï¼Œå¯¹å›¾åƒç‰¹å¾è¿›è¡Œ Attention åŠ æƒ
        # è¿™æ˜¯ MP-SAM çš„å…³é”®ï¼šExplicit Knowledge guiding Vision
        
        # æ³¨æ„ï¼šæˆ‘ä»¬éœ€è¦ç¡®ä¿ PNuRL åœ¨æ­£ç¡®è®¾å¤‡
        if next(self.pnurl.parameters()).device != device:
            self.pnurl = self.pnurl.to(device)
            
        # PNuRL Forward
        # è¿”å›: refined_embeddings (åŠ æƒåçš„å›¾åƒç‰¹å¾), context (å±æ€§ä¸Šä¸‹æ–‡å‘é‡)
        # å¦‚æœ attribute_texts ä¸ºç©ºï¼ŒPNuRL å†…éƒ¨åº”å¤„ç†ä¸º Identity æˆ– Zero
        refined_image_embeddings, pnurl_context, _, _ = self.pnurl(
            image_features=image_embeddings,
            attribute_prompts=attribute_texts
        )
        
        # === Step 5: Auto-Prompt Generation (SAC) ===
        # ä½¿ç”¨ "Refined" çš„å›¾åƒç‰¹å¾ + "Dual-Learned" çš„æ–‡æœ¬ç‰¹å¾
        # ç”Ÿæˆ Heatmap å’Œ Points
        heatmap_logits = self.prompt_generator(refined_image_embeddings, text_features)
        
        # æå–ç‚¹
        points_in_feat, point_labels = self.prompt_generator.get_points_from_heatmap(heatmap_logits, topk=1)
        
        # åæ ‡æ˜ å°„ (Feature Grid -> Original Image)
        feat_size = image_embeddings.shape[-1] 
        input_size = self.image_encoder.img_size 
        scale_factor = input_size / feat_size
        point_coords = (points_in_feat * scale_factor) + (scale_factor * 0.5)

        # === Step 6: SAM Mask Decoder ===
        # èåˆ PNuRL çš„å±æ€§ä¸Šä¸‹æ–‡åˆ° Prompt ä¸­
        # æœ€ç»ˆ Prompt Embedding = [Sparse(Points)] + [Dense(Refined Image)]
        # ä¹Ÿå¯ä»¥å°† pnurl_context ä½œä¸ºé¢å¤–çš„ Token è¾“å…¥ Decoder (å¦‚æœ Decoder æ”¯æŒ)
        # è¿™é‡Œæˆ‘ä»¬ä¸»è¦ä¾é  Refined Image Embeddings æ¥ä¼ é€’å±æ€§ä¿¡æ¯
        
        sparse_embeddings, dense_embeddings = self.prompt_encoder(
            points=(point_coords, point_labels),
            boxes=None,
            masks=None,
        )
        
        # ä½¿ç”¨ Refined Image Embeddings è¿›è¡Œè§£ç 
        low_res_masks, iou_predictions = self.mask_decoder(
            image_embeddings=refined_image_embeddings, # ğŸ”¥ ä½¿ç”¨ PNuRL å¢å¼ºåçš„ç‰¹å¾
            image_pe=self.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
        )
        
        # === Step 7: ç»“æœå°è£… ===
        outputs = []
        for i in range(len(batched_input)):
            mask_post = self.postprocess_masks(
                low_res_masks[i],
                input_size=batched_input[i]["image"].shape[-2:], 
                original_size=batched_input[i]["original_size"],
            )
            
            outputs.append({
                "masks": mask_post,
                "iou_predictions": iou_predictions[i],
                "low_res_masks": low_res_masks[i],
                "heatmap_logits": heatmap_logits[i].unsqueeze(0)
            })
            
        return outputs