"""
PNuRL (Prompting Nuclei Representation Learning) æ¨¡å—
åŠŸèƒ½ï¼š
1. å®è§‚ç›‘ç£ï¼šé€šè¿‡ 5 ä¸ªåˆ†ç±»å¤´å¼ºåˆ¶ Image Encoder å­¦ä¹ ç‰©ç†å±žæ€§ã€‚
2. ç‰¹å¾çŸ«æ­£ï¼šåˆ©ç”¨å±žæ€§æ–‡æœ¬ (CLIP Embedding) å¯¹å›¾åƒç‰¹å¾è¿›è¡Œ Attention åŠ æƒã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple, Union
import os

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("Warning: clip package not available. PNuRL will use random embeddings.")


class AttributeClassifier(nn.Module):
    """å•ä¸ªå±žæ€§åˆ†ç±»å™¨"""
    def __init__(self, in_dim: int, num_classes: int):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(in_dim, in_dim // 2), # ç¨å¾®å¢žåŠ ä¸­é—´å±‚ç»´åº¦
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(in_dim // 2, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.classifier(x)


class AttributeClassifiers(nn.Module):
    """5ä¸ªå±žæ€§åˆ†ç±»å™¨ï¼šé¢œè‰²ã€å½¢çŠ¶ã€æŽ’åˆ—ã€å¤§å°ã€åˆ†å¸ƒ"""
    def __init__(
        self,
        in_dim: int,
        num_classes_per_attr: List[int],
    ):
        super().__init__()
        self.classifiers = nn.ModuleList([
            AttributeClassifier(in_dim, num_classes) 
            for num_classes in num_classes_per_attr
        ])
    
    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        return [classifier(x) for classifier in self.classifiers]


class AttributeAttention(nn.Module):
    """å±žæ€§æ³¨æ„åŠ›æœºåˆ¶: Use Attribute Embedding to refine Image Features"""
    def __init__(self, feat_dim: int, embed_dim: int):
        super().__init__()
        self.feat_dim = feat_dim
        self.embed_dim = embed_dim
        
        # å°†å±žæ€§åµŒå…¥æŠ•å½±åˆ°ç‰¹å¾ç»´åº¦
        self.attr_proj = nn.Linear(embed_dim, feat_dim)
        
        # Channel Attention
        self.attention = nn.Sequential(
            nn.Linear(feat_dim * 2, feat_dim),
            nn.ReLU(),
            nn.Linear(feat_dim, feat_dim),
            nn.Sigmoid()
        )
    
    def forward(
        self, 
        image_features: torch.Tensor, 
        attribute_embedding: torch.Tensor
    ) -> torch.Tensor:
        """
        image_features: [B, C, H, W]
        attribute_embedding: [B, embed_dim]
        """
        B, C, H, W = image_features.shape
        
        # æŠ•å½±å±žæ€§åµŒå…¥ -> [B, C]
        attr_proj = self.attr_proj(attribute_embedding)
        
        # å…¨å±€æ± åŒ–å›¾åƒç‰¹å¾ -> [B, C]
        image_pooled = F.adaptive_avg_pool2d(image_features, 1).view(B, C)
        
        # æ‹¼æŽ¥ -> [B, 2C]
        combined = torch.cat([image_pooled, attr_proj], dim=1)
        
        # è®¡ç®—æƒé‡ -> [B, C]
        attention_weights = self.attention(combined)
        
        # åº”ç”¨æƒé‡ (Channel-wise scaling)
        # Residual connection: F_new = F_old * (1 + Attention)
        attention_weights = attention_weights.view(B, C, 1, 1)
        weighted_features = image_features * (1 + attention_weights)
        
        return weighted_features


class PNuRL(nn.Module):
    def __init__(
        self,
        embed_dim: int = 256,  # ðŸ”¥ [ä¿®æ­£] ç»Ÿä¸€å‚æ•°åä¸º embed_dim (å¯¹åº” SAM feature dim)
        text_dim: int = 256,   # æŠ•å½±åŽçš„æ–‡æœ¬ç»´åº¦
        clip_model_path: Optional[str] = "ViT-B/16",
        # ðŸ”¥ [ä¿®æ­£] é»˜è®¤ç±»åˆ«æ•°åŒ¹é… DataLoader: [Color(2), Shape(3), Arrange(2), Size(3), Density(3)]
        num_classes_per_attr: List[int] = [2, 3, 2, 3, 3], 
        attr_loss_weight: float = 1.0,
    ):
        super().__init__()
        self.feat_dim = embed_dim
        self.embed_dim = text_dim # è¿™é‡Œå¤ç”¨å˜é‡åï¼Œå®žé™…æ˜¯ projected text dim
        self.attr_loss_weight = attr_loss_weight
        
        # 1. å±žæ€§åˆ†ç±»å¤´
        self.attribute_classifiers = AttributeClassifiers(
            in_dim=embed_dim,
            num_classes_per_attr=num_classes_per_attr
        )
        
        # 2. CLIP åŠ è½½
        self.clip_model = None
        if CLIP_AVAILABLE:
            try:
                print(f"Loading CLIP for PNuRL: {clip_model_path}...")
                model, _ = clip.load(clip_model_path, device="cpu", jit=False)
                self.clip_model = model
                # å†»ç»“ CLIP
                for param in self.clip_model.parameters():
                    param.requires_grad = False
            except Exception as e:
                print(f"Warning: PNuRL CLIP load failed: {e}")
        
        # 3. æ–‡æœ¬æŠ•å½± (CLIP 512 -> SAM 256)
        clip_out_dim = 512 # ViT-B/16 default
        if self.clip_model is not None and hasattr(self.clip_model, 'text_projection'):
            clip_out_dim = self.clip_model.text_projection.shape[1]
            
        self.text_proj = nn.Linear(clip_out_dim, text_dim)
        
        # 4. å±žæ€§æ³¨æ„åŠ›
        self.attribute_attention = AttributeAttention(embed_dim, text_dim)
        
        # 5. ä¸Šä¸‹æ–‡èžåˆ (ç”Ÿæˆé¢å¤–çš„ context token å–‚ç»™ decoder)
        self.context_fusion = nn.Sequential(
            nn.Linear(embed_dim + text_dim, embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim, embed_dim)
        )
    
    def encode_attribute_text(self, attribute_prompts: List[str], device) -> torch.Tensor:
        """ç¼–ç æ–‡æœ¬æç¤º"""
        if self.clip_model is None:
            # Fallback: éšæœºå‘é‡
            return self.text_proj(torch.randn(len(attribute_prompts), 512, device=device))
            
        with torch.no_grad():
            # ç¡®ä¿ CLIP åœ¨æ­£ç¡®è®¾å¤‡
            if next(self.clip_model.parameters()).device != device:
                self.clip_model.to(device)
            
            # Tokenize
            # å¤„ç†å¯èƒ½çš„ç©ºå­—ç¬¦ä¸²æˆ– list nesting
            clean_prompts = []
            for p in attribute_prompts:
                if isinstance(p, (list, tuple)): p = " ".join([str(x) for x in p])
                clean_prompts.append(str(p)[:77]) # æˆªæ–­é˜²æ­¢è¿‡é•¿
            
            tokens = clip.tokenize(clean_prompts, truncate=True).to(device)
            text_features = self.clip_model.encode_text(tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
        return self.text_proj(text_features.float()) # [B, embed_dim]

    def forward(
        self,
        image_features: torch.Tensor,
        attribute_labels: Optional[List[torch.Tensor]] = None,
        attribute_prompts: Optional[List[str]] = None,
        return_loss: bool = True,
    ):
        """
        Returns:
            refined_features: [B, C, H, W]
            context_embedding: [B, C]
            loss: scalar
            logits_dict: dict
        """
        B, C, H, W = image_features.shape
        device = image_features.device
        
        # === 1. å±žæ€§åˆ†ç±» (Auxiliary Task) ===
        attribute_logits = self.attribute_classifiers(image_features) # List[[B, N_cls]]
        
        # è®¡ç®—æ¦‚çŽ‡ç”¨äºŽåŽç»­åŠ æƒ (Soft Attribute Representation)
        # æ‹¼æŽ¥æ‰€æœ‰å±žæ€§çš„æ¦‚çŽ‡åˆ†å¸ƒ
        probs_list = [F.softmax(l, dim=1) for l in attribute_logits]
        p_i = torch.cat(probs_list, dim=1) # [B, Total_Classes]
        
        # === 2. æ–‡æœ¬ç¼–ç  ===
        if attribute_prompts is not None:
            text_embed = self.encode_attribute_text(attribute_prompts, device) # [B, embed_dim]
        else:
            text_embed = torch.zeros(B, self.embed_dim, device=device)
            
        # === 3. åµŒå…¥èžåˆ (Text * Predicted_Probabilities) ===
        # æˆ‘ä»¬éœ€è¦å°† p_i æ˜ å°„åˆ°ä¸Ž text_embed ç›¸åŒçš„ç»´åº¦æ‰èƒ½ç›¸ä¹˜
        if not hasattr(self, 'prob_proj'):
            self.prob_proj = nn.Linear(p_i.shape[1], self.embed_dim).to(device)
        
        p_i_proj = self.prob_proj(p_i)
        
        # E = Text_Embedding * Predicted_Attributes
        # åªæœ‰å½“æ¨¡åž‹é¢„æµ‹çš„å±žæ€§ä¸Žæ–‡æœ¬æè¿°ä¸€è‡´æ—¶ï¼ŒE æ‰ä¼šæ¿€æ´»
        E = text_embed * p_i_proj # [B, embed_dim]
        
        # === 4. ç‰¹å¾çŸ«æ­£ (Refinement) ===
        refined_features = self.attribute_attention(image_features, E)
        
        # === 5. ç”Ÿæˆä¸Šä¸‹æ–‡ Context ===
        image_pooled = F.adaptive_avg_pool2d(refined_features, 1).view(B, C)
        context_in = torch.cat([image_pooled, E], dim=1)
        learnable_context = self.context_fusion(context_in)
        
        # === 6. è®¡ç®— Loss ===
        loss = torch.tensor(0.0, device=device)
        if return_loss and attribute_labels is not None:
            loss = self.compute_attribute_loss(attribute_logits, attribute_labels)
            
        logits_dict = {
            'color': attribute_logits[0],
            'shape': attribute_logits[1],
            'arrange': attribute_logits[2],
            'size': attribute_logits[3],
            'density': attribute_logits[4],
        }
        
        return refined_features, learnable_context, loss, logits_dict

    def compute_attribute_loss(self, logits_list, labels_list):
        total_loss = 0.0
        # æƒé‡: Color, Shape, Arrange, Size, Density
        # ç»™ Size å’Œ Density æ›´é«˜çš„æƒé‡ï¼Œå› ä¸ºå®ƒä»¬å¯¹åˆ†å‰²å½±å“æœ€å¤§
        weights = [1.0, 1.0, 1.0, 2.0, 2.0]
        
        for i, (logits, label) in enumerate(zip(logits_list, labels_list)):
            # label shape: [B] (indices)
            if label.dim() > 1: label = label.squeeze()
            
            # å®‰å…¨æ£€æŸ¥
            if i < len(weights):
                w = weights[i]
            else:
                w = 1.0
                
            loss_i = F.cross_entropy(logits, label.long())
            total_loss += w * loss_i
            
        return total_loss / len(logits_list)