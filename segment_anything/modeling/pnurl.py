"""
PNuRL (Prompting Nuclei Representation Learning) æ¨¡å—
åŠŸèƒ½ï¼š
1. å®è§‚ç›‘ç£ï¼šé€šè¿‡ 5 ä¸ªåˆ†ç±»å¤´å¼ºåˆ¶ Image Encoder å­¦ä¹ ç‰©ç†å±žæ€§ã€‚
2. ç‰¹å¾çŸ«æ­£ï¼šåˆ©ç”¨å±žæ€§æ–‡æœ¬ (CLIP Embedding) å¯¹å›¾åƒç‰¹å¾è¿›è¡Œ Attention åŠ æƒã€‚
3. å¯†åº¦ç‰¹å¾æå–ï¼šä¸“é—¨ä¸º Density å±žæ€§è®¾è®¡å¤šå°ºåº¦åˆ†æ”¯ï¼Œä¸ºåŽç»­æ¨¡å—æä¾›çº¹ç†ç‰¹å¾ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Tuple, Union
import os

try:
    import clip
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("Warning: clip package not available. PNuRL will use random embeddings.")


class AttributeClassifier(nn.Module):
    """é€šç”¨å±žæ€§åˆ†ç±»å™¨ (ç”¨äºŽ Color, Shape, Arrange, Size)"""
    def __init__(self, in_dim: int, num_classes: int):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(in_dim, in_dim // 2), 
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(in_dim // 2, num_classes)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.classifier(x)


class MultiScaleAttributeHead(nn.Module):
    """
    å¤šå°ºåº¦å±žæ€§åˆ†ç±»å¤´
    é€‚ç”¨äºŽéœ€è¦åŒæ—¶å…³æ³¨å±€éƒ¨çº¹ç† (Texture) å’Œå…¨å±€è¯­ä¹‰ (Semantics) çš„å±žæ€§
    ä¾‹å¦‚: Shape (å±€éƒ¨è¾¹ç¼˜), Size (å…¨å±€é¢ç§¯), Density (å±€éƒ¨å¯†é›†åº¦)
    """
    def __init__(self, in_dim: int, num_classes: int):
        super().__init__()
        # æµ…å±‚åˆ†æ”¯ï¼šæå–çº¹ç†/è¾¹ç¼˜ç‰¹å¾ -> [B, C/4, H, W]
        self.shallow_branch = nn.Sequential(
            nn.Conv2d(in_dim, in_dim // 2, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_dim // 2),
            nn.ReLU(),
            nn.Conv2d(in_dim // 2, in_dim // 4, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_dim // 4),
            nn.ReLU()
        )
        
        # æ·±å±‚åˆ†æ”¯ï¼šæå–å…¨å±€è¯­ä¹‰ -> [B, C/2]
        self.deep_branch = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(in_dim, in_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(in_dim // 2, num_classes)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """
        Returns:
            logits: [B, num_classes]
            features: [feat_low, feat_high]
                - feat_low: [B, C//4, H, W] (ä¿ç•™ç©ºé—´ä¿¡æ¯)
                - feat_high: [B, C//2] (å…¨å±€å‘é‡)
        """
        feat_low = self.shallow_branch(x) 
        feat_high = self.deep_branch(x)
        logits = self.classifier(feat_high)
        return logits, [feat_low, feat_high]


class AttributeClassifiers(nn.Module):
    """
    ç»„åˆåˆ†ç±»å™¨ç®¡ç†å™¨ (å‡çº§ç‰ˆ)
    å±žæ€§é¡ºåº: 0:Color, 1:Shape, 2:Arrange, 3:Size, 4:Density
    ç­–ç•¥: å¯¹ Shape(1), Size(3), Density(4) ä½¿ç”¨å¤šå°ºåº¦å¤´
    """
    def __init__(
        self,
        in_dim: int,
        num_classes_per_attr: List[int],
    ):
        super().__init__()
        # ç¡®ä¿å±žæ€§æ•°é‡æ­£ç¡® (é»˜è®¤ä¸º5ä¸ª)
        assert len(num_classes_per_attr) == 5, "Must provide class counts for 5 attributes"
        
        self.heads = nn.ModuleList()
        # å®šä¹‰å“ªäº›å±žæ€§éœ€è¦å¤šå°ºåº¦ç‰¹å¾ (Indices)
        self.multiscale_indices = {1, 3, 4}  # Shape, Size, Density
        
        for i, num_classes in enumerate(num_classes_per_attr):
            if i in self.multiscale_indices:
                self.heads.append(MultiScaleAttributeHead(in_dim, num_classes))
            else:
                self.heads.append(AttributeClassifier(in_dim, num_classes))
    
    def forward(self, x: torch.Tensor) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """
        Returns:
            logits_list: List of 5 logits tensors
            fused_features: [fused_low, fused_high] - æ‹¼æŽ¥åŽçš„å¤šå°ºåº¦ç‰¹å¾
        """
        logits_list = []
        visual_feats_low = []
        visual_feats_high = []
        
        for i, head in enumerate(self.heads):
            if i in self.multiscale_indices:
                # å¤šå°ºåº¦å¤´è¿”å›ž: logits, [low, high]
                logits, feats = head(x)
                logits_list.append(logits)
                visual_feats_low.append(feats[0])
                visual_feats_high.append(feats[1])
            else:
                # æ™®é€šå¤´è¿”å›ž: logits
                logits = head(x)
                logits_list.append(logits)
        
        # ðŸ”¥ [æ ¸å¿ƒèžåˆ] å°† Shape, Size, Density çš„ç‰¹å¾æ‹¼æŽ¥
        # low: 3 * [B, C/4, H, W] -> [B, 3*C/4, H, W]
        # high: 3 * [B, C/2]      -> [B, 3*C/2]
        if len(visual_feats_low) != 3 or len(visual_feats_high) != 3:
            raise ValueError(f"Expected 3 multiscale features, got {len(visual_feats_low)} low and {len(visual_feats_high)} high")
        fused_low = torch.cat(visual_feats_low, dim=1)
        fused_high = torch.cat(visual_feats_high, dim=1)
        
        return logits_list, [fused_low, fused_high]


class AttributeAttention(nn.Module):
    """å±žæ€§æ³¨æ„åŠ›æœºåˆ¶"""
    def __init__(self, feat_dim: int, embed_dim: int):
        super().__init__()
        self.attr_proj = nn.Linear(embed_dim, feat_dim)
        
        self.attention = nn.Sequential(
            nn.Linear(feat_dim * 2, feat_dim),
            nn.ReLU(),
            nn.Linear(feat_dim, feat_dim),
            nn.Sigmoid()
        )
    
    def forward(self, image_features: torch.Tensor, attribute_embedding: torch.Tensor) -> torch.Tensor:
        B, C, H, W = image_features.shape
        
        attr_proj = self.attr_proj(attribute_embedding) # [B, C]
        image_pooled = F.adaptive_avg_pool2d(image_features, 1).view(B, C) # [B, C]
        
        combined = torch.cat([image_pooled, attr_proj], dim=1) # [B, 2C]
        attention_weights = self.attention(combined).view(B, C, 1, 1) # [B, C, 1, 1]
        
        # Channel-wise scaling (Residual)
        return image_features * (1 + attention_weights)


class PNuRL(nn.Module):
    def __init__(
        self,
        embed_dim: int = 256,
        text_dim: int = 256,
        clip_model_path: Optional[str] = "ViT-B/16",
        num_classes_per_attr: List[int] = [2, 3, 2, 3, 3], 
        attr_loss_weight: float = 1.0,
    ):
        super().__init__()
        self.feat_dim = embed_dim
        self.embed_dim = text_dim
        self.attr_loss_weight = attr_loss_weight
        
        # 1. å±žæ€§åˆ†ç±»å¤´
        self.attribute_classifiers = AttributeClassifiers(
            in_dim=embed_dim,
            num_classes_per_attr=num_classes_per_attr
        )
        
        # 2. CLIP åŠ è½½
        self.clip_model = None
        if CLIP_AVAILABLE:
            try:
                print(f"Loading CLIP for PNuRL: {clip_model_path}...")
                model, _ = clip.load(clip_model_path, device="cpu", jit=False)
                self.clip_model = model
                for param in self.clip_model.parameters():
                    param.requires_grad = False
            except Exception as e:
                print(f"Warning: PNuRL CLIP load failed: {e}")
        
        # 3. æ–‡æœ¬æŠ•å½±
        clip_out_dim = 512
        if self.clip_model is not None and hasattr(self.clip_model, 'text_projection'):
            clip_out_dim = self.clip_model.text_projection.shape[1]
        self.text_proj = nn.Linear(clip_out_dim, text_dim)
        
        # 4. å±žæ€§æ³¨æ„åŠ›
        self.attribute_attention = AttributeAttention(embed_dim, text_dim)
        
        # 5. ä¸Šä¸‹æ–‡èžåˆ
        self.context_fusion = nn.Sequential(
            nn.Linear(embed_dim + text_dim, embed_dim),
            nn.ReLU(),
            nn.Linear(embed_dim, embed_dim)
        )

        # ðŸ”¥ [ä¿®æ­£] 6. é¢„å®šä¹‰æ¦‚çŽ‡æŠ•å½±å±‚ (Prob Projection)
        # å¿…é¡»åœ¨ __init__ ä¸­å®šä¹‰ï¼Œå¦åˆ™ä¼˜åŒ–å™¨æ— æ³•æ›´æ–°å‚æ•°
        total_classes = sum(num_classes_per_attr) # e.g. 2+3+2+3+3 = 13
        self.prob_proj = nn.Linear(total_classes, text_dim) 

    def encode_attribute_text(self, attribute_prompts: List[str], device) -> torch.Tensor:
        if self.clip_model is None:
            return self.text_proj(torch.randn(len(attribute_prompts), 512, device=device))
            
        with torch.no_grad():
            if next(self.clip_model.parameters()).device != device:
                self.clip_model.to(device)
            
            clean_prompts = []
            for p in attribute_prompts:
                if isinstance(p, (list, tuple)): p = " ".join([str(x) for x in p])
                clean_prompts.append(str(p)[:77])
            
            tokens = clip.tokenize(clean_prompts, truncate=True).to(device)
            text_features = self.clip_model.encode_text(tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)
            
        return self.text_proj(text_features.float())

    def forward(
        self,
        image_features: torch.Tensor,
        attribute_labels: Optional[List[torch.Tensor]] = None,
        attribute_prompts: Optional[List[str]] = None,
        return_loss: bool = True,
    ):
        B, C, H, W = image_features.shape
        device = image_features.device
        
        # === 1. å±žæ€§åˆ†ç±» & ç‰¹å¾æå– ===
        # logits_list åŒ…å« 5 ä¸ªå±žæ€§çš„ logit
        # fused_features åŒ…å« [Concat_Low, Concat_High]
        attribute_logits, fused_features = self.attribute_classifiers(image_features)
        
        # Soft Attribute Representation
        probs_list = [F.softmax(l, dim=1) for l in attribute_logits]
        p_i = torch.cat(probs_list, dim=1) # [B, Total_Classes]
        
        # === 2. æ–‡æœ¬ç¼–ç  ===
        if attribute_prompts is not None:
            text_embed = self.encode_attribute_text(attribute_prompts, device)
        else:
            text_embed = torch.zeros(B, self.embed_dim, device=device)
            
        # === 3. åµŒå…¥èžåˆ ===
        # ðŸ”¥ [ä¿®æ­£] ä½¿ç”¨åœ¨ init ä¸­å®šä¹‰çš„å±‚
        p_i_proj = self.prob_proj(p_i)
        E = text_embed * p_i_proj
        
        # === 4. ç‰¹å¾çŸ«æ­£ ===
        refined_features = self.attribute_attention(image_features, E)
        
        # === 5. ç”Ÿæˆä¸Šä¸‹æ–‡ ===
        image_pooled = F.adaptive_avg_pool2d(refined_features, 1).view(B, C)
        context_in = torch.cat([image_pooled, E], dim=1)
        learnable_context = self.context_fusion(context_in)
        
        # === 6. Loss ===
        loss = torch.tensor(0.0, device=device)
        if return_loss and attribute_labels is not None:
            loss = self.compute_attribute_loss(attribute_logits, attribute_labels)
            
        logits_dict = {
            'color': attribute_logits[0],
            'shape': attribute_logits[1],
            'arrange': attribute_logits[2],
            'size': attribute_logits[3],
            'density': attribute_logits[4],
        }
        
        # è¿”å›ž fused_features (åŒ…å« Shape+Size+Density çš„ç‰©ç†ä¿¡æ¯)
        return refined_features, learnable_context, loss, logits_dict, fused_features

    def compute_attribute_loss(self, logits_list, labels_list):
        total_loss = 0.0
        # æƒé‡: Color, Shape, Arrange, Size, Density
        weights = [1.0, 1.0, 1.0, 2.0, 2.0]
        
        for i, (logits, label) in enumerate(zip(logits_list, labels_list)):
            # ðŸ”¥ [ä¿®æ­£] å®‰å…¨çš„ squeeze
            # åªæœ‰å½“ label æ˜¯ [B, 1] æ—¶æ‰ squeezeï¼Œå¦‚æžœæ˜¯ [B] åˆ™ä¸åŠ¨
            if label.dim() > 1 and label.shape[1] == 1:
                label = label.squeeze(1)
            
            w = weights[i] if i < len(weights) else 1.0
            loss_i = F.cross_entropy(logits, label.long())
            total_loss += w * loss_i
            
        return total_loss / len(logits_list)