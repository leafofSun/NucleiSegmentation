import os
import cv2
import json
import torch
import numpy as np
import random
import glob
from torch.utils import data
import albumentations as A
from albumentations.pytorch import ToTensorV2

# å°è¯•å¯¼å…¥ COCO å·¥å…·
try:
    from pycocotools import mask as coco_mask
except ImportError:
    print("âš ï¸ [DataLoader] pycocotools not installed. SA-1B RLE decoding might fail.")

# === ğŸ”¥ æ ¸å¿ƒç»„ä»¶ 1: åŒ»å­¦å›°éš¾è´Ÿæ ·æœ¬æ±  (Hard Negatives) ===
NEGATIVE_PROMPTS = [
    # Level 1: æœ€éš¾çš„å¹²æ‰° (ç”Ÿç‰©å­¦ç›¸ä¼¼)
    "Red blood cells", "Eosinophilic cytoplasm", "Stromal tissue", 
    "Extracellular matrix", "Collagen fibers", "Adipose tissue cells",
    "Blood vessel lumen",
    # Level 2: ä¼ªå½±ä¸èƒŒæ™¯
    "Tissue folds", "Air bubbles", "Glass slide background", "Blurred regions",
    # Level 3: è¯­ä¹‰é™·é˜± & é€šç”¨ç‰©ä½“
    "Mitochondria", "Golgi apparatus", "A photo of a cat", "A car"
]

def stack_dict_batched(batch):
    """è‡ªå®šä¹‰ collate_fn"""
    tensor_dict = {}
    for key, value in batch[0].items():
        if key == 'text_prompt' or key == 'name':
            tensor_dict[key] = [sample[key] for sample in batch]
        elif isinstance(value, torch.Tensor):
            tensor_dict[key] = torch.stack([sample[key] for sample in batch])
        else:
            tensor_dict[key] = [sample[key] for sample in batch]
    return tensor_dict

class TrainingDataset(data.Dataset):
    def __init__(self, data_dir, image_size=1024, crop_size=1024, mode='train', 
                 mask_num=1, requires_name=True, 
                 # ğŸ”¥ æŒ‡å‘ç»Ÿè®¡å­¦æ–‡ä»¶
                 dynamic_attr_path="data/MoNuSeg_SA1B/dataset_stats.json"):
        
        self.data_dir = data_dir
        self.image_size = image_size
        self.crop_size = crop_size
        self.mode = mode
        
        # === 1. åŠ è½½ç»Ÿè®¡å­¦é˜ˆå€¼ (PromptNu Logic) ===
        # é»˜è®¤å¤‡ç”¨å€¼ (ä¸‡ä¸€æ–‡ä»¶è¯»ä¸åˆ°)
        self.size_thresholds = {"small_upper": 300, "large_lower": 600}
        
        if os.path.exists(dynamic_attr_path):
            print(f"ğŸ“– [DataLoader] Loading Statistics from {dynamic_attr_path}...")
            with open(dynamic_attr_path, 'r') as f:
                stats = json.load(f)
                # è¯»å– PromptNu è®¡ç®—å‡ºçš„é˜ˆå€¼ (Small < Mean, Large > Mean + 2*Std)
                if "thresholds" in stats:
                    self.size_thresholds = stats["thresholds"]
                    print(f"   âœ… Using PromptNu Statistical Thresholds: Small < {self.size_thresholds['small_upper']:.1f}, Large > {self.size_thresholds['large_lower']:.1f}")
        else:
            if mode == 'train':
                print(f"âš ï¸ [DataLoader] Stats file not found at {dynamic_attr_path}. Using default fallback.")

        # === 2. æ‰«ææ–‡ä»¶ ===
        self.image_paths = []
        extensions = ['*.tif', '*.png', '*.jpg', '*.jpeg']
        for ext in extensions:
            self.image_paths.extend(glob.glob(os.path.join(data_dir, "**", ext), recursive=True))
        
        # è¿‡æ»¤æ‰ mask æ–‡ä»¶
        self.image_paths = [p for p in self.image_paths if "mask" not in p.lower()]
        
        # SA-1B æ ¼å¼æ£€æŸ¥
        valid_paths = []
        for p in self.image_paths:
            json_path = os.path.splitext(p)[0] + ".json"
            if os.path.exists(json_path):
                valid_paths.append(p)
        
        if len(valid_paths) > 0:
            self.image_paths = valid_paths
            print(f"âœ… [DataLoader] Found {len(self.image_paths)} valid image-json pairs.")
        else:
            print(f"âš ï¸ [DataLoader] No valid pairs found! SA-1B mode requires local JSONs.")

        # === 3. å¢å¼ºç­–ç•¥ (å…³é”®ä¿®æ”¹ï¼šä¿®å¤ CropSizeError) ===
        if mode == 'train':
            self.transform = A.Compose([
                # ğŸ”¥ ç¬¬ä¸€æ­¥ï¼šPadIfNeeded
                # å¦‚æœåŸå›¾(1000)å°äº crop_size(1024)ï¼Œå…ˆå¡«å……è¾¹ç¼˜ï¼Œé˜²æ­¢ RandomCrop æŠ¥é”™
                # åŒæ—¶ä¹Ÿä¿è¯äº†è®­ç»ƒæ—¶çœ‹åˆ°çš„ç»†èƒå°ºå¯¸ä¸æµ‹è¯•æ—¶ä¸€è‡´ (1:1)
                A.PadIfNeeded(
                    min_height=crop_size, 
                    min_width=crop_size, 
                    border_mode=cv2.BORDER_CONSTANT, 
                    value=0, 
                    mask_value=0
                ),
                # ğŸ”¥ ç¬¬äºŒæ­¥ï¼šRandomCrop
                # åœ¨ Pad åçš„å›¾ä¸Šéšæœºåˆ‡ 1024 (å¦‚æœ Pad åˆ° 1024ï¼Œè¿™å°±ç­‰äºå…¨å›¾)
                A.RandomCrop(width=crop_size, height=crop_size, p=1.0),
                
                # å…¶ä»–å¢å¼º
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.5),
                A.RandomRotate90(p=0.5),
                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),
                A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),
                
                # æœ€å Resize (è™½ç„¶ crop_size=1024=image_sizeï¼Œä½†è¿™æ­¥ç•™ç€ä¿é™©)
                A.Resize(height=image_size, width=image_size, interpolation=cv2.INTER_LINEAR),
                ToTensorV2(),
            ])
        else:
            self.transform = A.Compose([
                # æµ‹è¯•æ—¶ï¼šç›´æ¥ Pad åˆ° 1024ï¼Œä¿æŒåŸå§‹æ¯”ä¾‹å’Œåˆ†è¾¨ç‡
                A.PadIfNeeded(
                    min_height=image_size, 
                    min_width=image_size, 
                    border_mode=cv2.BORDER_CONSTANT, 
                    value=0, 
                    mask_value=0
                ),
                A.Resize(height=image_size, width=image_size, interpolation=cv2.INTER_LINEAR),
                ToTensorV2(),
            ])

    def __len__(self):
        return len(self.image_paths)

    # === åŸºäº PromptNu ç»Ÿè®¡æ•°æ®çš„è§£ç  ===
    def decode_sa1b_mask(self, annotations, h, w, size_mode=None):
        """
        è§£ç å¹¶æ ¹æ® dataset_stats.json é‡Œçš„é˜ˆå€¼è¿›è¡Œè¿‡æ»¤
        """
        mask = np.zeros((h, w), dtype=np.uint8)
        valid_pixel_count = 0
        
        # ä»åŠ è½½çš„ç»Ÿè®¡æ•°æ®ä¸­è·å–åŠ¨æ€é˜ˆå€¼
        small_thresh = self.size_thresholds['small_upper']  # Mean
        large_thresh = self.size_thresholds['large_lower']  # Mean + 2*Std
        
        for ann in annotations:
            if 'segmentation' in ann:
                seg = ann['segmentation']
                m = None
                
                # RLE
                if isinstance(seg, dict) and 'counts' in seg:
                    m = coco_mask.decode(seg)
                # Polygon
                elif isinstance(seg, list):
                    m = np.zeros((h, w), dtype=np.uint8)
                    for poly in seg:
                        pts = np.array(poly, dtype=np.int32).reshape((-1, 2))
                        cv2.fillPoly(m, [pts], 1)
                
                if m is not None:
                    # å®æ—¶è®¡ç®—é¢ç§¯
                    area = np.sum(m > 0)
                    
                    keep = False
                    if size_mode == 'large':
                        # PromptNu å®šä¹‰: Area > Mean + 2*Std
                        if area > large_thresh: keep = True 
                    elif size_mode == 'small':
                        # PromptNu å®šä¹‰: Area < Mean
                        if area < small_thresh: keep = True 
                    else:
                        keep = True # Generic æ¨¡å¼
                    
                    if keep:
                        mask[m > 0] = 1
                        valid_pixel_count += area
                    
        return mask, valid_pixel_count

    def __getitem__(self, index):
        img_path = self.image_paths[index]
        filename = os.path.basename(img_path)
        base_name = os.path.splitext(img_path)[0]
        json_path = base_name + ".json"
        
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]
        
        # è¯»å– JSON
        annotations = []
        if os.path.exists(json_path):
            try:
                with open(json_path, 'r') as f:
                    data = json.load(f)
                    if isinstance(data, dict):
                        annotations = data.get('annotations', [])
                    elif isinstance(data, list):
                        annotations = data
            except:
                pass

        # === é‡‡æ ·é€»è¾‘ ===
        rand = random.random()
        text_prompt = "Cell nuclei"
        target_mask = np.zeros((h, w), dtype=np.uint8)
        
        # 1. è´Ÿæ ·æœ¬ (10%)
        if self.mode == 'train' and rand < 0.1:
            text_prompt = random.choice(NEGATIVE_PROMPTS)
            target_mask = np.zeros((h, w), dtype=np.uint8)
            
        # 2. å±æ€§è®­ç»ƒ (45%)
        elif self.mode == 'train' and rand < 0.55 and len(annotations) > 0:
            if random.random() < 0.5:
                # Large
                text_prompt = random.choice(["Large nuclei", "Tumor nuclei"])
                target_mask, px_count = self.decode_sa1b_mask(annotations, h, w, size_mode='large')
            else:
                # Small
                text_prompt = random.choice(["Small nuclei", "Lymphocyte nuclei"])
                target_mask, px_count = self.decode_sa1b_mask(annotations, h, w, size_mode='small')
            
            # ğŸ”¥ å…œåº•æœºåˆ¶ï¼šå¦‚æœå½“å‰å›¾æ²¡æœ‰ç¬¦åˆç»Ÿè®¡é˜ˆå€¼çš„ç»†èƒï¼ˆå…¨é»‘ï¼‰ï¼Œå›é€€åˆ° Generic
            if target_mask.sum() == 0:
                text_prompt = "Cell nuclei"
                target_mask, _ = self.decode_sa1b_mask(annotations, h, w, size_mode=None)

        # 3. é€šç”¨è®­ç»ƒ (45%)
        else:
            text_prompt = "Cell nuclei"
            target_mask, _ = self.decode_sa1b_mask(annotations, h, w, size_mode=None)
        
        # å¢å¼º (æ­¤æ—¶ target_mask å°ºå¯¸æ˜¯åŸå§‹çš„ï¼Œå¢å¼ºåå˜æˆ 1024)
        augmented = self.transform(image=image, mask=target_mask)
        
        return {
            "image": augmented['image'].float(),
            "label": augmented['mask'].long().unsqueeze(0),
            "text_prompt": text_prompt,
            "name": filename.split('.')[0],
            "original_size": (self.image_size, self.image_size)
        }